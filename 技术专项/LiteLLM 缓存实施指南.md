---
tags: [AI/RAG, AI/ç¼“å­˜ï¼Œæ¶æ„/æ€§èƒ½ä¼˜åŒ–ï¼ŒLiteLLM]
created: 2026-02-28
modified: 2026-02-28
status: å®æ–½ä¸­
owner: Eddy
priority: P0
---

# LiteLLM ç¼“å­˜å®æ–½æŒ‡å—

**ç›®æ ‡**ï¼šåœ¨å…¬å¸ RAG ç³»ç»Ÿä¸­å®æ–½ LiteLLM ç¼“å­˜ï¼Œé™ä½ 70%+ LLM æˆæœ¬ï¼Œæå‡ 2-3 å€å“åº”é€Ÿåº¦

**é¢„è®¡æ—¶é—´**ï¼š2-3 å¤©ï¼ˆPOCï¼‰+ 1 å‘¨ï¼ˆç”Ÿäº§ï¼‰

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#1-ç¯å¢ƒå‡†å¤‡)
2. [å®‰è£…é…ç½®](#2-å®‰è£…é…ç½®)
3. [ä»£ç å®ç°](#3-ä»£ç å®ç°)
4. [æµ‹è¯•æ–¹æ¡ˆ](#4-æµ‹è¯•æ–¹æ¡ˆ)
5. [ç›‘æ§æ–¹æ¡ˆ](#5-ç›‘æ§æ–¹æ¡ˆ)
6. [æ•…éšœæ’æŸ¥](#6-æ•…éšœæ’æŸ¥)
7. [æˆæœ¬ä¼°ç®—](#7-æˆæœ¬ä¼°ç®—)

---

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® | è¯´æ˜ |
|------|---------|---------|------|
| **Redis æœåŠ¡å™¨** | 2 æ ¸ 4GB | 4 æ ¸ 8GB | æ ¹æ®ç¼“å­˜é‡è°ƒæ•´ |
| **åº”ç”¨æœåŠ¡å™¨** | 2 æ ¸ 4GB | 4 æ ¸ 8GB | LiteLLM + åº”ç”¨ |
| **ç½‘ç»œ** | 100Mbps | 1Gbps | ä½å»¶è¿Ÿè¿æ¥ |

### 1.2 è½¯ä»¶è¦æ±‚

```bash
# å¿…éœ€
Python >= 3.9
Redis >= 7.0
LiteLLM >= 1.40.0

# å¯é€‰ï¼ˆè¯­ä¹‰ç¼“å­˜ï¼‰
OpenAI API Keyï¼ˆEmbeddingï¼‰
```

### 1.3 API Keys å‡†å¤‡

```bash
# .env æ–‡ä»¶
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
REDIS_PASSWORD=your-redis-password
```

---

## 2. å®‰è£…é…ç½®

### 2.1 å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate

# å®‰è£… LiteLLM åŠç¼“å­˜ä¾èµ–
pip install litellm[redis]
pip install redis
pip install openai  # è¯­ä¹‰ç¼“å­˜éœ€è¦

# æˆ–ä½¿ç”¨ requirements.txt
cat > requirements.txt << EOF
litellm[redis]>=1.40.0
redis>=5.0.0
openai>=1.12.0
python-dotenv>=1.0.0
EOF

pip install -r requirements.txt
```

### 2.2 éƒ¨ç½² Redis

#### æ–¹æ¡ˆ Aï¼šDockerï¼ˆæ¨èå¼€å‘/æµ‹è¯•ï¼‰

```bash
# å¯åŠ¨ Redis
docker run -d \
  --name redis-cache \
  -p 6379:6379 \
  -v redis-data:/data \
  redis:7-alpine \
  redis-server --appendonly yes

# éªŒè¯
docker exec -it redis-cache redis-cli ping
# è¾“å‡ºï¼šPONG
```

#### æ–¹æ¡ˆ Bï¼šDocker Composeï¼ˆç”Ÿäº§æ¨èï¼‰

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: rag-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  rag-service:
    build: .
    container_name: rag-service
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped

volumes:
  redis-data:
```

```conf
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru
appendonly yes
appendfsync everysec
requirepass your-redis-password
bind 0.0.0.0
protected-mode yes
port 6379
```

#### æ–¹æ¡ˆ Cï¼šäº‘ Redisï¼ˆç”Ÿäº§æ¨èï¼‰

| æœåŠ¡å•† | äº§å“ | ä»·æ ¼ | è¯´æ˜ |
|-------|------|------|------|
| AWS | ElastiCache | ~$50/æœˆ | æ‰˜ç®¡æœåŠ¡ |
| é˜¿é‡Œäº‘ | Redis ç‰ˆ | ~Â¥300/æœˆ | å›½å†…æ¨è |
| è…¾è®¯äº‘ | CKV | ~Â¥280/æœˆ | å›½å†…å¤‡é€‰ |

---

### 2.3 é…ç½®æ–‡ä»¶

```python
# config.py
import os
from dotenv import load_dotenv

load_dotenv()

class CacheConfig:
    """LiteLLM ç¼“å­˜é…ç½®"""
    
    # Redis é…ç½®
    REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
    REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
    REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", "")
    REDIS_DB = int(os.getenv("REDIS_DB", 0))
    
    # ç¼“å­˜ TTLï¼ˆç§’ï¼‰
    CACHE_TTL = int(os.getenv("CACHE_TTL", 3600))  # 1 å°æ—¶
    
    # è¯­ä¹‰ç¼“å­˜é…ç½®
    SEMANTIC_CACHE_ENABLED = os.getenv("SEMANTIC_CACHE_ENABLED", "false").lower() == "true"
    SEMANTIC_THRESHOLD = float(os.getenv("SEMANTIC_THRESHOLD", 0.9))
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    
    # Provider é…ç½®
    ANTHROPIC_MODEL = os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
    
    # æ—¥å¿—é…ç½®
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# ç”Ÿäº§ç¯å¢ƒé…ç½®ç¤ºä¾‹
# .env.production
REDIS_HOST=redis.internal.company.com
REDIS_PORT=6379
REDIS_PASSWORD=strong-password-here
CACHE_TTL=7200  # 2 å°æ—¶
SEMANTIC_CACHE_ENABLED=true
SEMANTIC_THRESHOLD=0.85
ANTHROPIC_MODEL=claude-sonnet-4-20250514
OPENAI_MODEL=gpt-4o
LOG_LEVEL=WARNING
```

---

## 3. ä»£ç å®ç°

### 3.1 åŸºç¡€ç¼“å­˜å®ç°

```python
# cache_basic.py
import litellm
from litellm import completion
from config import CacheConfig

# é…ç½® Redis ç¼“å­˜
litellm.cache = litellm.CacheConfig(
    type="redis",
    host=f"redis://{CacheConfig.REDIS_HOST}:{CacheConfig.REDIS_PORT}",
    password=CacheConfig.REDIS_PASSWORD if CacheConfig.REDIS_PASSWORD else None,
    db=CacheConfig.REDIS_DB,
    ttl=CacheConfig.CACHE_TTL
)

def basic_rag_query(query: str, context: str) -> str:
    """
    åŸºç¡€ RAG æŸ¥è¯¢ï¼ˆå¯ç”¨ LiteLLM ç¼“å­˜ï¼‰
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£
    
    Returns:
        LLM ç”Ÿæˆçš„å›ç­”
    """
    response = completion(
        model=f"anthropic/{CacheConfig.ANTHROPIC_MODEL}",
        messages=[
            {
                "role": "user",
                "content": f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{query}"
            }
        ],
        max_tokens=1024,
        caching=True  # å¯ç”¨ç¼“å­˜
    )
    
    return response.choices[0].message.content

# æµ‹è¯•
if __name__ == "__main__":
    context = "å…¬å¸æ˜¯ 2020 å¹´æˆç«‹çš„ï¼Œä¸“æ³¨äº AI æŠ€æœ¯ç ”å‘..."
    query = "å…¬å¸ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ"
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    answer1 = basic_rag_query(query, context)
    print(f"ç­”æ¡ˆ 1: {answer1}")
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    answer2 = basic_rag_query(query, context)
    print(f"ç­”æ¡ˆ 2: {answer2}")
```

---

### 3.2 è¿›é˜¶ï¼šä¸‰å±‚ç¼“å­˜å®ç°

```python
# cache_advanced.py
import litellm
from litellm import completion
from config import CacheConfig
import time
import hashlib

class RAGCache:
    """RAG ä¸‰å±‚ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self):
        # é…ç½® LiteLLM Redis ç¼“å­˜
        litellm.cache = litellm.CacheConfig(
            type="redis",
            host=f"redis://{CacheConfig.REDIS_HOST}:{CacheConfig.REDIS_PORT}",
            password=CacheConfig.REDIS_PASSWORD if CacheConfig.REDIS_PASSWORD else None,
            db=CacheConfig.REDIS_DB,
            ttl=CacheConfig.CACHE_TTL
        )
        
        # è¯­ä¹‰ç¼“å­˜é…ç½®
        self.semantic_cache_config = {
            "type": "semantic",
            "similarity_threshold": CacheConfig.SEMANTIC_THRESHOLD,
            "embedding_model": CacheConfig.EMBEDDING_MODEL
        } if CacheConfig.SEMANTIC_CACHE_ENABLED else None
    
    def _generate_cache_key(self, query: str, context: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}:{context}"
        return f"rag:{hashlib.md5(content.encode()).hexdigest()}"
    
    def query(self, query: str, context: str, use_provider_cache: bool = True) -> dict:
        """
        RAG æŸ¥è¯¢ï¼ˆä¸‰å±‚ç¼“å­˜ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£
            use_provider_cache: æ˜¯å¦å¯ç”¨ Provider åŸç”Ÿç¼“å­˜
        
        Returns:
            dict: {
                "answer": str,  # å›ç­”
                "cache_hit": str,  # ç¼“å­˜å‘½ä¸­å±‚çº§ (redis/semantic/provider/miss)
                "latency_ms": float,  # å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
                "tokens": dict  # token ä½¿ç”¨ç»Ÿè®¡
            }
        """
        start_time = time.time()
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n\n{context}"
                    },
                    {
                        "type": "text",
                        "text": f"é—®é¢˜ï¼š{query}"
                    }
                ]
            }
        ]
        
        # æ„å»º completion å‚æ•°
        completion_kwargs = {
            "model": f"anthropic/{CacheConfig.ANTHROPIC_MODEL}",
            "messages": messages,
            "max_tokens": 1024,
            "caching": True,  # LiteLLM ç¼“å­˜
        }
        
        # Provider åŸç”Ÿç¼“å­˜ï¼ˆPrompt Cachingï¼‰
        if use_provider_cache:
            # ä¸ºä¸Šä¸‹æ–‡å†…å®¹æ·»åŠ  cache_control
            messages[0]["content"][0]["cache_control"] = {"type": "ephemeral"}
            
            # é€ä¼  Anthropic beta header
            completion_kwargs["extra_headers"] = {
                "anthropic-beta": "prompt-caching-2024-07-31"
            }
        
        # è¯­ä¹‰ç¼“å­˜
        if self.semantic_cache_config:
            completion_kwargs["cache"] = self.semantic_cache_config
        
        # æ‰§è¡ŒæŸ¥è¯¢
        response = completion(**completion_kwargs)
        
        # è®¡ç®—å»¶è¿Ÿ
        latency_ms = (time.time() - start_time) * 1000
        
        # æå–ç¼“å­˜ä¿¡æ¯
        cache_hit = self._extract_cache_info(response)
        
        return {
            "answer": response.choices[0].message.content,
            "cache_hit": cache_hit,
            "latency_ms": latency_ms,
            "tokens": {
                "input": response.usage.prompt_tokens,
                "output": response.usage.completion_tokens,
                "total": response.usage.total_tokens
            }
        }
    
    def _extract_cache_info(self, response) -> str:
        """ä»å“åº”ä¸­æå–ç¼“å­˜å‘½ä¸­ä¿¡æ¯"""
        # æ£€æŸ¥ LiteLLM ç¼“å­˜å‘½ä¸­
        if hasattr(response, '_hidden_params') and response._hidden_params.get('cache_hit'):
            return "redis"
        
        # æ£€æŸ¥è¯­ä¹‰ç¼“å­˜
        if hasattr(response, '_hidden_params'):
            metadata = response._hidden_params.get('metadata', {})
            if metadata.get('cache_hit') == 'semantic':
                return "semantic"
        
        # æ£€æŸ¥ Provider ç¼“å­˜
        if hasattr(response, 'usage'):
            # Anthropic ç¼“å­˜è¯»å–ä¼šæœ‰ cache_read_input_tokens
            if hasattr(response.usage, 'cache_read_input_tokens'):
                return "provider"
        
        return "miss"

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    cache = RAGCache()
    
    context = "å…¬å¸æ˜¯ 2020 å¹´æˆç«‹çš„ï¼Œæ€»éƒ¨ä½äºåŒ—äº¬..."
    query = "å…¬å¸ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ"
    
    # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    result1 = cache.query(query, context)
    print(f"æŸ¥è¯¢ 1: {result1['cache_hit']}, å»¶è¿Ÿï¼š{result1['latency_ms']:.0f}ms")
    
    # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆRedis ç¼“å­˜å‘½ä¸­ï¼‰
    result2 = cache.query(query, context)
    print(f"æŸ¥è¯¢ 2: {result2['cache_hit']}, å»¶è¿Ÿï¼š{result2['latency_ms']:.0f}ms")
    
    # ç›¸ä¼¼é—®é¢˜ï¼ˆè¯­ä¹‰ç¼“å­˜å¯èƒ½å‘½ä¸­ï¼‰
    result3 = cache.query("å…¬å¸æˆç«‹æ—¶é—´ï¼Ÿ", context)
    print(f"æŸ¥è¯¢ 3: {result3['cache_hit']}, å»¶è¿Ÿï¼š{result3['latency_ms']:.0f}ms")
```

---

### 3.3 ç”Ÿäº§å°è£…ï¼šFastAPI æœåŠ¡

```python
# app.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from cache_advanced import RAGCache
from config import CacheConfig
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=getattr(logging, CacheConfig.LOG_LEVEL))
logger = logging.getLogger(__name__)

app = FastAPI(title="RAG Cache Service", version="1.0.0")

# åˆå§‹åŒ–ç¼“å­˜
rag_cache = RAGCache()

class RAGQuery(BaseModel):
    query: str
    context: str
    use_provider_cache: bool = True

class RAGResponse(BaseModel):
    answer: str
    cache_hit: str
    latency_ms: float
    tokens: dict

@app.post("/rag/query", response_model=RAGResponse)
async def rag_query(request: RAGQuery):
    """
    RAG æŸ¥è¯¢æ¥å£
    
    ä¸‰å±‚ç¼“å­˜ï¼š
    1. Redis ç¼“å­˜ï¼ˆå®Œå…¨ç›¸åŒæŸ¥è¯¢ï¼‰
    2. è¯­ä¹‰ç¼“å­˜ï¼ˆç›¸ä¼¼é—®é¢˜ï¼‰
    3. Provider ç¼“å­˜ï¼ˆç›¸åŒæ–‡æ¡£ï¼‰
    """
    try:
        result = rag_cache.query(
            query=request.query,
            context=request.context,
            use_provider_cache=request.use_provider_cache
        )
        
        logger.info(f"æŸ¥è¯¢å®Œæˆï¼Œç¼“å­˜å‘½ä¸­ï¼š{result['cache_hit']}, å»¶è¿Ÿï¼š{result['latency_ms']:.0f}ms")
        
        return RAGResponse(**result)
    
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "version": "1.0.0"}

@app.get("/metrics")
async def metrics():
    """ç›‘æ§æŒ‡æ ‡ï¼ˆå¯æ‰©å±•ï¼‰"""
    return {
        "cache_enabled": True,
        "cache_ttl": CacheConfig.CACHE_TTL,
        "semantic_cache": CacheConfig.SEMANTIC_CACHE_ENABLED
    }

# è¿è¡Œï¼šuvicorn app:app --host 0.0.0.0 --port 8000
```

---

### 3.4 æ‰¹é‡å¤„ç†è„šæœ¬

```python
# batch_process.py
"""
æ‰¹é‡å¤„ç†è„šæœ¬ - ç”¨äºç¦»çº¿æ•°æ®å¤„ç†å’Œç¼“å­˜é¢„çƒ­
"""
import litellm
from cache_advanced import RAGCache
from config import CacheConfig
import json
import time
from concurrent.futures import ThreadPoolExecutor

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 10):
        self.cache = RAGCache()
        self.max_workers = max_workers
        self.stats = {
            "total": 0,
            "cache_hit": 0,
            "cache_miss": 0,
            "errors": 0,
            "total_latency_ms": 0
        }
    
    def process_query(self, item: dict) -> dict:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        try:
            result = self.cache.query(
                query=item["query"],
                context=item["context"],
                use_provider_cache=True
            )
            
            self.stats["total"] += 1
            if result["cache_hit"] != "miss":
                self.stats["cache_hit"] += 1
            else:
                self.stats["cache_miss"] += 1
            self.stats["total_latency_ms"] += result["latency_ms"]
            
            return {
                "query": item["query"],
                "answer": result["answer"],
                "cache_hit": result["cache_hit"],
                "latency_ms": result["latency_ms"]
            }
        
        except Exception as e:
            self.stats["errors"] += 1
            return {
                "query": item["query"],
                "error": str(e)
            }
    
    def process_batch(self, queries: list, output_file: str):
        """
        æ‰¹é‡å¤„ç†æŸ¥è¯¢
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨ [{"query": "...", "context": "..."}, ...]
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        results = []
        start_time = time.time()
        
        # çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self.process_query, q) for q in queries]
            
            for i, future in enumerate(futures):
                result = future.result()
                results.append(result)
                
                # è¿›åº¦æ—¥å¿—
                if (i + 1) % 100 == 0:
                    elapsed = time.time() - start_time
                    avg_latency = self.stats["total_latency_ms"] / max(1, self.stats["total"])
                    hit_rate = self.stats["cache_hit"] / max(1, self.stats["total"]) * 100
                    
                    print(f"è¿›åº¦ï¼š{i+1}/{len(queries)}, "
                          f"å‘½ä¸­ç‡ï¼š{hit_rate:.1f}%, "
                          f"å¹³å‡å»¶è¿Ÿï¼š{avg_latency:.0f}ms, "
                          f"è€—æ—¶ï¼š{elapsed:.1f}s")
        
        # ä¿å­˜ç»“æœ
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump({
                "results": results,
                "stats": self.stats,
                "summary": {
                    "total_queries": self.stats["total"],
                    "cache_hit_rate": f"{self.stats['cache_hit']/max(1, self.stats['total'])*100:.1f}%",
                    "avg_latency_ms": f"{self.stats['total_latency_ms']/max(1, self.stats['total']):.0f}",
                    "errors": self.stats["errors"],
                    "total_time_s": time.time() - start_time
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\næ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"æ€»æŸ¥è¯¢æ•°ï¼š{self.stats['total']}")
        print(f"ç¼“å­˜å‘½ä¸­ç‡ï¼š{self.stats['cache_hit']/max(1, self.stats['total'])*100:.1f}%")
        print(f"å¹³å‡å»¶è¿Ÿï¼š{self.stats['total_latency_ms']/max(1, self.stats['total']):.0f}ms")
        print(f"é”™è¯¯æ•°ï¼š{self.stats['errors']}")
        print(f"ç»“æœå·²ä¿å­˜è‡³ï¼š{output_file}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open("test_queries.json", "r", encoding="utf-8") as f:
        queries = json.load(f)
    
    # æ‰¹é‡å¤„ç†
    processor = BatchProcessor(max_workers=10)
    processor.process_batch(queries, "batch_results.json")
```

---

## 4. æµ‹è¯•æ–¹æ¡ˆ

### 4.1 å•å…ƒæµ‹è¯•

```python
# test_cache.py
import pytest
from cache_advanced import RAGCache

@pytest.fixture
def cache():
    return RAGCache()

def test_cache_hit(cache):
    """æµ‹è¯•ç¼“å­˜å‘½ä¸­"""
    context = "æµ‹è¯•æ–‡æ¡£å†…å®¹" * 100  # ç¡®ä¿è¶…è¿‡æœ€å°ç¼“å­˜é•¿åº¦
    query = "æµ‹è¯•é—®é¢˜"
    
    # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆæœªå‘½ä¸­ï¼‰
    result1 = cache.query(query, context)
    assert result1["cache_hit"] == "miss"
    
    # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆåº”å‘½ä¸­ï¼‰
    result2 = cache.query(query, context)
    assert result2["cache_hit"] in ["redis", "semantic", "provider"]
    assert result2["latency_ms"] < result1["latency_ms"]

def test_semantic_cache(cache):
    """æµ‹è¯•è¯­ä¹‰ç¼“å­˜"""
    context = "æµ‹è¯•æ–‡æ¡£" * 100
    
    # åŸå§‹é—®é¢˜
    result1 = cache.query("å…¬å¸æˆç«‹æ—¶é—´ï¼Ÿ", context)
    
    # ç›¸ä¼¼é—®é¢˜ï¼ˆåº”è§¦å‘è¯­ä¹‰ç¼“å­˜ï¼‰
    result2 = cache.query("å…¬å¸ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ", context)
    
    # ç­”æ¡ˆåº”ç›¸ä¼¼
    assert result1["answer"][:50] == result2["answer"][:50]

def test_provider_cache(cache):
    """æµ‹è¯• Provider ç¼“å­˜"""
    context = "é•¿æ–‡æ¡£" * 1000  # å¤§æ–‡æ¡£
    
    # ç›¸åŒæ–‡æ¡£ï¼Œä¸åŒé—®é¢˜
    result1 = cache.query("é—®é¢˜ 1", context, use_provider_cache=True)
    result2 = cache.query("é—®é¢˜ 2", context, use_provider_cache=True)
    
    # ç¬¬äºŒæ¬¡åº”å‘½ä¸­ Provider ç¼“å­˜
    assert result2["cache_hit"] == "provider"
    assert result2["tokens"]["input"] < result1["tokens"]["input"]  # è¾“å…¥ token å‡å°‘
```

### 4.2 æ€§èƒ½æµ‹è¯•

```python
# performance_test.py
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬
"""
import time
import statistics
from cache_advanced import RAGCache

def benchmark_cache(cache: RAGCache, query: str, context: str, iterations: int = 10):
    """
    åŸºå‡†æµ‹è¯•
    
    Returns:
        dict: æ€§èƒ½æŒ‡æ ‡
    """
    latencies = []
    cache_hits = []
    
    for i in range(iterations):
        result = cache.query(query, context)
        latencies.append(result["latency_ms"])
        cache_hits.append(result["cache_hit"])
    
    return {
        "min_ms": min(latencies),
        "max_ms": max(latencies),
        "avg_ms": statistics.mean(latencies),
        "median_ms": statistics.median(latencies),
        "p95_ms": sorted(latencies)[int(len(latencies) * 0.95)],
        "cache_hit_rate": f"{cache_hits.count('redis') + cache_hits.count('semantic') + cache_hits.count('provider')}/{iterations}",
        "cache_types": {
            "redis": cache_hits.count("redis"),
            "semantic": cache_hits.count("semantic"),
            "provider": cache_hits.count("provider"),
            "miss": cache_hits.count("miss")
        }
    }

if __name__ == "__main__":
    cache = RAGCache()
    
    # æµ‹è¯•åœºæ™¯ 1ï¼šå®Œå…¨ç›¸åŒæŸ¥è¯¢
    print("=" * 50)
    print("åœºæ™¯ 1ï¼šå®Œå…¨ç›¸åŒæŸ¥è¯¢ï¼ˆRedis ç¼“å­˜ï¼‰")
    print("=" * 50)
    result1 = benchmark_cache(cache, "å…¬å¸æˆç«‹æ—¶é—´ï¼Ÿ", "å…¬å¸æ–‡æ¡£" * 100)
    print(f"æœ€å°å»¶è¿Ÿï¼š{result1['min_ms']:.0f}ms")
    print(f"å¹³å‡å»¶è¿Ÿï¼š{result1['avg_ms']:.0f}ms")
    print(f"P95 å»¶è¿Ÿï¼š{result1['p95_ms']:.0f}ms")
    print(f"ç¼“å­˜å‘½ä¸­ï¼š{result1['cache_hit_rate']}")
    print(f"ç¼“å­˜ç±»å‹ï¼š{result1['cache_types']}")
    
    # æµ‹è¯•åœºæ™¯ 2ï¼šç›¸ä¼¼é—®é¢˜
    print("\n" + "=" * 50)
    print("åœºæ™¯ 2ï¼šç›¸ä¼¼é—®é¢˜ï¼ˆè¯­ä¹‰ç¼“å­˜ï¼‰")
    print("=" * 50)
    # ... ç±»ä¼¼æµ‹è¯•
    
    # æµ‹è¯•åœºæ™¯ 3ï¼šç›¸åŒæ–‡æ¡£ä¸åŒé—®é¢˜
    print("\n" + "=" * 50)
    print("åœºæ™¯ 3ï¼šç›¸åŒæ–‡æ¡£ä¸åŒé—®é¢˜ï¼ˆProvider ç¼“å­˜ï¼‰")
    print("=" * 50)
    # ... ç±»ä¼¼æµ‹è¯•
```

### 4.3 æµ‹è¯•æ•°æ®ç”Ÿæˆ

```python
# generate_test_data.py
"""
ç”Ÿæˆæµ‹è¯•æ•°æ®
"""
import json

test_queries = [
    {
        "query": "å…¬å¸ä»€ä¹ˆæ—¶å€™æˆç«‹çš„ï¼Ÿ",
        "context": "å…¬å¸æˆç«‹äº 2020 å¹´ï¼Œæ€»éƒ¨ä½äºåŒ—äº¬ä¸­å…³æ‘..." * 50
    },
    {
        "query": "å…¬å¸çš„ä¸»è¦äº§å“æ˜¯ä»€ä¹ˆï¼Ÿ",
        "context": "å…¬å¸ä¸»è¦äº§å“åŒ…æ‹¬ AI åŠ©æ‰‹ã€RAG ç³»ç»Ÿã€æ•°æ®åˆ†æå¹³å°..." * 50
    },
    {
        "query": "å¦‚ä½•ä¼˜åŒ– RAG ç³»ç»Ÿçš„æ£€ç´¢é€Ÿåº¦ï¼Ÿ",
        "context": "RAG ç³»ç»Ÿä¼˜åŒ–æ–¹æ³•ï¼š1.ä½¿ç”¨å‘é‡æ•°æ®åº“ 2.å®ç°ç¼“å­˜å±‚ 3.ä¼˜åŒ–æ£€ç´¢ç­–ç•¥..." * 50
    },
    # ... æ›´å¤šæµ‹è¯•æŸ¥è¯¢
]

with open("test_queries.json", "w", encoding="utf-8") as f:
    json.dump(test_queries, f, ensure_ascii=False, indent=2)

print(f"å·²ç”Ÿæˆ {len(test_queries)} æ¡æµ‹è¯•æ•°æ®")
```

---

## 5. ç›‘æ§æ–¹æ¡ˆ

### 5.1 ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | å‘Šè­¦é˜ˆå€¼ |
|------|------|---------|
| **Cache Hit Rate** | ç¼“å­˜å‘½ä¸­ç‡ | <30% |
| **å¹³å‡å»¶è¿Ÿ** | å¹³å‡å“åº”æ—¶é—´ | >3s |
| **P95 å»¶è¿Ÿ** | 95% è¯·æ±‚å»¶è¿Ÿ | >5s |
| **é”™è¯¯ç‡** | è¯·æ±‚å¤±è´¥ç‡ | >1% |
| **Redis å†…å­˜ä½¿ç”¨** | ç¼“å­˜å ç”¨å†…å­˜ | >80% |
| **Token ä½¿ç”¨é‡** | LLM Token æ¶ˆè€— | çªå¢ 50% |

### 5.2 Prometheus + Grafana

```python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰æŒ‡æ ‡
CACHE_REQUESTS = Counter('rag_cache_requests_total', 'Total cache requests', ['cache_type'])
CACHE_LATENCY = Histogram('rag_cache_latency_seconds', 'Cache latency', ['cache_type'])
CACHE_HITS = Counter('rag_cache_hits_total', 'Total cache hits', ['cache_type'])
TOKENS_USED = Counter('rag_tokens_total', 'Total tokens used', ['type'])

class MetricsCollector:
    def __init__(self):
        start_http_server(8000)  # Prometheus æŠ“å–ç«¯å£
    
    def record_request(self, result: dict):
        """è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        cache_type = result.get('cache_hit', 'miss')
        CACHE_REQUESTS.labels(cache_type=cache_type).inc()
        CACHE_LATENCY.labels(cache_type=cache_type).observe(result['latency_ms'] / 1000)
        
        if cache_type != 'miss':
            CACHE_HITS.labels(cache_type=cache_type).inc()
        
        TOKENS_USED.labels(type='input').inc(result['tokens']['input'])
        TOKENS_USED.labels(type='output').inc(result['tokens']['output'])

# ä½¿ç”¨
metrics = MetricsCollector()
result = cache.query(query, context)
metrics.record_request(result)
```

### 5.3 æ—¥å¿—è®°å½•

```python
# logging_config.py
import logging
import json
from datetime import datetime

class CacheLogger:
    def __init__(self, log_file: str = "cache.log"):
        self.logger = logging.getLogger("rag_cache")
        self.logger.setLevel(logging.INFO)
        
        handler = logging.FileHandler(log_file)
        handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
        self.logger.addHandler(handler)
    
    def log_query(self, result: dict):
        """è®°å½•æŸ¥è¯¢æ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "cache_hit": result["cache_hit"],
            "latency_ms": result["latency_ms"],
            "tokens": result["tokens"],
            "cost_estimate": self._estimate_cost(result["tokens"])
        }
        self.logger.info(json.dumps(log_entry))
    
    def _estimate_cost(self, tokens: dict) -> float:
        """ä¼°ç®—æˆæœ¬ï¼ˆClaude Sonnet å®šä»·ï¼‰"""
        input_cost = tokens["input"] * 3 / 1_000_000
        output_cost = tokens["output"] * 15 / 1_000_000
        return input_cost + output_cost

# ä½¿ç”¨
logger = CacheLogger()
result = cache.query(query, context)
logger.log_query(result)
```

---

## 6. æ•…éšœæ’æŸ¥

### 6.1 å¸¸è§é—®é¢˜

| é—®é¢˜ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|---------|---------|
| **ç¼“å­˜ä¸å‘½ä¸­** | Redis è¿æ¥å¤±è´¥ | æ£€æŸ¥ Redis æœåŠ¡å’Œç½‘ç»œ |
| **ç¼“å­˜å‘½ä¸­ç‡ä½** | TTL å¤ªçŸ­ | å¢åŠ  CACHE_TTL |
| **å»¶è¿Ÿæœªé™ä½** | ä¸Šä¸‹æ–‡å¤ªçŸ­ | ç¡®ä¿>1K tokensï¼ˆæœ€å°ç¼“å­˜é•¿åº¦ï¼‰ |
| **Provider ç¼“å­˜æœªç”Ÿæ•ˆ** | æœªæ·»åŠ  beta header | æ£€æŸ¥ extra_headers é…ç½® |
| **Redis å†…å­˜æº¢å‡º** | ç¼“å­˜æ•°æ®è¿‡å¤š | é…ç½® maxmemory å’Œæ·˜æ±°ç­–ç•¥ |

### 6.2 è¯Šæ–­è„šæœ¬

```python
# diagnose.py
"""
ç¼“å­˜ç³»ç»Ÿè¯Šæ–­è„šæœ¬
"""
import redis
import litellm
from config import CacheConfig

def diagnose():
    print("=" * 50)
    print("LiteLLM ç¼“å­˜ç³»ç»Ÿè¯Šæ–­")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ Redis è¿æ¥
    print("\n1. Redis è¿æ¥æ£€æŸ¥")
    try:
        r = redis.Redis(
            host=CacheConfig.REDIS_HOST,
            port=CacheConfig.REDIS_PORT,
            password=CacheConfig.REDIS_PASSWORD
        )
        r.ping()
        print(f"   âœ… Redis è¿æ¥æˆåŠŸ ({CacheConfig.REDIS_HOST}:{CacheConfig.REDIS_PORT})")
        
        # Redis ä¿¡æ¯
        info = r.info()
        print(f"   - Redis ç‰ˆæœ¬ï¼š{info['redis_version']}")
        print(f"   - å·²ç”¨å†…å­˜ï¼š{info['used_memory_human']}")
        print(f"   - è¿æ¥æ•°ï¼š{info['connected_clients']}")
    
    except Exception as e:
        print(f"   âŒ Redis è¿æ¥å¤±è´¥ï¼š{str(e)}")
        return
    
    # 2. æ£€æŸ¥ LiteLLM ç¼“å­˜é…ç½®
    print("\n2. LiteLLM ç¼“å­˜é…ç½®")
    if litellm.cache:
        print(f"   âœ… ç¼“å­˜å·²å¯ç”¨")
        print(f"   - ç±»å‹ï¼š{litellm.cache.type}")
        print(f"   - TTL: {litellm.cache.ttl}s")
    else:
        print(f"   âŒ ç¼“å­˜æœªå¯ç”¨")
    
    # 3. æµ‹è¯•ç¼“å­˜å†™å…¥
    print("\n3. ç¼“å­˜å†™å…¥æµ‹è¯•")
    try:
        r.set("test:key", "test_value", ex=60)
        value = r.get("test:key")
        if value == b"test_value":
            print(f"   âœ… ç¼“å­˜å†™å…¥/è¯»å–æˆåŠŸ")
        else:
            print(f"   âŒ ç¼“å­˜è¯»å–å¤±è´¥")
    except Exception as e:
        print(f"   âŒ ç¼“å­˜æµ‹è¯•å¤±è´¥ï¼š{str(e)}")
    
    # 4. æ£€æŸ¥ API Keys
    print("\n4. API Keys æ£€æŸ¥")
    import os
    if os.getenv("ANTHROPIC_API_KEY"):
        print(f"   âœ… Anthropic API Key å·²é…ç½®")
    else:
        print(f"   âŒ Anthropic API Key æœªé…ç½®")
    
    if os.getenv("OPENAI_API_KEY"):
        print(f"   âœ… OpenAI API Key å·²é…ç½®")
    else:
        print(f"   âŒ OpenAI API Key æœªé…ç½®")
    
    print("\n" + "=" * 50)
    print("è¯Šæ–­å®Œæˆ")
    print("=" * 50)

if __name__ == "__main__":
    diagnose()
```

---

## 7. æˆæœ¬ä¼°ç®—

### 7.1 å‡è®¾æ¡ä»¶

| å‚æ•° | å€¼ |
|------|-----|
| æ—¥æŸ¥è¯¢é‡ | 10,000 æ¬¡ |
| å¹³å‡è¾“å…¥ tokens | 10,000 |
| å¹³å‡è¾“å‡º tokens | 2,000 |
| æ¨¡å‹ | Claude Sonnet |
| å·¥ä½œæ—¥ | 22 å¤©/æœˆ |

### 7.2 æˆæœ¬å¯¹æ¯”

#### æ— ç¼“å­˜

```
è¾“å…¥æˆæœ¬ï¼š10,000 æŸ¥è¯¢ Ã— 22 å¤© Ã— 10K tokens Ã— $3/1M = $6,600/æœˆ
è¾“å‡ºæˆæœ¬ï¼š10,000 æŸ¥è¯¢ Ã— 22 å¤© Ã— 2K tokens Ã— $15/1M = $6,600/æœˆ
æ€»è®¡ï¼š$13,200/æœˆ
```

#### æœ‰ç¼“å­˜ï¼ˆ70% å‘½ä¸­ç‡ï¼‰

```
LLM è°ƒç”¨ï¼š10,000 Ã— 22 Ã— 30% = 66,000 æ¬¡/æœˆ

è¾“å…¥æˆæœ¬ï¼š66,000 Ã— 10K tokens Ã— $3/1M = $1,980/æœˆ
è¾“å‡ºæˆæœ¬ï¼š66,000 Ã— 2K tokens Ã— $15/1M = $1,980/æœˆ

ç¼“å­˜æˆæœ¬ï¼š
- Redis: ~$50/æœˆ
- Embeddingï¼ˆè¯­ä¹‰ç¼“å­˜ï¼‰: ~$20/æœˆ

æ€»è®¡ï¼š$4,030/æœˆ

èŠ‚çœï¼š$13,200 - $4,030 = $9,170/æœˆ (69.5%)
```

### 7.3 ROI åˆ†æ

| é¡¹ç›® | æˆæœ¬ |
|------|------|
| **å®æ–½æˆæœ¬** | |
| - å¼€å‘æ—¶é—´ï¼ˆ3 å¤©ï¼‰ | Â¥15,000 |
| - æµ‹è¯•æ—¶é—´ï¼ˆ2 å¤©ï¼‰ | Â¥10,000 |
| - éƒ¨ç½²æ—¶é—´ï¼ˆ1 å¤©ï¼‰ | Â¥5,000 |
| **æœˆåº¦æˆæœ¬** | |
| - Redis æœåŠ¡å™¨ | Â¥350 |
| - åº”ç”¨æœåŠ¡å™¨ | Â¥500 |
| - Embedding API | Â¥150 |
| **æœˆåº¦èŠ‚çœ** | **Â¥66,000** ($9,170) |
| **æŠ•èµ„å›æ”¶æœŸ** | **<1 å‘¨** |

---

## 8. éƒ¨ç½²æ£€æŸ¥æ¸…å•

### 8.1 éƒ¨ç½²å‰

- [ ] Redis æœåŠ¡å™¨å·²éƒ¨ç½²å¹¶æµ‹è¯•
- [ ] API Keys å·²é…ç½®
- [ ] é…ç½®æ–‡ä»¶å·²æ›´æ–°
- [ ] æµ‹è¯•æ•°æ®å·²å‡†å¤‡
- [ ] ç›‘æ§å·²é…ç½®

### 8.2 éƒ¨ç½²ä¸­

- [ ] ä»£ç å·²éƒ¨ç½²åˆ°æœåŠ¡å™¨
- [ ] ä¾èµ–å·²å®‰è£…
- [ ] æœåŠ¡å·²å¯åŠ¨
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] æ—¥å¿—æ­£å¸¸

### 8.3 éƒ¨ç½²å

- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡
- [ ] ç¼“å­˜å‘½ä¸­ç‡>30%
- [ ] å¹³å‡å»¶è¿Ÿ<3s
- [ ] é”™è¯¯ç‡<1%
- [ ] ç›‘æ§å‘Šè­¦é…ç½®å®Œæˆ

---

## 9. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### æœ¬å‘¨ï¼ˆPOC é˜¶æ®µï¼‰

- [ ] éƒ¨ç½² Redisï¼ˆDockerï¼‰
- [ ] å®ç°åŸºç¡€ç¼“å­˜ä»£ç 
- [ ] è¿è¡Œæ€§èƒ½æµ‹è¯•
- [ ] ç¼–å†™æµ‹è¯•æŠ¥å‘Š

### ä¸‹å‘¨ï¼ˆè¯•ç‚¹é˜¶æ®µï¼‰

- [ ] é›†æˆåˆ°ç°æœ‰ RAG ç³»ç»Ÿ
- [ ] é…ç½®ç›‘æ§å‘Šè­¦
- [ ] å°æµé‡æµ‹è¯•ï¼ˆ10%ï¼‰
- [ ] æ”¶é›†æ€§èƒ½æ•°æ®

### ä¸‹æœˆï¼ˆç”Ÿäº§é˜¶æ®µï¼‰

- [ ] å…¨é‡ä¸Šçº¿
- [ ] ä¼˜åŒ–ç¼“å­˜ç­–ç•¥
- [ ] ç¼–å†™è¿ç»´æ–‡æ¡£
- [ ] å›¢é˜ŸåŸ¹è®­

---

## ğŸ”— ç›¸å…³èµ„æº

| èµ„æº | é“¾æ¥ |
|------|------|
| LiteLLM æ–‡æ¡£ | https://docs.litellm.ai |
| LiteLLM ç¼“å­˜ | https://docs.litellm.ai/docs/caching |
| Redis æ–‡æ¡£ | https://redis.io/docs |
| Anthropic Prompt Caching | https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching |

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š1.0  
**æœ€åæ›´æ–°**ï¼š2026-02-28  
**è´Ÿè´£äºº**ï¼šEddy
