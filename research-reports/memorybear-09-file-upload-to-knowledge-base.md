---
tags: [memorybear, rag, file-upload, knowledge-base, chunking]
created: 2026-02-28
type: technical-analysis
status: draft
---

# MemoryBear - æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“å®Œæ•´æµç¨‹åˆ†æ

**ç ”ç©¶æ—¥æœŸ**ï¼š2026-02-28  
**ç ”ç©¶ç›®æ ‡**ï¼šç†è§£ MemoryBear å¦‚ä½•å¤„ç†ä¸Šä¼ æ–‡ä»¶å¹¶è½¬ä¸ºçŸ¥è¯†åº“  
**æ ¸å¿ƒæ–‡ä»¶**ï¼š`extraction_orchestrator.py`ï¼ˆ93KB, 1900+ è¡Œï¼‰

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### å®Œæ•´æµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
    â†“
1. æ–‡ä»¶å­˜å‚¨ï¼ˆOSS/S3/Localï¼‰
    â†“
2. æ–‡ä»¶è§£æï¼ˆPDF/Word/Excel ç­‰ï¼‰
    â†“
3. æ•°æ®é¢„å¤„ç†ï¼ˆæ¸…æ´—ã€æ ¼å¼åŒ–ï¼‰
    â†“
4. åˆ†å—ï¼ˆChunkingï¼‰
    â†“
5. çŸ¥è¯†æå–ï¼ˆé™ˆè¿°å¥/ä¸‰å…ƒç»„/æ—¶é—´ä¿¡æ¯ï¼‰
    â†“
6. åµŒå…¥å‘é‡ç”Ÿæˆ
    â†“
7. å»é‡æ¶ˆæ­§
    â†“
8. å†™å…¥ Neo4j çŸ¥è¯†å›¾è°±
    â†“
9. å†™å…¥å‘é‡æ•°æ®åº“
    â†“
çŸ¥è¯†åº“å¯ç”¨
```

---

## ğŸ—ï¸ æ ¸å¿ƒç»„ä»¶

### 1. æ–‡ä»¶è§£æå™¨ï¼ˆParsersï¼‰

**ä½ç½®**ï¼š`api/app/core/rag/deepdoc/parser/`

**æ”¯æŒçš„æ ¼å¼**ï¼š

| è§£æå™¨ | æ–‡ä»¶ | ä»£ç è¡Œ | è¯´æ˜ |
|--------|------|--------|------|
| **PDFParser** | pdf_parser.py | 56KB | æœ€å¤æ‚ï¼Œæ”¯æŒ OCRã€è¡¨æ ¼ã€å›¾è¡¨ |
| **MinerUParser** | mineru_parser.py | 21KB | MagicData é«˜ç²¾åº¦è§£æ |
| **ExcelParser** | excel_parser.py | 10KB | è¡¨æ ¼æ•°æ®å¤„ç† |
| **HTMLParser** | html_parser.py | 8KB | ç½‘é¡µå†…å®¹è§£æ |
| **DocxParser** | docx_parser.py | 4KB | Word æ–‡æ¡£ |
| **PPTParser** | ppt_parser.py | 3KB | PowerPoint |
| **JSONParser** | json_parser.py | 6KB | JSON/JSONL |
| **MarkdownParser** | markdown_parser.py | 10KB | Markdown æ–‡æ¡£ |
| **TxTParser** | txt_parser.py | 1KB | çº¯æ–‡æœ¬ |

---

### 2. æ•°æ®é¢„å¤„ç†å™¨

**ä½ç½®**ï¼š`api/app/core/memory/storage_services/extraction_engine/data_preprocessing/data_preprocessor.py`

**åŠŸèƒ½**ï¼š
- æ”¯æŒå¤šç§æ ¼å¼ï¼šJSON, CSV, Excel, TXT
- è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç¼–ç 
- æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®
- è½¬æ¢ä¸º DialogData å¯¹è±¡

**æ ¸å¿ƒæ–¹æ³•**ï¼š
```python
class DataPreprocessor:
    def __init__(self, input_file_path: str = None, output_file_path: str = None):
        self.supported_formats = ['.json', '.csv', '.txt', '.xlsx', '.tsv']
    
    def preprocess(self, input_file_path: str = None, output_file_path: str = None) -> List[DialogData]:
        # 1. æ£€æµ‹æ–‡ä»¶æ ¼å¼
        file_format = self.get_file_format(input_file_path)
        
        # 2. æ£€æµ‹ç¼–ç 
        encoding = self._detect_encoding(input_file_path)
        
        # 3. æ ¹æ®æ ¼å¼è¯»å–
        if file_format == '.json':
            data = self._read_json(input_file_path)
        elif file_format == '.csv':
            data = self._read_csv(input_file_path)
        # ...
        
        # 4. è½¬æ¢ä¸º DialogData
        dialogs = self._convert_to_dialog_data(data)
        
        return dialogs
```

---

### 3. åˆ†å—å™¨ï¼ˆChunkerï¼‰

**ä½ç½®**ï¼š`api/app/core/memory/llm_tools/chunker_client.py`

**æ³¨æ„**ï¼š`data_chunker.py` æ˜¯å ä½ç¬¦ï¼Œå®é™…å®ç°åœ¨ `chunker_client.py`

**æ”¯æŒ 7 ç§åˆ†å—ç­–ç•¥**ï¼š

| ç­–ç•¥ | å®ç° | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| **TokenChunker** | chonkie.TokenChunker | ç®€å•å¿«é€Ÿ |
| **SemanticChunker** | chonkie.SemanticChunker | è¯­ä¹‰è¿è´¯ |
| **RecursiveChunker** | chonkie.RecursiveChunker | ç»“æ„åŒ–æ–‡æœ¬ |
| **LateChunker** | chonkie.LateChunker | é•¿æ–‡æ¡£ |
| **NeuralChunker** | chonkie.NeuralChunker | é«˜è´¨é‡ |
| **LLMChunker** | è‡ªå®šä¹‰ LLM åˆ†å— | æ™ºèƒ½åˆ†å— |
| **HybridChunker** | è‡ªå®šä¹‰æ··åˆç­–ç•¥ | å¹³è¡¡æ•ˆæœ |

**é…ç½®æ¨¡å‹**ï¼š
```python
class ChunkerConfig(BaseModel):
    chunker_strategy: str = "RecursiveChunker"
    embedding_model: str
    chunk_size: int = 2048
    threshold: float = 0.8
    language: str = "zh"
```

---

### 4. çŸ¥è¯†æå–ç¼–æ’å™¨

**ä½ç½®**ï¼š`api/app/core/memory/storage_services/extraction_engine/extraction_orchestrator.py`

**æ ¸å¿ƒç±»**ï¼š`ExtractionOrchestrator`ï¼ˆ93KB, 1900+ è¡Œï¼‰

**èŒè´£**ï¼šåè°ƒæ•´ä¸ªçŸ¥è¯†æå–æµç¨‹

---

## ğŸ“‹ å®Œæ•´å¤„ç†æµç¨‹

### é˜¶æ®µ 1ï¼šæ–‡ä»¶ä¸Šä¼ ä¸å­˜å‚¨

**æµç¨‹**ï¼š
```
ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶
    â†“
FileController.receive_file()
    â†“
FileStorageService.save_file()
    â†“
å­˜å‚¨ç­–ç•¥ï¼ˆLocal/OSS/S3ï¼‰
    â†“
è¿”å› file_url / file_id
```

**ä»£ç ä½ç½®**ï¼š
- Controller: `api/app/controllers/file_controller.py`
- Service: `api/app/services/file_storage_service.py`
- Storage: `api/app/core/storage/` (local.py, oss.py, s3.py)

---

### é˜¶æ®µ 2ï¼šæ–‡ä»¶è§£æ

**PDF è§£æç¤ºä¾‹**ï¼ˆæœ€å¤æ‚çš„åœºæ™¯ï¼‰ï¼š

```python
# api/app/core/rag/deepdoc/parser/pdf_parser.py

class PDFParser:
    def parse(self, pdf_path: str, callback=None) -> Dict:
        """
        è§£æ PDF æ–‡ä»¶
        
        è¿”å›ï¼š
        {
            "pages": [...],      # æ¯é¡µå†…å®¹
            "tables": [...],     # è¡¨æ ¼æ•°æ®
            "figures": [...],    # å›¾è¡¨æ•°æ®
            "text": "...",       # å®Œæ•´æ–‡æœ¬
            "layout": [...]      # ç‰ˆé¢åˆ†æ
        }
        """
        # 1. PDF åŠ è½½
        doc = fitz.open(pdf_path)
        
        # 2. é€é¡µè§£æ
        for page in doc:
            # OCR è¯†åˆ«ï¼ˆå¦‚æœæ˜¯æ‰«æç‰ˆï¼‰
            if self.need_ocr(page):
                text = self.ocr_page(page)
            else:
                text = page.get_text()
            
            # è¡¨æ ¼æ£€æµ‹
            tables = self.detect_tables(page)
            
            # å›¾è¡¨æ£€æµ‹
            figures = self.detect_figures(page)
            
            # ç‰ˆé¢åˆ†æ
            layout = self.analyze_layout(page)
        
        # 3. åå¤„ç†
        result = self.postprocess({
            "text": text,
            "tables": tables,
            "figures": figures,
            "layout": layout
        })
        
        return result
```

**å…¶ä»–æ ¼å¼**ï¼š
- Word: `docx_parser.py` â†’ python-docx
- Excel: `excel_parser.py` â†’ openpyxl/pandas
- HTML: `html_parser.py` â†’ BeautifulSoup

---

### é˜¶æ®µ 3ï¼šæ•°æ®é¢„å¤„ç†

**æµç¨‹**ï¼š
```
è§£æåçš„æ–‡æœ¬/æ•°æ®
    â†“
DataPreprocessor.preprocess()
    â†“
1. ç¼–ç æ£€æµ‹ï¼ˆchardetï¼‰
    â†“
2. æ•°æ®è¯»å–ï¼ˆJSON/CSV/TXTï¼‰
    â†“
3. æ•°æ®æ¸…æ´—
   - å»é™¤ç©ºç™½å­—ç¬¦
   - æ ‡å‡†åŒ–æ ¼å¼
   - ä¿®å¤ç¼–ç é—®é¢˜
    â†“
4. è½¬æ¢ä¸º DialogData
    â†“
List[DialogData]
```

**DialogData ç»“æ„**ï¼š
```python
class DialogData:
    ref_id: str              # å¼•ç”¨ ID
    content: str             # å®Œæ•´å†…å®¹
    context: ConversationContext
    chunks: List[Chunk]      # åˆ†å—ç»“æœï¼ˆåç»­å¡«å……ï¼‰
    metadata: Dict           # å…ƒæ•°æ®
```

---

### é˜¶æ®µ 4ï¼šåˆ†å—ï¼ˆChunkingï¼‰

**æµç¨‹**ï¼š
```
List[DialogData]
    â†“
ChunkerClient.generate_chunks()
    â†“
éå†æ¯ä¸ªæ¶ˆæ¯/æ®µè½
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å†…å®¹é•¿åº¦ > chunk_size?       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚ æ˜¯    â”‚  å¦
    â†“       â†“
ä½¿ç”¨åˆ†å—ç­–ç•¥  ç›´æ¥ä½œä¸º chunk
è¿›ä¸€æ­¥åˆ†å—    
    â†“       â†“
åˆ›å»º chunks  åˆ›å»º chunk
    â†“
æ·»åŠ  metadata
    â†“
è¿”å› chunked_dialogs
```

**ä»£ç **ï¼š
```python
# api/app/core/memory/llm_tools/chunker_client.py

class ChunkerClient:
    async def generate_chunks(self, dialogue: DialogData) -> DialogData:
        dialogue.chunks = []
        
        for msg_idx, msg in enumerate(dialogue.context.msgs):
            msg_content = msg.msg.strip()
            
            if len(msg_content) > self.chunk_size:
                # æ¶ˆæ¯å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å—
                sub_chunks = self.chunker(msg_content)
                for idx, sub_chunk in enumerate(sub_chunks):
                    chunk = Chunk(
                        content=f"{msg.role}: {sub_chunk.text}",
                        speaker=msg.role,
                        metadata={
                            "message_index": msg_idx,
                            "sub_chunk_index": idx,
                            "chunker_strategy": self.chunker_config.chunker_strategy,
                        }
                    )
                    dialogue.chunks.append(chunk)
            else:
                # ç›´æ¥ä½œä¸º chunk
                chunk = Chunk(
                    content=f"{msg.role}: {msg_content}",
                    speaker=msg.role,
                    metadata={
                        "message_index": msg_idx,
                        "chunker_strategy": self.chunker_config.chunker_strategy,
                    }
                )
                dialogue.chunks.append(chunk)
        
        return dialogue
```

---

### é˜¶æ®µ 5ï¼šçŸ¥è¯†æå–ï¼ˆæ ¸å¿ƒï¼‰

**ExtractionOrchestrator.run()** åè°ƒæ•´ä¸ªæµç¨‹ï¼š

```python
# api/app/core/memory/storage_services/extraction_engine/extraction_orchestrator.py

async def run(
    self,
    dialog_data_list: List[DialogData],
    is_pilot_run: bool = False,
) -> Tuple[...]:
    """
    è¿è¡Œå®Œæ•´çš„çŸ¥è¯†æå–æµæ°´çº¿
    """
    logger.info(f"å¼€å§‹çŸ¥è¯†æå–æµæ°´çº¿ï¼Œå…± {len(dialog_data_list)} ä¸ªå¯¹è¯")
    
    # ========== æ­¥éª¤ 1: é™ˆè¿°å¥æå– ==========
    statement_tasks = []
    for dialog in dialog_data_list:
        for chunk in dialog.chunks:
            task = self.statement_extractor.process_chunk(chunk)
            statement_tasks.append(task)
    
    statement_results = await asyncio.gather(*statement_tasks)
    
    # ========== æ­¥éª¤ 2: å¹¶è¡Œæ‰§è¡Œï¼ˆä¼˜åŒ–ç‚¹ï¼‰==========
    # 2a. ä¸‰å…ƒç»„æå–
    triplet_task = self._extract_triplets(dialog_data_list)
    
    # 2b. æ—¶é—´ä¿¡æ¯æå–
    temporal_task = self._extract_temporal_info(dialog_data_list)
    
    # 2c. é™ˆè¿°å¥/åˆ†å—åµŒå…¥ç”Ÿæˆ
    embedding_task = self._generate_statement_embeddings(dialog_data_list)
    
    # å¹¶è¡Œæ‰§è¡Œ
    triplet_results, temporal_results, embedding_results = await asyncio.gather(
        triplet_task,
        temporal_task,
        embedding_task,
    )
    
    # ========== æ­¥éª¤ 3: å®ä½“åµŒå…¥ç”Ÿæˆï¼ˆä¾èµ–ä¸‰å…ƒç»„ï¼‰==========
    entity_embeddings = await self._generate_entity_embeddings(triplet_results)
    
    # ========== æ­¥éª¤ 4: æ•°æ®èµ‹å€¼ ==========
    # å°†ä¸‰å…ƒç»„ã€æ—¶é—´ä¿¡æ¯èµ‹å€¼åˆ°é™ˆè¿°å¥
    
    # ========== æ­¥éª¤ 5: åˆ›å»ºèŠ‚ç‚¹å’Œè¾¹ ==========
    chunk_nodes = self._create_chunk_nodes(dialog_data_list)
    statement_nodes = self._create_statement_nodes(statement_results)
    entity_nodes = self._create_entity_nodes(triplet_results)
    
    # ========== æ­¥éª¤ 6: ä¸¤é˜¶æ®µå»é‡æ¶ˆæ­§ ==========
    deduped_entities, merge_records = await dedup_layers_and_merge_and_return(
        entity_nodes,
        statement_entity_edges,
        entity_entity_edges,
        llm_client=self.llm_client,
        connector=self.connector,
    )
    
    # ========== æ­¥éª¤ 7: å†™å…¥æ•°æ®åº“ ==========
    if not is_pilot_run:
        # å†™å…¥ Neo4j
        await self._write_nodes_to_neo4j(chunk_nodes, statement_nodes, deduped_entities)
        await self._write_edges_to_neo4j(statement_entity_edges, entity_entity_edges)
        
        # å†™å…¥å‘é‡æ•°æ®åº“
        await self._write_embeddings_to_vector_db(embedding_results)
    
    # ========== æ­¥éª¤ 8: ç”Ÿæˆæ‘˜è¦ ==========
    await self._generate_knowledge_base_summary()
    
    return (
        (dialog_nodes, chunk_nodes, statement_nodes),
        (entity_nodes, statement_entity_edges, entity_entity_edges),
        (deduped_entities, statement_entity_edges, entity_entity_edges),
    )
```

---

### é˜¶æ®µ 6ï¼šçŸ¥è¯†æå–å­æµç¨‹

#### 6.1 é™ˆè¿°å¥æå–ï¼ˆStatement Extractionï¼‰

**ä½ç½®**ï¼š`knowledge_extraction/statement_extraction.py`

**åŠŸèƒ½**ï¼šä» chunk ä¸­æå–ç»“æ„åŒ–é™ˆè¿°å¥

**Prompt ç¤ºä¾‹**ï¼š
```
ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®é™ˆè¿°å¥ï¼š

æ–‡æœ¬ï¼š{chunk_content}

è¦æ±‚ï¼š
1. æ¯ä¸ªé™ˆè¿°å¥è¡¨è¾¾ä¸€ä¸ªå®Œæ•´çš„äº‹å®
2. ä¿æŒå®¢è§‚ï¼Œä¸æ·»åŠ ä¸»è§‚åˆ¤æ–­
3. ä½¿ç”¨ç®€æ´çš„è¯­è¨€
4. è¾“å‡º JSON æ ¼å¼

è¾“å‡ºæ ¼å¼ï¼š
{
    "statements": [
        {"text": "...", "subject": "...", "predicate": "...", "object": "..."},
        ...
    ]
}
```

---

#### 6.2 ä¸‰å…ƒç»„æå–ï¼ˆTriplet Extractionï¼‰

**ä½ç½®**ï¼š`knowledge_extraction/triplet_extraction.py`

**åŠŸèƒ½**ï¼šæå– (ä¸»ä½“ï¼Œè°“è¯ï¼Œå®¢ä½“) ä¸‰å…ƒç»„

**ä»£ç **ï¼š
```python
class TripletExtractor:
    async def extract(self, text: str) -> List[Triplet]:
        prompt = f"""
        ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å…³ç³»ä¸‰å…ƒç»„ï¼š
        
        æ–‡æœ¬ï¼š{text}
        
        æœ¬ä½“ç±»å‹ï¼š{self.ontology_types}
        
        è¾“å‡ºæ ¼å¼ï¼š
        {{
            "triplets": [
                {{"head": "å®ä½“ 1", "relation": "å…³ç³»", "tail": "å®ä½“ 2"}},
                ...
            ]
        }}
        """
        
        response = await self.llm_client.chat(prompt)
        triplets = self.parse_response(response)
        
        return triplets
```

---

#### 6.3 æ—¶é—´ä¿¡æ¯æå–ï¼ˆTemporal Extractionï¼‰

**ä½ç½®**ï¼š`knowledge_extraction/temporal_extraction.py`

**åŠŸèƒ½**ï¼šæå–æ—¶é—´è¡¨è¾¾å¼å’Œæ—¶æ•ˆæ€§ä¿¡æ¯

**æå–å†…å®¹**ï¼š
- æ—¶é—´ç‚¹ï¼ˆ2023 å¹´ Q4ï¼‰
- æ—¶é—´æ®µï¼ˆ2023-2024 å¹´ï¼‰
- ç›¸å¯¹æ—¶é—´ï¼ˆå»å¹´ã€ä¸Šä¸ªæœˆï¼‰
- æ—¶æ•ˆæ€§ï¼ˆæ°¸ä¹…ã€ä¸´æ—¶ã€è¿‡æœŸï¼‰

---

#### 6.4 åµŒå…¥å‘é‡ç”Ÿæˆï¼ˆEmbedding Generationï¼‰

**ä½ç½®**ï¼š`knowledge_extraction/embedding_generation.py`

**åŠŸèƒ½**ï¼šä¸ºé™ˆè¿°å¥å’Œå®ä½“ç”Ÿæˆå‘é‡åµŒå…¥

**ä»£ç **ï¼š
```python
async def embedding_generation(
    texts: List[str],
    embedder_client: OpenAIEmbedderClient,
    batch_size: int = 32,
) -> List[List[float]]:
    """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = await embedder_client.embed(batch_texts)
        all_embeddings.extend(batch_embeddings)
    
    return all_embeddings
```

---

### é˜¶æ®µ 7ï¼šå»é‡æ¶ˆæ­§

**ä½ç½®**ï¼š`deduplication/two_stage_dedup.py`

**ä¸¤é˜¶æ®µå»é‡**ï¼š

#### é˜¶æ®µ 1ï¼šå»é‡ï¼ˆDeduplicationï¼‰

**ç›®æ ‡**ï¼šè¯†åˆ«å¹¶åˆå¹¶é‡å¤å®ä½“

**æ–¹æ³•**ï¼š
1. åŸºäºå‘é‡ç›¸ä¼¼åº¦èšç±»
2. LLM åˆ¤æ–­æ˜¯å¦é‡å¤
3. åˆå¹¶é‡å¤å®ä½“

**ä»£ç **ï¼š
```python
async def dedup_layers_and_merge_and_return(
    entity_nodes: List[ExtractedEntityNode],
    ...
) -> Tuple[List[ExtractedEntityNode], List[Dict]]:
    # 1. å‘é‡ç›¸ä¼¼åº¦èšç±»
    clusters = cluster_entities_by_similarity(entity_nodes, threshold=0.85)
    
    # 2. LLM åˆ¤æ–­æ¯å¯¹å®ä½“æ˜¯å¦é‡å¤
    dedup_tasks = []
    for cluster in clusters:
        for pair in combinations(cluster, 2):
            task = llm_judge_duplicate(pair, llm_client)
            dedup_tasks.append(task)
    
    dedup_results = await asyncio.gather(*dedup_tasks)
    
    # 3. åˆå¹¶é‡å¤å®ä½“
    merged_entities = merge_duplicate_entities(entity_nodes, dedup_results)
    
    return merged_entities, merge_records
```

---

#### é˜¶æ®µ 2ï¼šæ¶ˆæ­§ï¼ˆDisambiguationï¼‰

**ç›®æ ‡**ï¼šåŒºåˆ†åŒåå¼‚ä¹‰å®ä½“

**æ–¹æ³•**ï¼š
1. åŸºäºä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
2. LLM åˆ¤æ–­æ˜¯å¦åŒä¹‰
3. æ‹†åˆ†æˆ–åˆå¹¶å®ä½“

---

### é˜¶æ®µ 8ï¼šå†™å…¥æ•°æ®åº“

#### 8.1 Neo4j çŸ¥è¯†å›¾è°±

**èŠ‚ç‚¹ç±»å‹**ï¼š
- DialogueNodeï¼ˆå¯¹è¯èŠ‚ç‚¹ï¼‰
- ChunkNodeï¼ˆåˆ†å—èŠ‚ç‚¹ï¼‰
- StatementNodeï¼ˆé™ˆè¿°å¥èŠ‚ç‚¹ï¼‰
- EntityNodeï¼ˆå®ä½“èŠ‚ç‚¹ï¼‰

**è¾¹ç±»å‹**ï¼š
- StatementChunkEdgeï¼ˆé™ˆè¿°å¥ - åˆ†å—ï¼‰
- StatementEntityEdgeï¼ˆé™ˆè¿°å¥ - å®ä½“ï¼‰
- EntityEntityEdgeï¼ˆå®ä½“ - å®ä½“ï¼‰

**ä»£ç **ï¼š
```python
async def _write_nodes_to_neo4j(
    self,
    chunk_nodes: List[ChunkNode],
    statement_nodes: List[StatementNode],
    entity_nodes: List[ExtractedEntityNode],
):
    if self.is_pilot_run:
        return
    
    # æ‰¹é‡å†™å…¥
    await self.connector.create_nodes(chunk_nodes)
    await self.connector.create_nodes(statement_nodes)
    await self.connector.create_nodes(entity_nodes)
```

---

#### 8.2 å‘é‡æ•°æ®åº“

**ç”¨é€”**ï¼šè¯­ä¹‰æ£€ç´¢

**å­˜å‚¨å†…å®¹**ï¼š
- é™ˆè¿°å¥å‘é‡
- å®ä½“å‘é‡
- Chunk å‘é‡

**ä»£ç **ï¼š
```python
async def _write_embeddings_to_vector_db(
    self,
    embedding_results: Dict[str, List[float]],
):
    for statement_id, embedding in embedding_results.items():
        await self.vector_db.upsert(
            id=statement_id,
            vector=embedding,
            metadata={"type": "statement"}
        )
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶è¡Œæ‰§è¡Œ

**ä¼˜åŒ–ç‚¹**ï¼š
```python
# ä¸²è¡Œæ‰§è¡Œï¼ˆæ…¢ï¼‰
statements = await extract_statements()
triplets = await extract_triplets()
temporal = await extract_temporal()

# å¹¶è¡Œæ‰§è¡Œï¼ˆå¿« 3 å€ï¼‰
statements, triplets, temporal = await asyncio.gather(
    extract_statements(),
    extract_triplets(),
    extract_temporal(),
)
```

---

### 2. æ‰¹é‡å¤„ç†

**åµŒå…¥ç”Ÿæˆ**ï¼š
```python
# å•ä¸ªå¤„ç†ï¼ˆæ…¢ï¼‰
for text in texts:
    embedding = await embedder.embed(text)

# æ‰¹é‡å¤„ç†ï¼ˆå¿« 10 å€ï¼‰
embeddings = await embedder.embed_batch(texts, batch_size=32)
```

---

### 3. è¯•è¿è¡Œæ¨¡å¼

**ç”¨é€”**ï¼šæµ‹è¯•æµç¨‹ï¼Œä¸å†™å…¥æ•°æ®åº“

```python
result = await orchestrator.run(
    dialog_data_list,
    is_pilot_run=True  # è¯•è¿è¡Œï¼Œä¸å†™å…¥
)
```

---

## ğŸ¯ å¯¹ 100 ä¸‡ç ”æŠ¥åœºæ™¯çš„å¯ç¤º

### å¯å€Ÿé‰´çš„è®¾è®¡

#### 1. å¤šæ ¼å¼è§£ææ”¯æŒ

**å»ºè®®**ï¼š
```python
# æ”¯æŒå¤šç§ç ”æŠ¥æ ¼å¼
parsers = {
    ".pdf": PDFParser,
    ".docx": DocxParser,
    ".html": HTMLParser,
    ".txt": TxTParser,
}
```

---

#### 2. çµæ´»çš„åˆ†å—ç­–ç•¥

**å»ºè®®**ï¼š
```python
# é’ˆå¯¹ä¸åŒæ–‡æ¡£ç±»å‹ä½¿ç”¨ä¸åŒç­–ç•¥
if document_type == "è´¢æŠ¥":
    strategy = "SemanticChunker"
elif document_type == "ç ”æŠ¥":
    strategy = "RecursiveChunker"  # æŒ‰ç« èŠ‚
elif document_type == "æ–°é—»":
    strategy = "SentenceChunker"
```

---

#### 3. çŸ¥è¯†æå–æµç¨‹

**å»ºè®®**ï¼š
```python
# ç ”æŠ¥åœºæ™¯çš„çŸ¥è¯†æå–
async def extract_knowledge_from_report(report: ReportData):
    # 1. é™ˆè¿°å¥æå–ï¼ˆå…³é”®äº‹å®ï¼‰
    statements = await extract_statements(report.chunks)
    
    # 2. ä¸‰å…ƒç»„æå–ï¼ˆå…¬å¸ - æŒ‡æ ‡ - æ•°å€¼ï¼‰
    triplets = await extract_triplets(report.chunks)
    # ä¾‹ï¼š(è´µå·èŒ…å°ï¼Œ2023 å¹´è¥æ”¶ï¼Œ500 äº¿å…ƒ)
    
    # 3. æ—¶é—´ä¿¡æ¯æå–ï¼ˆæŠ¥å‘ŠæœŸï¼‰
    temporal = await extract_temporal(report.chunks)
    
    # 4. åµŒå…¥ç”Ÿæˆ
    embeddings = await generate_embeddings(statements)
    
    return statements, triplets, temporal, embeddings
```

---

#### 4. å»é‡æ¶ˆæ­§æœºåˆ¶

**å»ºè®®**ï¼š
```python
# å…¬å¸åæ¶ˆæ­§
# "èŒ…å°"ã€"è´µå·èŒ…å°"ã€"600519.SH" â†’ åŒä¸€å®ä½“
merged_entities = await dedup_entities(entity_nodes)
```

---

### MemoryBear çš„å±€é™æ€§ï¼ˆå¯¹ç ”æŠ¥åœºæ™¯ï¼‰

| å±€é™ | è¯´æ˜ | æ”¹è¿›æ–¹æ¡ˆ |
|------|------|---------|
| **é¢å‘å¯¹è¯** | è®¾è®¡åˆè¡·æ˜¯å¯¹è¯è®°å¿† | éœ€è¦é€‚é…æ–‡æ¡£åœºæ™¯ |
| **ç¼ºå°‘æ–‡æ¡£çº§ä¸Šä¸‹æ–‡** | Chunk åªæœ‰è§’è‰²ä¿¡æ¯ | éœ€è¦æ·»åŠ æ–‡æ¡£å…ƒæ•°æ® |
| **å¤æ‚åº¦é«˜** | 93KB ç¼–æ’å™¨ï¼Œå­¦ä¹ æ›²çº¿é™¡ | ç®€åŒ–æµç¨‹ï¼Œä¸“æ³¨æ ¸å¿ƒ |
| **æˆæœ¬** | å¤§é‡ LLM è°ƒç”¨ | ä½¿ç”¨ Contextual Retrieval ä¼˜åŒ– |

---

## ğŸ“‹ å®Œæ•´æµç¨‹å›¾

```
ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶ï¼ˆPDF/Word/Excel ç­‰ï¼‰
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. æ–‡ä»¶è§£æ                         â”‚
â”‚     PDFParser / DocxParser / ...    â”‚
â”‚     è¾“å‡ºï¼šçº¯æ–‡æœ¬ + è¡¨æ ¼ + å›¾è¡¨        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. æ•°æ®é¢„å¤„ç†                       â”‚
â”‚     DataPreprocessor.preprocess()   â”‚
â”‚     - ç¼–ç æ£€æµ‹                       â”‚
â”‚     - æ•°æ®æ¸…æ´—                       â”‚
â”‚     - è½¬æ¢ä¸º DialogData              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. åˆ†å—ï¼ˆChunkingï¼‰                 â”‚
â”‚     ChunkerClient.generate_chunks() â”‚
â”‚     - 7 ç§ç­–ç•¥å¯é€‰                    â”‚
â”‚     - é»˜è®¤ chunk_size=2048          â”‚
â”‚     è¾“å‡ºï¼šList[Chunk]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. çŸ¥è¯†æå–ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰             â”‚
â”‚     ExtractionOrchestrator.run()    â”‚
â”‚                                     â”‚
â”‚     4a. é™ˆè¿°å¥æå– â† LLM            â”‚
â”‚     4b. ä¸‰å…ƒç»„æå– â† LLM            â”‚
â”‚     4c. æ—¶é—´ä¿¡æ¯æå– â† LLM          â”‚
â”‚     4d. åµŒå…¥ç”Ÿæˆ â† Embedding æ¨¡å‹   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. å»é‡æ¶ˆæ­§                         â”‚
â”‚     two_stage_dedup()               â”‚
â”‚     - é˜¶æ®µ 1ï¼šå»é‡ï¼ˆå‘é‡èšç±»+LLMï¼‰    â”‚
â”‚     - é˜¶æ®µ 2ï¼šæ¶ˆæ­§ï¼ˆä¸Šä¸‹æ–‡+LLMï¼‰      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. å†™å…¥æ•°æ®åº“                       â”‚
â”‚     - Neo4jï¼ˆçŸ¥è¯†å›¾è°±ï¼‰             â”‚
â”‚     - å‘é‡æ•°æ®åº“ï¼ˆè¯­ä¹‰æ£€ç´¢ï¼‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
çŸ¥è¯†åº“å¯ç”¨
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. æ£€ç´¢                             â”‚
â”‚     SearchService.execute_hybrid_search() â”‚
â”‚     - å‘é‡æ£€ç´¢ + å›¾è°±æ£€ç´¢            â”‚
â”‚     - Rerankingï¼ˆå¯é€‰ï¼‰             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”— å…³é”®æ–‡ä»¶ç´¢å¼•

| ç»„ä»¶ | æ–‡ä»¶è·¯å¾„ | ä»£ç è¡Œ | è¯´æ˜ |
|------|---------|--------|------|
| **ç¼–æ’å™¨** | extraction_orchestrator.py | 1900+ | æ ¸å¿ƒæµç¨‹åè°ƒ |
| **åˆ†å—å™¨** | chunker_client.py | 300+ | 7 ç§åˆ†å—ç­–ç•¥ |
| **é¢„å¤„ç†å™¨** | data_preprocessor.py | 600+ | å¤šæ ¼å¼æ”¯æŒ |
| **é™ˆè¿°å¥æå–** | statement_extraction.py | - | LLM æå– |
| **ä¸‰å…ƒç»„æå–** | triplet_extraction.py | - | å…³ç³»æå– |
| **å»é‡æ¶ˆæ­§** | two_stage_dedup.py | - | ä¸¤é˜¶æ®µå»é‡ |
| **PDF è§£æ** | pdf_parser.py | 56KB | æœ€å¤æ‚è§£æå™¨ |

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0  
**åˆ›å»ºæ—¥æœŸ**ï¼š2026-02-28  
**ä½œè€…**ï¼šJarvisï¼ˆåŸºäº MemoryBear ä»£ç åˆ†æï¼‰  
**çŠ¶æ€**ï¼šDraft  
**ä¸‹æ¬¡ Review**ï¼šå®æ–½é˜¶æ®µ 1 åæ›´æ–°
