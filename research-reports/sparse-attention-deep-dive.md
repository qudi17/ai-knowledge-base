# ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰æ·±åº¦è§£æ

**ç ”ç©¶æ—¥æœŸ**: 2026-03-01  
**ç ”ç©¶è€…**: Jarvis  
**æŠ€æœ¯åˆ†ç±»**: æ¨¡å‹å±‚ä¸Šä¸‹æ–‡ä¼˜åŒ–æŠ€æœ¯

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

### æ ‡å‡†æ³¨æ„åŠ›çš„ç¼ºé™·

**æ ‡å‡† Self-Attention**:
```
å¤æ‚åº¦ï¼šO(nÂ²)
æ¯ä¸ª token å…³æ³¨æ‰€æœ‰å…¶ä»– token

[1][2][3][4][5]...[n]
 â†“
[1] â†’ å…³æ³¨ [1,2,3,4,5,...,n] (n ä¸ªè¿æ¥)
[2] â†’ å…³æ³¨ [1,2,3,4,5,...,n] (n ä¸ªè¿æ¥)
...
[n] â†’ å…³æ³¨ [1,2,3,4,5,...,n] (n ä¸ªè¿æ¥)

æ€»è¿æ¥æ•°ï¼šn Ã— n = nÂ²
```

**é—®é¢˜**:
- âŒ è®¡ç®—é‡å¤§ï¼ˆnÂ²å¤æ‚åº¦ï¼‰
- âŒ å†…å­˜å ç”¨é«˜ï¼ˆå­˜å‚¨ nÃ—n æ³¨æ„åŠ›çŸ©é˜µï¼‰
- âŒ æ— æ³•å¤„ç†é•¿åºåˆ—ï¼ˆ10K+ tokensï¼‰
- âŒ æ³¨æ„åŠ›åˆ†æ•£ï¼ˆå…³æ³¨æ‰€æœ‰ä½ç½®ï¼ŒåŒ…æ‹¬æ— å…³çš„ï¼‰

---

### ç¨€ç–æ³¨æ„åŠ›çš„è§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒæ€æƒ³**:
```
æ¯ä¸ª token åªå…³æ³¨å…³é”®ä½ç½®ï¼Œè€Œéæ‰€æœ‰ä½ç½®

[1][2][3][4][5]...[n]
 â†“
[1] â†’ å…³æ³¨ [1,2,5,10,20...] (k ä¸ªè¿æ¥ï¼Œk << n)
[2] â†’ å…³æ³¨ [2,5,10,20...] (k ä¸ªè¿æ¥)
...
[n] â†’ å…³æ³¨ [n-20,n-10,n-5,n] (k ä¸ªè¿æ¥)

æ€»è¿æ¥æ•°ï¼šn Ã— k = O(n)
```

**ä¼˜åŠ¿**:
- âœ… è®¡ç®—é‡é™ä½ï¼ˆO(n) vs O(nÂ²)ï¼‰
- âœ… å†…å­˜å ç”¨å‡å°‘
- âœ… æ”¯æŒè¶…é•¿åºåˆ—ï¼ˆ100K+ tokensï¼‰
- âœ… æ³¨æ„åŠ›é›†ä¸­ï¼ˆåªå…³æ³¨å…³é”®ä½ç½®ï¼‰

---

## ğŸ“Š ä¸»è¦æŠ€æœ¯è·¯çº¿

### 1. Fixed Pattern Sparse Attention

**æ ¸å¿ƒ**: é¢„å®šä¹‰å›ºå®šç¨€ç–æ¨¡å¼

#### 1.1 Sliding Windowï¼ˆæ»‘åŠ¨çª—å£ï¼‰

```
æ¯ä¸ª token åªå…³æ³¨å±€éƒ¨çª—å£å†…çš„ token

[1][2][3][4][5][6][7][8][9][10]
         â†“
çª—å£å¤§å°=5:
[5] â†’ å…³æ³¨ [3,4,5,6,7] (å‰åå„ 2 ä¸ª)

å¤æ‚åº¦ï¼šO(n Ã— window_size) = O(n)
```

**å®ç°**ï¼ˆä¼ªä»£ç ï¼‰:
```python
class SlidingWindowAttention(nn.Module):
    def __init__(self, window_size=128):
        self.window_size = window_size
    
    def forward(self, x):
        # x: [batch, seq_len, dim]
        batch, seq_len, dim = x.shape
        
        # ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆæ³¨æ„åŠ›æ©ç 
        mask = torch.zeros(seq_len, seq_len)
        for i in range(seq_len):
            # åªå…³æ³¨çª—å£å†…çš„ä½ç½®
            start = max(0, i - self.window_size // 2)
            end = min(seq_len, i + self.window_size // 2)
            mask[i, start:end] = 1
        
        # åº”ç”¨æ³¨æ„åŠ›
        attn = torch.matmul(x, x.transpose(1, 2)) / sqrt(dim)
        attn = attn.masked_fill(mask == 0, float('-inf'))
        attn = torch.softmax(attn, dim=-1)
        
        return torch.matmul(attn, x)
```

**ä¼˜åŠ¿**:
- âœ… å®ç°ç®€å•
- âœ… ä¿æŒå±€éƒ¨æ€§ï¼ˆlocal contextï¼‰
- âœ… è®¡ç®—é«˜æ•ˆ

**åŠ£åŠ¿**:
- âŒ æ— æ³•æ•æ‰é•¿è·ç¦»ä¾èµ–
- âŒ å…¨å±€ä¿¡æ¯ä¸¢å¤±

---

#### 1.2 Strided Attentionï¼ˆè·¨æ­¥æ³¨æ„åŠ›ï¼‰

```
æ¯ä¸ª token å…³æ³¨å›ºå®šé—´éš”çš„ token

[1][2][3][4][5][6][7][8][9][10]
         â†“
è·¨æ­¥=3:
[5] â†’ å…³æ³¨ [2,5,8] (æ¯éš” 3 ä¸ª)

å¤æ‚åº¦ï¼šO(n Ã— n/stride) = O(n)
```

**å®ç°**ï¼ˆä¼ªä»£ç ï¼‰:
```python
class StridedAttention(nn.Module):
    def __init__(self, stride=3):
        self.stride = stride
    
    def forward(self, x):
        batch, seq_len, dim = x.shape
        
        # ç”Ÿæˆè·¨æ­¥æ©ç 
        mask = torch.zeros(seq_len, seq_len)
        for i in range(seq_len):
            # å…³æ³¨è·¨æ­¥ä½ç½®
            for j in range(0, seq_len, self.stride):
                mask[i, j] = 1
        
        # åº”ç”¨æ³¨æ„åŠ›
        attn = torch.matmul(x, x.transpose(1, 2)) / sqrt(dim)
        attn = attn.masked_fill(mask == 0, float('-inf'))
        attn = torch.softmax(attn, dim=-1)
        
        return torch.matmul(attn, x)
```

**ä¼˜åŠ¿**:
- âœ… æ•æ‰é•¿è·ç¦»ä¾èµ–
- âœ… è®¡ç®—é«˜æ•ˆ

**åŠ£åŠ¿**:
- âŒ å¯èƒ½é”™è¿‡å…³é”®ä½ç½®
- âŒ å›ºå®šæ¨¡å¼ä¸çµæ´»

---

### 2. Learnable Sparse Attention

**æ ¸å¿ƒ**: å­¦ä¹ ç¨€ç–æ¨¡å¼ï¼Œè€Œéé¢„å®šä¹‰

#### 2.1 Sparse Transformer

**è®ºæ–‡**: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

**æ ¸å¿ƒè®¾è®¡**:
```
ç»„åˆä¸¤ç§æ³¨æ„åŠ›ï¼š
1. è¡Œæ³¨æ„åŠ›ï¼ˆRow-wiseï¼‰ï¼šæ¯è¡Œå…³æ³¨å›ºå®šæ•°é‡ä½ç½®
2. åˆ—æ³¨æ„åŠ›ï¼ˆColumn-wiseï¼‰ï¼šæ¯åˆ—è¢«å›ºå®šæ•°é‡ä½ç½®å…³æ³¨

ç¤ºä¾‹ï¼ˆåºåˆ—é•¿åº¦=16ï¼Œæ¯è¡Œå…³æ³¨ 4 ä¸ªï¼‰:
[1][2][3][4]     [5][6][7][8]     [9][10][11][12]     [13][14][15][16]
 â†“                â†“                â†“                   â†“
[1] â†’ [1,2,3,4]  [5] â†’ [5,6,7,8]  [9] â†’ [9,10,11,12]  [13] â†’ [13,14,15,16]

åˆ—æ³¨æ„åŠ›ç¡®ä¿ä¿¡æ¯è·¨ç»„ä¼ æ’­
```

**å®ç°**ï¼ˆç®€åŒ–ç‰ˆï¼‰:
```python
class SparseTransformerBlock(nn.Module):
    def __init__(self, dim, num_sparse=32):
        self.row_attn = nn.Linear(dim, dim)
        self.col_attn = nn.Linear(dim, dim)
        self.num_sparse = num_sparse
    
    def forward(self, x):
        # è¡Œæ³¨æ„åŠ›
        row_out = self.row_attn(x)
        
        # åˆ—æ³¨æ„åŠ›
        col_out = self.col_attn(x)
        
        # ç»„åˆ
        return row_out + col_out
```

**ä¼˜åŠ¿**:
- âœ… è‡ªåŠ¨å­¦ä¹ é‡è¦ä½ç½®
- âœ… ä¿æŒé•¿è·ç¦»ä¾èµ–
- âœ… O(nâˆšn) å¤æ‚åº¦

**åŠ£åŠ¿**:
- âŒ å®ç°å¤æ‚
- âŒ è®­ç»ƒä¸ç¨³å®š

---

#### 2.2 Longformer

**è®ºæ–‡**: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)

**æ ¸å¿ƒè®¾è®¡**:
```
ç»„åˆä¸‰ç§æ³¨æ„åŠ›ï¼š
1. æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆå±€éƒ¨ï¼‰
2. å…¨å±€æ³¨æ„åŠ›ï¼ˆå…³é”® tokenï¼‰
3. ä»»åŠ¡ç‰¹å®šçš„å…¨å±€ token

ç¤ºä¾‹:
æ™®é€š token: æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ˆå‰å 128 ä¸ªï¼‰
[CLS] token: å…¨å±€æ³¨æ„åŠ›ï¼ˆå…³æ³¨æ‰€æœ‰ä½ç½®ï¼‰
é—®é¢˜ token: å…¨å±€æ³¨æ„åŠ›ï¼ˆå…³æ³¨æ‰€æœ‰ä½ç½®ï¼‰
```

**æ³¨æ„åŠ›æ¨¡å¼**:
```
æ™®é€š token:
[1][2][3]...[128][129][130]...[256]
         â†“
[129] â†’ å…³æ³¨ [1,2,3,...,256] (çª—å£å†…)

å…¨å±€ token:
[CLS][1][2][3]...[n]
  â†“
[CLS] â†’ å…³æ³¨ [1,2,3,...,n] (æ‰€æœ‰ä½ç½®)
```

**å®ç°**ï¼ˆç®€åŒ–ç‰ˆï¼‰:
```python
class LongformerAttention(nn.Module):
    def __init__(self, dim, window_size=128):
        self.local_attn = SlidingWindowAttention(window_size)
        self.global_attn = GlobalAttention()
        self.global_token_ids = []  # æ ‡è®°å“ªäº›æ˜¯å…¨å±€ token
    
    def forward(self, x, is_global_token):
        # å±€éƒ¨æ³¨æ„åŠ›ï¼ˆæ‰€æœ‰ tokenï¼‰
        local_out = self.local_attn(x)
        
        # å…¨å±€æ³¨æ„åŠ›ï¼ˆåªå…¨å±€ tokenï¼‰
        global_out = self.global_attn(x, is_global_token)
        
        # ç»„åˆ
        return local_out + global_out
```

**ä¼˜åŠ¿**:
- âœ… å¤„ç†è¶…é•¿æ–‡æ¡£ï¼ˆ10K+ tokensï¼‰
- âœ… ä¿æŒå±€éƒ¨ + å…¨å±€ä¿¡æ¯
- âœ… O(n) å¤æ‚åº¦

**åŠ£åŠ¿**:
- âŒ éœ€è¦æ ‡è®°å…¨å±€ token
- âŒ å®ç°å¤æ‚

---

### 3. StreamingLLM

**æ ¸å¿ƒæ€æƒ³**: ä¿ç•™åˆå§‹ token + æ»‘åŠ¨çª—å£

```
æ ‡å‡†æ»‘åŠ¨çª—å£:
[ç³»ç»Ÿæç¤º][ç”¨æˆ· 1][AI 1][ç”¨æˆ· 2][AI 2]...[ç”¨æˆ· N][AI N]
              â†“ è¶…å‡ºçª—å£åˆ™ä¸¢å¼ƒ
[ç”¨æˆ· 2][AI 2]...[ç”¨æˆ· N][AI N][ç”¨æˆ· N+1][AI N+1]
ï¼ˆç³»ç»Ÿæç¤ºå’Œç”¨æˆ· 1 å·²ä¸¢å¤±ï¼‰

StreamingLLM:
[ç³»ç»Ÿæç¤º][ç”¨æˆ· 1][AI 1][ç”¨æˆ· 2][AI 2]...[ç”¨æˆ· N][AI N]
    â†“ æ°¸è¿œä¿ç•™           â†“ æ»‘åŠ¨çª—å£
[ç³»ç»Ÿæç¤º][ç”¨æˆ· 1] + [ç”¨æˆ· N-4][AI N-4]...[ç”¨æˆ· N][AI N]
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**:
- âœ… åˆå§‹ token åŒ…å«å…³é”®ä¿¡æ¯ï¼ˆç³»ç»Ÿæç¤ºã€ç”¨æˆ·èº«ä»½ï¼‰
- âœ… é¿å…æ³¨æ„åŠ›å´©æºƒï¼ˆattention sinkï¼‰
- âœ… æ— éœ€é‡æ–°è®¡ç®— KV Cache

**å®ç°**ï¼ˆä¼ªä»£ç ï¼‰:
```python
class StreamingLLMAttention(nn.Module):
    def __init__(self, window_size=128, initial_tokens=4):
        self.window_size = window_size
        self.initial_tokens = initial_tokens
    
    def forward(self, x, kv_cache):
        # åˆ†ç¦»åˆå§‹ token å’Œæ»‘åŠ¨çª—å£
        initial_kv = kv_cache[:self.initial_tokens]
        window_kv = kv_cache[-self.window_size:]
        
        # ç»„åˆ
        effective_kv = torch.cat([initial_kv, window_kv], dim=0)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attn = torch.matmul(x, effective_kv.transpose(1, 2)) / sqrt(dim)
        attn = torch.softmax(attn, dim=-1)
        
        return torch.matmul(attn, effective_kv)
```

**ä¼˜åŠ¿**:
- âœ… å®ç°ç®€å•
- âœ… æ— éœ€é‡æ–°è®­ç»ƒ
- âœ… ä¿æŒé•¿å¯¹è¯ç¨³å®šæ€§

**åŠ£åŠ¿**:
- âŒ åªèƒ½ç”¨äºæ¨ç†
- âŒ åˆå§‹ token é€‰æ‹©å…³é”®

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å¤æ‚åº¦å¯¹æ¯”

| æ–¹æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | æ”¯æŒé•¿åº¦ |
|------|----------|----------|---------|
| **æ ‡å‡†æ³¨æ„åŠ›** | O(nÂ²) | O(nÂ²) | ~4K |
| **æ»‘åŠ¨çª—å£** | O(nÃ—w) | O(nÃ—w) | ~10K |
| **è·¨æ­¥æ³¨æ„åŠ›** | O(nÃ—n/s) | O(nÃ—n/s) | ~10K |
| **Sparse Transformer** | O(nâˆšn) | O(nâˆšn) | ~64K |
| **Longformer** | O(n) | O(n) | ~100K |
| **StreamingLLM** | O(nÃ—w) | O(nÃ—w) | æ— é™ |

---

### å®é™…æ€§èƒ½æ•°æ®

**æµ‹è¯•æ¡ä»¶**: A100 GPU, åºåˆ—é•¿åº¦=16K

| æ–¹æ³• | æ¨ç†é€Ÿåº¦ | å†…å­˜å ç”¨ | è´¨é‡æŸå¤± |
|------|---------|---------|---------|
| **æ ‡å‡†æ³¨æ„åŠ›** | 1.0x | 100% | 0% |
| **æ»‘åŠ¨çª—å£** | 8.5x | 12% | -2% |
| **Longformer** | 6.2x | 16% | -1% |
| **StreamingLLM** | 9.1x | 11% | -3% |

---

## ğŸ¯ åº”ç”¨åœºæ™¯

### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | æ¨èæ–¹æ³• | ç†ç”± |
|------|---------|------|
| **é•¿æ–‡æ¡£å¤„ç†** | Longformer | 100K+ tokens |
| **å®æ—¶å¯¹è¯** | StreamingLLM | ä½å»¶è¿Ÿ + æ— é™é•¿åº¦ |
| **ä»£ç ç”Ÿæˆ** | æ»‘åŠ¨çª—å£ | ä¿æŒå±€éƒ¨æ€§ |
| **è¶…é•¿åºåˆ—** | Sparse Transformer | 64K+ tokens |

---

### ä¸é€‚ç”¨åœºæ™¯

| åœºæ™¯ | åŸå›  | æ›¿ä»£æ–¹æ¡ˆ |
|------|------|---------|
| **çŸ­æ–‡æœ¬** | è¿‡åº¦ä¼˜åŒ–ï¼Œæ”¶ç›Šå° | æ ‡å‡†æ³¨æ„åŠ› |
| **éœ€è¦å…¨å±€ä¾èµ–** | ç¨€ç–å¯èƒ½ä¸¢å¤±ä¿¡æ¯ | æ ‡å‡†æ³¨æ„åŠ› |
| **åº”ç”¨å±‚å¼€å‘** | éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„ | RAG æ£€ç´¢ |

---

## ğŸ”§ å®ç°å»ºè®®

### å¿«é€Ÿå¼€å§‹ï¼ˆStreamingLLMï¼‰

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

class StreamingLLM:
    def __init__(self, model_name, window_size=128, initial_tokens=4):
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.window_size = window_size
        self.initial_tokens = initial_tokens
        self.kv_cache = None
    
    def generate(self, prompt, max_length=1000):
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
        
        # åˆ†ç¦»åˆå§‹ token
        initial_ids = input_ids[:, :self.initial_tokens]
        window_ids = input_ids[:, self.initial_tokens:]
        
        generated = []
        for _ in range(max_length):
            # ç»„åˆåˆå§‹ + çª—å£
            if self.kv_cache is not None:
                effective_ids = torch.cat([initial_ids, window_ids], dim=1)
            else:
                effective_ids = input_ids
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(effective_ids, past_key_values=self.kv_cache)
            self.kv_cache = outputs.past_key_values
            
            # é‡‡æ ·
            next_token = outputs.logits[:, -1].argmax(dim=-1)
            generated.append(next_token.item())
            
            # æ›´æ–°çª—å£
            window_ids = torch.cat([window_ids, next_token.unsqueeze(0)], dim=1)
            if window_ids.shape[1] > self.window_size:
                window_ids = window_ids[:, -self.window_size:]
        
        return self.tokenizer.decode(generated)
```

---

### ç”Ÿäº§éƒ¨ç½²ï¼ˆLongformerï¼‰

```python
from transformers import LongformerModel, LongformerTokenizer

class LongformerProcessor:
    def __init__(self, model_name='allenai/longformer-base-4096'):
        self.model = LongformerModel.from_pretrained(model_name)
        self.tokenizer = LongformerTokenizer.from_pretrained(model_name)
    
    def process_document(self, document, global_token_ids=[0]):
        # ç¼–ç 
        inputs = self.tokenizer(document, return_tensors='pt', truncation=False)
        
        # æ ‡è®°å…¨å±€ tokenï¼ˆå¦‚ [CLS]ï¼‰
        global_attention_mask = torch.zeros_like(inputs['attention_mask'])
        global_attention_mask[:, global_token_ids] = 1
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(
            **inputs,
            global_attention_mask=global_attention_mask
        )
        
        return outputs.last_hidden_state
```

---

## ğŸ“Š ä¸ RAG å¯¹æ¯”

### ç¨€ç–æ³¨æ„åŠ› vs RAG æ£€ç´¢

| ç»´åº¦ | ç¨€ç–æ³¨æ„åŠ› | RAG æ£€ç´¢ |
|------|----------|---------|
| **å±‚çº§** | æ¨¡å‹å±‚ | åº”ç”¨å±‚ |
| **å®ç°éš¾åº¦** | éš¾ï¼ˆéœ€ä¿®æ”¹æ¨¡å‹ï¼‰ | æ˜“ï¼ˆ<200 è¡Œä»£ç ï¼‰ |
| **æ€§èƒ½å¼€é”€** | ä½ï¼ˆO(n)ï¼‰ | ä¸­ï¼ˆ+50msï¼‰ |
| **é•¿è·ç¦»ä¾èµ–** | â­â­â­â­ | â­â­â­â­â­ |
| **æ— å…³å†…å®¹è¿‡æ»¤** | â­â­ | â­â­â­â­â­ |
| **è·¨ä¼šè¯è®°å¿†** | âŒ | âœ… |
| **é€‚ç”¨åœºæ™¯** | è¶…é•¿æ–‡æ¡£ | é€šç”¨å¯¹è¯ |

---

### æœ€ä½³å®è·µï¼šç»„åˆä½¿ç”¨

```
åº”ç”¨å±‚ï¼šRAG æ£€ç´¢
    â†“
æ£€ç´¢ç›¸å…³å†å²ï¼ˆTop-5ï¼‰
    â†“
æ¨¡å‹å±‚ï¼šç¨€ç–æ³¨æ„åŠ›
    â†“
å¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼ˆæ»‘åŠ¨çª—å£ + å…¨å±€ tokenï¼‰
    â†“
ç”Ÿæˆå›ç­”
```

**ä¼˜åŠ¿**:
- âœ… RAG è¿‡æ»¤æ— å…³å†…å®¹
- âœ… ç¨€ç–æ³¨æ„åŠ›å¤„ç†é•¿åºåˆ—
- âœ… ç»¼åˆæ€§èƒ½æœ€ä¼˜

---

## ğŸ”— ç›¸å…³èµ„æº

### è®ºæ–‡
- [Sparse Transformers](https://arxiv.org/abs/1904.10509)
- [Longformer](https://arxiv.org/abs/2004.05150)
- [StreamingLLM](https://arxiv.org/abs/2309.17453)

### ä»£ç å®ç°
- [Longformer GitHub](https://github.com/allenai/longformer)
- [StreamingLLM GitHub](https://github.com/mit-han-lab/streaming-llm)

### HuggingFace æ¨¡å‹
- `allenai/longformer-base-4096`
- `mistralai/Mistral-7B` (æ”¯æŒæ»‘åŠ¨çª—å£)

---

**ç ”ç©¶æ—¥æœŸ**: 2026-03-01  
**ç ”ç©¶è€…**: Jarvis  
**çŠ¶æ€**: âœ… å®Œæˆ
