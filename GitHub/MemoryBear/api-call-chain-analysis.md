# MemoryBear - API è°ƒç”¨é“¾æ·±åº¦åˆ†æ

**åˆ†ææ—¥æœŸ**ï¼š2026-02-28  
**åˆ†æå…¥å£**ï¼š`/v1/app/chat` API  
**è¿½è¸ªç›®æ ‡**ï¼šç”¨æˆ·æ¶ˆæ¯ â†’ LLM Agent å®Œæ•´æµç¨‹

---

## ğŸ“Š API è°ƒç”¨é“¾æ€»è§ˆ

```
ç”¨æˆ·è¯·æ±‚
  â†“
[v1/app/chat] API ç«¯ç‚¹
  â†“
app_api_controller.py:chat()
  â†“
AppChatService.agnet_chat() / agnet_chat_stream()
  â†“
LangChainAgent.chat() / chat_stream()
  â†“
LangChain create_agent() + LLM è°ƒç”¨
  â†“
å·¥å…·æ‰§è¡Œï¼ˆå¯é€‰ï¼‰
  â†“
è®°å¿†å†™å…¥ï¼ˆwrite_long_termï¼‰
  â†“
è¿”å›å“åº”
```

---

## 1ï¸âƒ£ API å±‚ï¼š`/v1/app/chat`

**æ–‡ä»¶**ï¼š[`app/controllers/service/app_api_controller.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/controllers/service/app_api_controller.py)

### è·¯ç”±å®šä¹‰

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/controllers/service/app_api_controller.py#L119-L121
router = APIRouter(prefix="/v1/app", tags=["V1 - App API"])

@router.post("/chat")
@require_api_key(scopes=["app"])
async def chat(
    request: Request,
    api_key_auth: ApiKeyAuth = None,
    db: Session = Depends(get_db),
    conversation_service: ConversationService = Depends(get_conversation_service),
    app_chat_service: AppChatService = Depends(get_app_chat_service),
    app_service: AppService = Depends(get_app_service),
    message: str = Body(..., description="èŠå¤©æ¶ˆæ¯å†…å®¹"),
):
```

### æ ¸å¿ƒæµç¨‹

```python
# 1. è§£æè¯·æ±‚ä½“
body = await request.json()
payload = AppChatRequest(**body)

# 2. è·å–åº”ç”¨é…ç½®
app = app_service.get_app(api_key_auth.resource_id, api_key_auth.workspace_id)
workspace_id = app.workspace_id

# 3. è·å–æˆ–åˆ›å»ºç”¨æˆ·
end_user_repo = EndUserRepository(db)
new_end_user = end_user_repo.get_or_create_end_user(
    app_id=app.id,
    other_id=payload.user_id,
    original_user_id=payload.user_id
)
end_user_id = str(new_end_user.id)

# 4. ç¡®å®šå­˜å‚¨ç±»å‹
storage_type = workspace_service.get_workspace_storage_type_without_auth(
    db=db, workspace_id=workspace_id
)
if storage_type is None:
    storage_type = 'neo4j'

# 5. è·å–æˆ–åˆ›å»ºä¼šè¯
conversation = conversation_service.create_or_get_conversation(
    app_id=app.id,
    workspace_id=workspace_id,
    user_id=end_user_id,
    is_draft=False
)

# 6. æ ¹æ®åº”ç”¨ç±»å‹è°ƒç”¨ä¸åŒæœåŠ¡
if app.type == AppType.AGENT:
    # Agent åº”ç”¨
    agent_config = agent_config_4_app_release(app.current_release)
    if payload.stream:
        # æµå¼å“åº”
        return StreamingResponse(
            app_chat_service.agnet_chat_stream(...),
            media_type="text/event-stream"
        )
    else:
        # éæµå¼å“åº”
        result = await app_chat_service.agnet_chat(...)
        return success(data=ChatResponse(**result))
```

**å…³é”®å‚æ•°**ï¼š
- `storage_type`: `'neo4j'` | `'rag'` | `'vector'` - è®°å¿†å­˜å‚¨ç±»å‹
- `user_rag_memory_id`: ç”¨æˆ· RAG è®°å¿† IDï¼ˆå¦‚æœå¯ç”¨ï¼‰
- `memory_flag`: True/False - æ˜¯å¦å†™å…¥è®°å¿†
- `web_search`: True/False - æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢

---

## 2ï¸âƒ£ æœåŠ¡å±‚ï¼š`AppChatService.agnet_chat()`

**æ–‡ä»¶**ï¼š[`app/services/app_chat_service.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/services/app_chat_service.py)

### æ ¸å¿ƒæµç¨‹

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/services/app_chat_service.py#L39-L227
async def agnet_chat(
    self,
    message: str,
    conversation_id: uuid.UUID,
    config: AgentConfig,
    user_id: Optional[str] = None,
    variables: Optional[Dict[str, Any]] = None,
    web_search: bool = False,
    memory: bool = True,
    storage_type: Optional[str] = None,
    user_rag_memory_id: Optional[str] = None,
    workspace_id: Optional[str] = None,
    files: Optional[List[FileInput]] = None
) -> Dict[str, Any]:
```

#### æ­¥éª¤ 1ï¼šå‡†å¤‡ç³»ç»Ÿæç¤ºè¯

```python
# è·å–æ¨¡å‹é…ç½®
model_config_id = config.default_model_config_id
api_key_obj = ModelApiKeyService.get_available_api_key(self.db, model_config_id)

# å¤„ç†ç³»ç»Ÿæç¤ºè¯ï¼ˆæ”¯æŒå˜é‡æ›¿æ¢ï¼‰
system_prompt = config.system_prompt
if variables:
    system_prompt_rendered = render_prompt_message(
        system_prompt,
        PromptMessageRole.USER,
        variables
    )
    system_prompt = system_prompt_rendered.get_text_content()
```

#### æ­¥éª¤ 2ï¼šå‡†å¤‡å·¥å…·åˆ—è¡¨

```python
tools = []
tool_service = ToolService(self.db)

# 1. ä»é…ç½®ä¸­è·å–å¯ç”¨çš„å·¥å…·
if hasattr(config, 'tools') and config.tools:
    for tool_config in config.tools:
        if tool_config.get("enabled", False):
            tool_instance = tool_service._get_tool_instance(
                tool_config.get("tool_id", ""), tenant_id
            )
            if tool_instance:
                # è½¬æ¢ä¸º LangChain å·¥å…·
                langchain_tool = tool_instance.to_langchain_tool(
                    tool_config.get("operation", None)
                )
                tools.append(langchain_tool)

# 2. åŠ è½½æŠ€èƒ½å…³è”çš„å·¥å…·
if hasattr(config, 'skills') and config.skills:
    skills = config.skills
    middleware = AgentMiddleware(skills=skills)
    skill_tools, skill_configs, tool_to_skill_map = middleware.load_skill_tools(
        self.db, tenant_id
    )
    tools.extend(skill_tools)
    
    # åº”ç”¨åŠ¨æ€è¿‡æ»¤
    if skill_configs:
        tools, activated_skill_ids = middleware.filter_tools(
            tools, message, skill_configs, tool_to_skill_map
        )
        active_prompts = AgentMiddleware.get_active_prompts(
            activated_skill_ids, skill_configs
        )
        system_prompt = f"{system_prompt}\n\n{active_prompts}"

# 3. æ·»åŠ çŸ¥è¯†åº“æ£€ç´¢å·¥å…·
if knowledge_retrieval:
    kb_ids = [kb.get("kb_id") for kb in knowledge_bases]
    if kb_ids:
        kb_tool = create_knowledge_retrieval_tool(knowledge_retrieval, kb_ids, user_id)
        tools.append(kb_tool)

# 4. æ·»åŠ é•¿æœŸè®°å¿†å·¥å…·
if memory == True:
    memory_config = config.memory
    if memory_config.get("enabled") and user_id:
        memory_flag = True
        memory_tool = create_long_term_memory_tool(memory_config, user_id)
        tools.append(memory_tool)
```

#### æ­¥éª¤ 3ï¼šåˆ›å»º LangChain Agent

```python
# åˆ›å»º LangChain Agent
agent = LangChainAgent(
    model_name=api_key_obj.model_name,
    api_key=api_key_obj.api_key,
    provider=api_key_obj.provider,
    api_base=api_key_obj.api_base,
    temperature=model_parameters.get("temperature", 0.7),
    max_tokens=model_parameters.get("max_tokens", 2000),
    system_prompt=system_prompt,
    tools=tools,
    streaming=payload.stream,
    max_iterations=None,  # è‡ªåŠ¨è®¡ç®—
    max_tool_consecutive_calls=3
)
```

#### æ­¥éª¤ 4ï¼šåŠ è½½å†å²æ¶ˆæ¯

```python
history = []
memory_config = {"enabled": True, 'max_history': 10}
if memory_config.get("enabled"):
    messages = self.conversation_service.get_messages(
        conversation_id=conversation_id,
        limit=memory_config.get("max_history", 10)
    )
    history = [
        {"role": msg.role, "content": msg.content}
        for msg in messages
    ]
```

#### æ­¥éª¤ 5ï¼šè°ƒç”¨ Agent

```python
# å¤„ç†å¤šæ¨¡æ€æ–‡ä»¶
processed_files = None
if files:
    multimodal_service = MultimodalService(self.db)
    processed_files = await multimodal_service.process_files(files)

# è°ƒç”¨ Agent
result = await agent.chat(
    message=message,
    history=history,
    context=None,
    end_user_id=user_id,
    storage_type=storage_type,
    user_rag_memory_id=user_rag_memory_id,
    config_id=config_id,
    memory_flag=memory_flag,
    files=processed_files
)
```

#### æ­¥éª¤ 6ï¼šä¿å­˜æ¶ˆæ¯åˆ°æ•°æ®åº“

```python
# ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
self.conversation_service.add_message(
    conversation_id=conversation_id,
    role="user",
    content=message,
    sender_id=user_id
)

# ä¿å­˜ AI å›å¤
self.conversation_service.add_message(
    conversation_id=conversation_id,
    role="assistant",
    content=result["content"],
    sender_id="assistant",
    token_usage=result.get("usage", {})
)
```

---

## 3ï¸âƒ£ Agent å±‚ï¼š`LangChainAgent.chat()`

**æ–‡ä»¶**ï¼š[`app/core/agent/langchain_agent.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/core/agent/langchain_agent.py)

### æ ¸å¿ƒæµç¨‹

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/core/agent/langchain_agent.py#L194-L303
async def chat(
    self,
    message: str,
    history: Optional[List[Dict[str, str]]] = None,
    context: Optional[str] = None,
    end_user_id: Optional[str] = None,
    config_id: Optional[str] = None,
    storage_type: Optional[str] = None,
    user_rag_memory_id: Optional[str] = None,
    memory_flag: Optional[bool] = True,
    files: Optional[List[Dict[str, Any]]] = None
) -> Dict[str, Any]:
```

#### æ­¥éª¤ 1ï¼šå‡†å¤‡æ¶ˆæ¯åˆ—è¡¨

```python
# å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
messages = self._prepare_messages(message, history, context, files)

def _prepare_messages(
    self,
    message: str,
    history: Optional[List[Dict[str, str]]] = None,
    context: Optional[str] = None,
    files: Optional[List[Dict[str, Any]]] = None
) -> List[BaseMessage]:
    messages = []
    
    # 1. æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    messages.append(SystemMessage(content=self.system_prompt))
    
    # 2. æ·»åŠ å†å²æ¶ˆæ¯
    if history:
        for msg in history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
    
    # 3. æ·»åŠ  RAG ä¸Šä¸‹æ–‡
    user_content = message
    if context:
        user_content = f"å‚è€ƒä¿¡æ¯ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š\n{user_content}"
    
    # 4. æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
    if files and len(files) > 0:
        content_parts = self._build_multimodal_content(user_content, files)
        messages.append(HumanMessage(content=content_parts))
    else:
        messages.append(HumanMessage(content=user_content))
    
    return messages
```

#### æ­¥éª¤ 2ï¼šè°ƒç”¨ LangChain Agent

```python
# ç»Ÿä¸€ä½¿ç”¨ agent.invoke è°ƒç”¨
# é€šè¿‡ recursion_limit é™åˆ¶æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢å·¥å…·è°ƒç”¨æ­»å¾ªç¯
try:
    result = await self.agent.ainvoke(
        {"messages": messages},
        config={"recursion_limit": self.max_iterations}
    )
except RecursionError as e:
    logger.warning(f"Agent è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶ ({self.max_iterations})")
    return {"content": f"å·²è¾¾åˆ°æœ€å¤§å¤„ç†æ­¥éª¤é™åˆ¶ï¼ˆ{self.max_iterations}æ¬¡ï¼‰"}
```

#### æ­¥éª¤ 3ï¼šæå– AI å›å¤

```python
# è·å–æœ€åçš„ AI æ¶ˆæ¯
output_messages = result.get("messages", [])
content = ""
total_tokens = 0

for msg in reversed(output_messages):
    if isinstance(msg, AIMessage):
        # å¤„ç†å¤šæ¨¡æ€å“åº”ï¼šcontent å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
        if isinstance(msg.content, str):
            content = msg.content
        elif isinstance(msg.content, list):
            # å¤šæ¨¡æ€å“åº”ï¼šæå–æ–‡æœ¬éƒ¨åˆ†
            text_parts = []
            for item in msg.content:
                if isinstance(item, dict):
                    if "text" in item:
                        text_parts.append(item["text"])
                elif isinstance(item, str):
                    text_parts.append(item)
            content = "".join(text_parts)
        
        # æå– token ä½¿ç”¨
        response_meta = msg.response_metadata if hasattr(msg, 'response_metadata') else None
        total_tokens = response_meta.get("token_usage", {}).get("total_tokens", 0) if response_meta else 0
        break
```

#### æ­¥éª¤ 4ï¼šå†™å…¥é•¿æœŸè®°å¿†

```python
elapsed_time = time.time() - start_time

if memory_flag:
    await write_long_term(
        storage_type,
        end_user_id,
        message_chat,      # ç”¨æˆ·æ¶ˆæ¯
        content,           # AI å›å¤
        user_rag_memory_id,
        actual_config_id
    )

response = {
    "content": content,
    "model": self.model_name,
    "elapsed_time": elapsed_time,
    "usage": {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": total_tokens
    }
}

return response
```

---

## 4ï¸âƒ£ è®°å¿†å†™å…¥ï¼š`write_long_term()`

**æ–‡ä»¶**ï¼š[`app/core/memory/agent/langgraph_graph/write_graph.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/core/memory/agent/langgraph_graph/write_graph.py)

### æ ¸å¿ƒæµç¨‹

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/core/memory/agent/langgraph_graph/write_graph.py
async def write_long_term(
    storage_type: str,
    end_user_id: str,
    user_message: str,
    ai_response: str,
    user_rag_memory_id: str,
    config_id: str
):
    """å†™å…¥é•¿æœŸè®°å¿†
    
    æµç¨‹ï¼š
    1. è®°å¿†èƒå–ï¼ˆLLM ç»“æ„åŒ–æå–ï¼‰
    2. ä¸‰å…ƒç»„æå–
    3. å­˜å‚¨åˆ° PostgreSQL + Neo4j
    4. å‘é‡åŒ–å­˜å‚¨
    """
```

**æ¨æ–­æµç¨‹**ï¼ˆåŸºäºæ¶æ„åˆ†æï¼‰ï¼š

```python
# 1. è®°å¿†èƒå–
extracted_memories = await extract_memories(
    conversation=f"{user_message}\n{ai_response}",
    llm_client=llm_client
)

# 2. ä¸‰å…ƒç»„æå–
triples = extract_triples(extracted_memories)

# 3. å­˜å‚¨åˆ° Neo4j
for triple in triples:
    neo4j_client.add_triple(
        subject=triple.subject,
        predicate=triple.predicate,
        object=triple.object,
        properties={
            "user_id": end_user_id,
            "timestamp": datetime.now(),
            "confidence": triple.confidence
        }
    )

# 4. å‘é‡åŒ–å­˜å‚¨
if storage_type == 'vector' or storage_type == 'rag':
    embeddings = embedding_client.embed(extracted_memories)
    vector_db.add(
        vectors=embeddings,
        metadata={
            "user_id": end_user_id,
            "content": extracted_memories
        }
    )

# 5. æ›´æ–°è®°å¿†å¼ºåº¦
update_memory_strength(
    user_id=end_user_id,
    memory_ids=extracted_memories.ids,
    operation="increment"
)
```

---

## ğŸ“Š å®Œæ•´è°ƒç”¨é“¾æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant API as API Controller
    participant Service as AppChatService
    participant Agent as LangChainAgent
    participant LLM as LLM Provider
    participant Tools as å·¥å…·ç³»ç»Ÿ
    participant Memory as è®°å¿†å†™å…¥
    participant DB as æ•°æ®åº“

    User->>API: POST /v1/app/chat
    API->>API: API Key è®¤è¯
    API->>Service: agnet_chat()
    
    Service->>Service: å‡†å¤‡ç³»ç»Ÿæç¤ºè¯
    Service->>Service: å‡†å¤‡å·¥å…·åˆ—è¡¨
    Service->>Service: åˆ›å»º LangChain Agent
    
    Service->>Agent: chat(message, history, ...)
    Agent->>Agent: _prepare_messages()
    
    loop Agent å¾ªç¯ï¼ˆæœ€å¤š N æ¬¡ï¼‰
        Agent->>LLM: è°ƒç”¨ LLM
        LLM-->>Agent: LLM å“åº”
        
        alt æœ‰å·¥å…·è°ƒç”¨
            Agent->>Tools: execute(tool_name, params)
            Tools-->>Agent: å·¥å…·ç»“æœ
            Agent->>Agent: æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
        else æ— å·¥å…·è°ƒç”¨
            Agent->>Agent: æå–æœ€ç»ˆå›å¤
        end
    end
    
    Agent->>Memory: write_long_term()
    Memory->>Memory: è®°å¿†èƒå–
    Memory->>Memory: ä¸‰å…ƒç»„æå–
    Memory->>DB: å­˜å‚¨åˆ° Neo4j/Vector
    Memory-->>Agent: å®Œæˆ
    
    Agent-->>Service: è¿”å›å“åº”
    
    Service->>DB: ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    Service->>DB: ä¿å­˜ AI å›å¤
    Service-->>API: è¿”å›ç»“æœ
    API-->>User: è¿”å›å“åº”
```

---

## ğŸ”‘ å…³é”®è®¾è®¡ç‚¹

### 1. æµå¼å“åº”æ”¯æŒ

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/controllers/service/app_api_controller.py#L158-L175
if payload.stream:
    async def event_generator():
        async for event in app_chat_service.agnet_chat_stream(
            message=payload.message,
            conversation_id=conversation.id,
            user_id=end_user_id,
            ...
        ):
            yield event
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )
```

### 2. å·¥å…·åŠ¨æ€è¿‡æ»¤

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/services/app_chat_service.py#L120-L128
# åº”ç”¨åŠ¨æ€è¿‡æ»¤
if skill_configs:
    tools, activated_skill_ids = middleware.filter_tools(
        tools, message, skill_configs, tool_to_skill_map
    )
    active_prompts = AgentMiddleware.get_active_prompts(
        activated_skill_ids, skill_configs
    )
    system_prompt = f"{system_prompt}\n\n{active_prompts}"
```

### 3. è®°å¿†å†™å…¥å¼‚æ­¥åŒ–

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/core/agent/langchain_agent.py#L280-L290
if memory_flag:
    await write_long_term(
        storage_type,
        end_user_id,
        message_chat,
        content,
        user_rag_memory_id,
        actual_config_id
    )
```

### 4. å¤šæ¨¡æ€æ”¯æŒ

```python
# æ¥æºï¼šhttps://github.com/qudi17/MemoryBear/blob/main/api/app/core/agent/langchain_agent.py#L167-L192
def _build_multimodal_content(self, text: str, files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    # æ ¹æ® provider ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æ ¼å¼
    if self.provider.lower() in ["bedrock", "anthropic"]:
        # Anthropic/Bedrock: {"type": "text", "text": "..."}
        content_parts = [{"type": "text", "text": text}]
    else:
        # é€šä¹‰åƒé—®ç­‰ï¼š{"text": "..."}
        content_parts = [{"text": text}]
    
    # æ·»åŠ æ–‡ä»¶å†…å®¹
    content_parts.extend(files)
    
    return content_parts
```

---

## ğŸ“ æ€»ç»“

### API è°ƒç”¨é“¾å…³é”®èŠ‚ç‚¹

| å±‚çº§ | ç»„ä»¶ | æ–‡ä»¶ | èŒè´£ |
|------|------|------|------|
| **API å±‚** | `app_api_controller` | `app/controllers/service/app_api_controller.py` | è·¯ç”±ã€è®¤è¯ã€å‚æ•°è§£æ |
| **æœåŠ¡å±‚** | `AppChatService` | `app/services/app_chat_service.py` | ä¸šåŠ¡é€»è¾‘ã€å·¥å…·å‡†å¤‡ |
| **Agent å±‚** | `LangChainAgent` | `app/core/agent/langchain_agent.py` | LLM è°ƒç”¨ã€å·¥å…·å¾ªç¯ |
| **è®°å¿†å±‚** | `write_long_term` | `app/core/memory/agent/langgraph_graph/write_graph.py` | è®°å¿†èƒå–ã€å­˜å‚¨ |

### æ ¸å¿ƒæµç¨‹

1. **API æ¥æ”¶** â†’ è®¤è¯ â†’ å‚æ•°è§£æ
2. **æœåŠ¡å‡†å¤‡** â†’ ç³»ç»Ÿæç¤ºè¯ â†’ å·¥å…·åˆ—è¡¨ â†’ Agent åˆ›å»º
3. **Agent å¾ªç¯** â†’ LLM è°ƒç”¨ â†’ å·¥å…·æ‰§è¡Œ â†’ ç»“æœæå–
4. **è®°å¿†å†™å…¥** â†’ èƒå– â†’ ä¸‰å…ƒç»„ â†’ å­˜å‚¨
5. **å“åº”è¿”å›** â†’ ä¿å­˜æ¶ˆæ¯ â†’ è¿”å›ç”¨æˆ·

---

**åˆ†æäºº**ï¼šJarvis  
**å®¡æ ¸äºº**ï¼šEddy  
**æœ€åæ›´æ–°**ï¼š2026-02-28  
**çŠ¶æ€**ï¼šâœ… å®Œæˆ
