# MemoryBear - åæ€å¼•æ“ä¸é—å¿˜è°ƒåº¦å™¨æ·±åº¦ç ”ç©¶

**ç ”ç©¶æ—¥æœŸ**ï¼š2026-02-28  
**ç ”ç©¶æ–¹æ³•**ï¼šæ¯›çº¿å›¢ç ”ç©¶æ³•ï¼ˆåˆ†æ”¯ç ”ç©¶ï¼‰  
**ç ”ç©¶å†…å®¹**ï¼šè‡ªæˆ‘åæ€å¼•æ“ã€é—å¿˜è°ƒåº¦å™¨ã€é—å¿˜ç­–ç•¥

---

## ğŸ§¶ ç ”ç©¶åˆ†æ”¯

è¿™æ˜¯ MemoryBear è®°å¿†ç³»ç»Ÿç ”ç©¶çš„æ·±å…¥åˆ†æ”¯ï¼Œç ”ç©¶ï¼š
1. âœ… **è‡ªæˆ‘åæ€å¼•æ“** - è®°å¿†å†²çªæ£€æµ‹å’Œè§£å†³
2. âœ… **é—å¿˜è°ƒåº¦å™¨** - å®šæœŸé—å¿˜å‘¨æœŸç®¡ç†
3. âœ… **é—å¿˜ç­–ç•¥** - åŸºäº ACT-R çš„èŠ‚ç‚¹èåˆ
4. âœ… **LLM æ‘˜è¦ç”Ÿæˆ** - é«˜è´¨é‡è®°å¿†èåˆ

---

## 1ï¸âƒ£ è‡ªæˆ‘åæ€å¼•æ“ï¼ˆReflection Engineï¼‰

### 1.1 æ ¸å¿ƒæ¶æ„

**æ–‡ä»¶**ï¼š[`app/core/memory/storage_services/reflection_engine/self_reflexion.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/core/memory/storage_services/reflection_engine/self_reflexion.py) (26.9KB)

**ç±»ç»“æ„**ï¼š
```python
class ReflectionEngine:
    """è‡ªæˆ‘åæ€å¼•æ“"""
    
    def __init__(self, config: ReflectionConfig, ...):
        self.config = config
        self.neo4j_connector = Neo4jConnector()
        self.llm_client = MemoryClientFactory().get_llm_client(...)
    
    async def run_reflection(self) -> ReflectionResult:
        """è¿è¡Œåæ€æµç¨‹"""
```

**é…ç½®ç±»**ï¼š
```python
class ReflectionConfig(BaseModel):
    enabled: bool = False                    # æ˜¯å¦å¯ç”¨
    iteration_period: str = "3"              # åæ€å‘¨æœŸï¼ˆå¤©ï¼‰
    reflexion_range: ReflectionRange = ReflectionRange.PARTIAL
    baseline: ReflectionBaseline = ReflectionBaseline.TIME
    model_id: Optional[str] = None           # LLM æ¨¡å‹ ID
    end_user_id: Optional[str] = None
    
    # è¯„ä¼°ç›¸å…³
    memory_verify: bool = True               # è®°å¿†éªŒè¯
    quality_assessment: bool = True          # è´¨é‡è¯„ä¼°
    violation_handling_strategy: str = "warn"  # è¿è§„å¤„ç†ç­–ç•¥
```

**æšä¸¾ç±»å‹**ï¼š
```python
class ReflectionRange(str, Enum):
    PARTIAL = "partial"  # ä»æ£€ç´¢ç»“æœä¸­åæ€
    ALL = "all"          # ä»æ•´ä¸ªæ•°æ®åº“ä¸­åæ€

class ReflectionBaseline(str, Enum):
    TIME = "TIME"   # åŸºäºæ—¶é—´çš„åæ€
    FACT = "FACT"   # åŸºäºäº‹å®çš„åæ€
    HYBRID = "HYBRID"  # æ··åˆåæ€
```

---

### 1.2 åæ€æµç¨‹

**å®Œæ•´æµç¨‹**ï¼š
```
1. åŠ è½½é…ç½® â†’ ReflectionConfig
   â†“
2. é€‰æ‹©åæ€èŒƒå›´ â†’ PARTIAL æˆ– ALL
   â†“
3. é€‰æ‹©åæ€åŸºçº¿ â†’ TIME æˆ– FACT æˆ– HYBRID
   â†“
4. è·å–è®°å¿†æ•°æ® â†’ Neo4j æŸ¥è¯¢
   â†“
5. LLM è¯„ä¼°å†²çª â†’ render_evaluate_prompt()
   â†“
6. LLM ç”Ÿæˆè§£å†³æ–¹æ¡ˆ â†’ render_reflexion_prompt()
   â†“
7. åº”ç”¨æ›´æ”¹ â†’ update Neo4j
   â†“
8. è¿”å›ç»“æœ â†’ ReflectionResult
```

**ä»£ç å®ç°**ï¼š
```python
async def run_reflection(self) -> ReflectionResult:
    """è¿è¡Œå®Œæ•´çš„åæ€æµç¨‹"""
    start_time = time.time()
    
    try:
        # 1. è·å–è®°å¿†æ•°æ®
        if self.config.reflexion_range == "partial":
            data = await self.get_data_func(
                neo4j_connector=self.neo4j_connector,
                end_user_id=self.config.end_user_id,
                limit=50  # é™åˆ¶æ£€ç´¢æ•°é‡
            )
        else:  # "all"
            data = await self.get_data_func(
                neo4j_connector=self.neo4j_connector,
                end_user_id=self.config.end_user_id,
                limit=None  # è·å–å…¨éƒ¨
            )
        
        # 2. æ¸²æŸ“è¯„ä¼°æç¤ºè¯
        evaluate_prompt = await self.render_evaluate_prompt_func(
            baseline=self.config.baseline,
            data=data,
            output_example=self.config.output_example
        )
        
        # 3. LLM è¯„ä¼°å†²çª
        conflict_response = await self.llm_client.chat(
            system_prompt="You are a memory conflict detector.",
            messages=[{"role": "user", "content": evaluate_prompt}],
            response_model=ConflictResultSchema
        )
        
        conflicts = conflict_response.conflicts
        conflicts_found = len(conflicts)
        
        if conflicts_found == 0:
            return ReflectionResult(
                success=True,
                message="No conflicts found",
                conflicts_found=0
            )
        
        # 4. æ¸²æŸ“åæ€æç¤ºè¯
        reflexion_prompt = await self.render_reflexion_prompt_func(
            baseline=self.config.baseline,
            data=data,
            conflicts=conflicts,
            output_example=self.config.output_example
        )
        
        # 5. LLM ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
        reflexion_response = await self.llm_client.chat(
            system_prompt="You are a memory conflict resolver.",
            messages=[{"role": "user", "content": reflexion_prompt}],
            response_model=ReflexionResultSchema
        )
        
        # 6. åº”ç”¨æ›´æ”¹
        changes = reflexion_response.changes
        for change in changes:
            await self._apply_change(change)
        
        conflicts_resolved = len([c for c in changes if c.resolved])
        memories_updated = len(changes)
        
        # 7. è¿”å›ç»“æœ
        execution_time = time.time() - start_time
        
        return ReflectionResult(
            success=True,
            message=f"Resolved {conflicts_resolved} conflicts",
            conflicts_found=conflicts_found,
            conflicts_resolved=conflicts_resolved,
            memories_updated=memories_updated,
            execution_time=execution_time
        )
        
    except Exception as e:
        return ReflectionResult(
            success=False,
            message=str(e),
            execution_time=time.time() - start_time
        )
```

---

### 1.3 å†²çªæ£€æµ‹

**è¯„ä¼°æç¤ºè¯æ¨¡æ¿**ï¼š
```python
def render_evaluate_prompt(
    baseline: str,
    data: Dict[str, Any],
    output_example: Optional[str] = None
) -> str:
    """æ¸²æŸ“å†²çªè¯„ä¼°æç¤ºè¯"""
    
    if baseline == "TIME":
        prompt = """
You are a memory conflict detector.

Analyze the following memory items for temporal conflicts:

Memory Items:
{data}

Look for:
1. Contradictory statements about the same event at different times
2. Overlapping time ranges with conflicting information
3. Invalid temporal sequences (effect before cause)

Output format:
{{
  "conflicts": [
    {{
      "type": "temporal",
      "statement_ids": ["id1", "id2"],
      "description": "Description of the conflict",
      "severity": "high|medium|low"
    }}
  ]
}}
""".format(data=json.dumps(data, indent=2))
    
    elif baseline == "FACT":
        prompt = """
You are a memory conflict detector.

Analyze the following memory items for factual conflicts:

Memory Items:
{data}

Look for:
1. Contradictory facts about the same entity
2. Inconsistent relationships between entities
3. Logical impossibilities

Output format:
{{
  "conflicts": [
    {{
      "type": "factual",
      "statement_ids": ["id1", "id2"],
      "description": "Description of the conflict",
      "severity": "high|medium|low"
    }}
  ]
}}
""".format(data=json.dumps(data, indent=2))
    
    return prompt
```

**å†²çªç±»å‹**ï¼š
1. **Temporal Conflicts** - æ—¶é—´å†²çª
   - åŒä¸€äº‹ä»¶çš„ä¸åŒæ—¶é—´æè¿°
   - æ—¶é—´èŒƒå›´é‡å ä½†ä¿¡æ¯å†²çª
   - æ— æ•ˆæ—¶é—´åºåˆ—ï¼ˆç»“æœåœ¨åŸå› ä¹‹å‰ï¼‰

2. **Factual Conflicts** - äº‹å®å†²çª
   - åŒä¸€å®ä½“çš„çŸ›ç›¾äº‹å®
   - å®ä½“é—´å…³ç³»ä¸ä¸€è‡´
   - é€»è¾‘ä¸å¯èƒ½

**å†²çªä¸¥é‡æ€§**ï¼š
- `high`: ç›´æ¥çŸ›ç›¾ï¼Œå¿…é¡»è§£å†³
- `medium`: å¯èƒ½çŸ›ç›¾ï¼Œéœ€è¦éªŒè¯
- `low`: è½»å¾®ä¸ä¸€è‡´ï¼Œå¯å¿½ç•¥

---

### 1.4 å†²çªè§£å†³

**åæ€æç¤ºè¯æ¨¡æ¿**ï¼š
```python
def render_reflexion_prompt(
    baseline: str,
    data: Dict[str, Any],
    conflicts: List[Dict[str, Any]],
    output_example: Optional[str] = None
) -> str:
    """æ¸²æŸ“å†²çªè§£å†³æç¤ºè¯"""
    
    prompt = """
You are a memory conflict resolver.

Memory Items:
{data}

Detected Conflicts:
{conflicts}

For each conflict, propose a resolution:
1. Merge conflicting statements into a unified statement
2. Mark one statement as invalid (if clearly wrong)
3. Create a new statement that reconciles both

Output format:
{{
  "changes": [
    {{
      "conflict_id": 1,
      "action": "merge|invalidate|reconcile",
      "affected_statement_ids": ["id1", "id2"],
      "new_statement": {{
        "statement": "Unified statement text",
        "valid_at": "2026-01-01T00:00:00",
        "invalid_at": null,
        "importance_score": 0.8,
        "activation_value": 0.6
      }},
      "reasoning": "Why this resolution was chosen",
      "resolved": true
    }}
  ]
}}
""".format(
        data=json.dumps(data, indent=2),
        conflicts=json.dumps(conflicts, indent=2)
    )
    
    return prompt
```

**è§£å†³ç­–ç•¥**ï¼š
1. **Merge** - åˆå¹¶å†²çªé™ˆè¿°
   - åˆ›å»ºç»Ÿä¸€çš„æ–°é™ˆè¿°
   - ä¿ç•™æº¯æºä¿¡æ¯
   - ç»§æ‰¿è¾ƒé«˜çš„æ¿€æ´»å€¼

2. **Invalidate** - æ ‡è®°ä¸ºæ— æ•ˆ
   - è®¾ç½® `invalid_at` æ—¶é—´æˆ³
   - ä¿ç•™å†å²è®°å½•
   - é™ä½æ¿€æ´»å€¼

3. **Reconcile** - è°ƒå’Œå†²çª
   - åˆ›å»ºæ–°é™ˆè¿°è°ƒå’Œä¸¤è€…
   - ä¿ç•™åŸå§‹é™ˆè¿°
   - æ·»åŠ å…³ç³»è¾¹

---

### 1.5 åº”ç”¨æ›´æ”¹

**Cypher æ›´æ–°æŸ¥è¯¢**ï¼š
```python
UPDATE_QUERY = """
UNWIND $changes AS change
MATCH (s:Statement {id: change.statement_id})

// å¦‚æœæ˜¯åˆå¹¶æ“ä½œ
CASE WHEN change.action = 'merge' THEN
    // åˆ›å»ºæ–°é™ˆè¿°
    CREATE (new:Statement {
        id: change.new_statement.id,
        statement: change.new_statement.statement,
        valid_at: change.new_statement.valid_at,
        invalid_at: change.new_statement.invalid_at,
        importance_score: change.new_statement.importance_score,
        activation_value: change.new_statement.activation_value,
        end_user_id: s.end_user_id
    })
    // åˆ é™¤åŸå§‹é™ˆè¿°
    DETACH DELETE s
    
// å¦‚æœæ˜¯æ ‡è®°æ— æ•ˆæ“ä½œ
CASE WHEN change.action = 'invalidate' THEN
    SET s.invalid_at = change.invalid_at,
        s.activation_value = s.activation_value * 0.5

// å¦‚æœæ˜¯è°ƒå’Œæ“ä½œ
CASE WHEN change.action = 'reconcile' THEN
    // åˆ›å»ºè°ƒå’Œé™ˆè¿°
    CREATE (new:Statement {
        id: change.new_statement.id,
        statement: change.new_statement.statement,
        ...
    })
    // ä¿ç•™åŸå§‹é™ˆè¿°ï¼Œæ·»åŠ å…³ç³»è¾¹
    CREATE (s)-[r:RECONCILED_BY]->(new)
"""
```

---

## 2ï¸âƒ£ é—å¿˜è°ƒåº¦å™¨ï¼ˆForgetting Schedulerï¼‰

### 2.1 æ ¸å¿ƒæ¶æ„

**æ–‡ä»¶**ï¼š[`app/core/memory/storage_services/forgetting_engine/forgetting_scheduler.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/core/memory/storage_services/forgetting_engine/forgetting_scheduler.py) (13.6KB)

**ç±»ç»“æ„**ï¼š
```python
class ForgettingScheduler:
    """é—å¿˜è°ƒåº¦å™¨"""
    
    def __init__(
        self,
        forgetting_strategy: ForgettingStrategy,
        connector: Neo4jConnector
    ):
        self.forgetting_strategy = forgetting_strategy
        self.connector = connector
        self.is_running = False
    
    async def run_forgetting_cycle(
        self,
        end_user_id: Optional[str] = None,
        max_merge_batch_size: int = 100,
        min_days_since_access: int = 30
    ) -> Dict[str, Any]:
        """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„é—å¿˜å‘¨æœŸ"""
```

**èŒè´£**ï¼š
1. æ‰‹åŠ¨è§¦å‘é—å¿˜å‘¨æœŸ
2. æ‰¹é‡å¤„ç†å¯é—å¿˜èŠ‚ç‚¹ï¼ˆé™åˆ¶æ‰¹é‡å¤§å°ï¼‰
3. æŒ‰æ¿€æ´»å€¼ä¼˜å…ˆçº§æ’åºï¼ˆæ¿€æ´»å€¼æœ€ä½çš„ä¼˜å…ˆï¼‰
4. è¿›åº¦è·Ÿè¸ªå’Œæ—¥å¿—è®°å½•
5. ç”Ÿæˆé—å¿˜æŠ¥å‘Š

---

### 2.2 é—å¿˜å‘¨æœŸæµç¨‹

**å®Œæ•´æµç¨‹**ï¼š
```
1. æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ â†’ is_running
   â†“
2. ç»Ÿè®¡é—å¿˜å‰èŠ‚ç‚¹æ•°é‡ â†’ _count_knowledge_nodes()
   â†“
3. è¯†åˆ«å¯é—å¿˜èŠ‚ç‚¹å¯¹ â†’ find_forgettable_nodes()
   â†“
4. æŒ‰æ¿€æ´»å€¼æ’åº â†’ sorted by avg_activation ASC
   â†“
5. é™åˆ¶æ‰¹é‡å¤§å° â†’ [:max_merge_batch_size]
   â†“
6. å»é‡ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰â†’ processed_statement_ids
   â†“
7. æ‰¹é‡èåˆèŠ‚ç‚¹ â†’ merge_nodes_to_summary()
   â†“
8. è®°å½•è¿›åº¦ï¼ˆæ¯ 10%ï¼‰â†’ logger.info()
   â†“
9. ç”Ÿæˆé—å¿˜æŠ¥å‘Š â†’ Dict[str, Any]
```

**ä»£ç å®ç°**ï¼š
```python
async def run_forgetting_cycle(
    self,
    end_user_id: Optional[str] = None,
    max_merge_batch_size: int = 100,
    min_days_since_access: int = 30
) -> Dict[str, Any]:
    """è¿è¡Œä¸€æ¬¡å®Œæ•´çš„é—å¿˜å‘¨æœŸ"""
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰é—å¿˜å‘¨æœŸåœ¨è¿è¡Œ
    if self.is_running:
        raise RuntimeError("é—å¿˜å‘¨æœŸå·²åœ¨è¿è¡Œä¸­")
    
    self.is_running = True
    start_time = datetime.now()
    
    try:
        # æ­¥éª¤ 1ï¼šç»Ÿè®¡é—å¿˜å‰çš„èŠ‚ç‚¹æ•°é‡
        nodes_before = await self._count_knowledge_nodes(end_user_id)
        logger.info(f"é—å¿˜å‰èŠ‚ç‚¹æ€»æ•°ï¼š{nodes_before}")
        
        # æ­¥éª¤ 2ï¼šè¯†åˆ«å¯é—å¿˜çš„èŠ‚ç‚¹å¯¹
        forgettable_pairs = await self.forgetting_strategy.find_forgettable_nodes(
            end_user_id=end_user_id,
            min_days_since_access=min_days_since_access
        )
        
        total_forgettable = len(forgettable_pairs)
        logger.info(f"è¯†åˆ«åˆ° {total_forgettable} ä¸ªå¯é—å¿˜èŠ‚ç‚¹å¯¹")
        
        if total_forgettable == 0:
            logger.info("æ²¡æœ‰å¯é—å¿˜çš„èŠ‚ç‚¹å¯¹ï¼Œé—å¿˜å‘¨æœŸç»“æŸ")
            return empty_report
        
        # æ­¥éª¤ 3ï¼šæŒ‰æ¿€æ´»å€¼æ’åºï¼ˆæ¿€æ´»å€¼æœ€ä½çš„ä¼˜å…ˆï¼‰
        sorted_pairs = sorted(
            forgettable_pairs,
            key=lambda x: x['avg_activation']
        )
        
        # æ­¥éª¤ 4ï¼šé™åˆ¶æ‰¹é‡å¤§å°
        pairs_to_process = sorted_pairs[:max_merge_batch_size]
        actual_batch_size = len(pairs_to_process)
        
        # æ­¥éª¤ 5ï¼šå»é‡ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
        processed_statement_ids = set()
        processed_entity_ids = set()
        unique_pairs = []
        
        for pair in pairs_to_process:
            if (pair['statement_id'] not in processed_statement_ids and
                pair['entity_id'] not in processed_entity_ids):
                unique_pairs.append(pair)
                processed_statement_ids.add(pair['statement_id'])
                processed_entity_ids.add(pair['entity_id'])
        
        # æ­¥éª¤ 6ï¼šæ‰¹é‡èåˆèŠ‚ç‚¹
        merged_count = 0
        failed_count = 0
        progress_interval = max(1, actual_batch_size // 10)
        
        for i, pair in enumerate(unique_pairs):
            try:
                await self.forgetting_strategy.merge_nodes_to_summary(
                    statement_node=pair,
                    entity_node=pair,
                    config_id=config_id,
                    db=db
                )
                merged_count += 1
            except Exception as e:
                failed_count += 1
                logger.error(f"èåˆå¤±è´¥ï¼š{e}")
            
            # è®°å½•è¿›åº¦ï¼ˆæ¯ 10%ï¼‰
            if (i + 1) % progress_interval == 0:
                progress = (i + 1) / actual_batch_size * 100
                logger.info(f"é—å¿˜è¿›åº¦ï¼š{progress:.1f}% ({i+1}/{actual_batch_size})")
        
        # æ­¥éª¤ 7ï¼šç»Ÿè®¡é—å¿˜åèŠ‚ç‚¹æ•°é‡
        nodes_after = await self._count_knowledge_nodes(end_user_id)
        
        # æ­¥éª¤ 8ï¼šç”Ÿæˆé—å¿˜æŠ¥å‘Š
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        reduction_rate = (nodes_before - nodes_after) / nodes_before if nodes_before > 0 else 0
        success_rate = merged_count / actual_batch_size if actual_batch_size > 0 else 0
        
        report = {
            'merged_count': merged_count,
            'nodes_before': nodes_before,
            'nodes_after': nodes_after,
            'reduction_rate': reduction_rate,
            'duration_seconds': duration,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'failed_count': failed_count,
            'success_rate': success_rate
        }
        
        logger.info(
            f"é—å¿˜å‘¨æœŸå®Œæˆï¼š"
            f"èåˆ {merged_count} ä¸ªèŠ‚ç‚¹å¯¹ï¼Œ"
            f"å‡å°‘ç‡ {reduction_rate:.2%}ï¼Œ"
            f"è€—æ—¶ {duration:.2f}ç§’"
        )
        
        return report
        
    finally:
        self.is_running = False
```

---

### 2.3 é—å¿˜æŠ¥å‘Š

**æŠ¥å‘Šæ ¼å¼**ï¼š
```json
{
  "merged_count": 45,              // èåˆçš„èŠ‚ç‚¹å¯¹æ•°é‡
  "nodes_before": 1250,            // é—å¿˜å‰çš„èŠ‚ç‚¹æ€»æ•°
  "nodes_after": 1205,             // é—å¿˜åçš„èŠ‚ç‚¹æ€»æ•°
  "reduction_rate": 0.036,         // èŠ‚ç‚¹å‡å°‘ç‡ï¼ˆ3.6%ï¼‰
  "duration_seconds": 125.5,       // æ‰§è¡Œè€—æ—¶ï¼ˆç§’ï¼‰
  "start_time": "2026-02-28T10:00:00",
  "end_time": "2026-02-28T10:02:05",
  "failed_count": 2,               // å¤±è´¥çš„èåˆæ•°é‡
  "success_rate": 0.957            // æˆåŠŸç‡ï¼ˆ95.7%ï¼‰
}
```

**å…³é”®æŒ‡æ ‡**ï¼š
- `reduction_rate`: èŠ‚ç‚¹å‡å°‘ç‡ï¼ˆé€šå¸¸ 3-5%ï¼‰
- `success_rate`: èåˆæˆåŠŸç‡ï¼ˆé€šå¸¸ >90%ï¼‰
- `duration_seconds`: æ‰§è¡Œæ—¶é—´ï¼ˆå–å†³äºæ‰¹é‡å¤§å°ï¼‰

---

## 3ï¸âƒ£ é—å¿˜ç­–ç•¥ï¼ˆForgetting Strategyï¼‰

### 3.1 æ ¸å¿ƒæ¶æ„

**æ–‡ä»¶**ï¼š[`app/core/memory/storage_services/forgetting_engine/forgetting_strategy.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/core/memory/storage_services/forgetting_engine/forgetting_strategy.py) (24.7KB)

**ç±»ç»“æ„**ï¼š
```python
class ForgettingStrategy:
    """é—å¿˜ç­–ç•¥æ‰§è¡Œå™¨"""
    
    def __init__(
        self,
        connector: Neo4jConnector,
        actr_calculator: ACTRCalculator,
        forgetting_threshold: float = 0.3,
        enable_llm_summary: bool = True
    ):
        self.connector = connector
        self.actr_calculator = actr_calculator
        self.forgetting_threshold = forgetting_threshold
        self.enable_llm_summary = enable_llm_summary
```

**èŒè´£**ï¼š
1. è¯†åˆ«ä½æ¿€æ´»å€¼çš„èŠ‚ç‚¹å¯¹ï¼ˆStatement-Entityï¼‰
2. å°†ä½æ¿€æ´»å€¼èŠ‚ç‚¹èåˆä¸º MemorySummary èŠ‚ç‚¹
3. ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
4. ä¿ç•™æº¯æºä¿¡æ¯å¹¶åˆ é™¤åŸå§‹èŠ‚ç‚¹

---

### 3.2 è¯†åˆ«å¯é—å¿˜èŠ‚ç‚¹

**Cypher æŸ¥è¯¢**ï¼š
```python
async def find_forgettable_nodes(
    self,
    end_user_id: Optional[str] = None,
    min_days_since_access: int = 30
) -> List[Dict[str, Any]]:
    """è¯†åˆ«å¯é—å¿˜çš„èŠ‚ç‚¹å¯¹"""
    
    # è®¡ç®—æ—¶é—´é˜ˆå€¼
    cutoff_time = datetime.now() - timedelta(days=min_days_since_access)
    
    # Cypher æŸ¥è¯¢
    query = """
    MATCH (s:Statement)-[r]-(e:ExtractedEntity)
    WHERE s.activation_value IS NOT NULL
      AND e.activation_value IS NOT NULL
      AND s.activation_value < $threshold
      AND e.activation_value < $threshold
      AND s.last_access_time < $cutoff_time
      AND e.last_access_time < $cutoff_time
      AND (e.entity_type IS NULL OR e.entity_type <> 'Person')
    """
    
    if end_user_id:
        query += " AND s.end_user_id = $end_user_id AND e.end_user_id = $end_user_id"
    
    query += """
    RETURN s.id as statement_id,
           s.statement as statement_text,
           s.activation_value as statement_activation,
           s.importance_score as statement_importance,
           s.last_access_time as statement_last_access,
           e.id as entity_id,
           e.name as entity_name,
           e.entity_type as entity_type,
           e.activation_value as entity_activation,
           e.importance_score as entity_importance,
           e.last_access_time as entity_last_access,
           (s.activation_value + e.activation_value) / 2.0 as avg_activation
    ORDER BY avg_activation ASC
    """
    
    params = {
        'threshold': self.forgetting_threshold,
        'cutoff_time': cutoff_time.isoformat()
    }
    if end_user_id:
        params['end_user_id'] = end_user_id
    
    results = await self.connector.execute_query(query, **params)
    
    return results
```

**é—å¿˜æ¡ä»¶**ï¼š
1. **æ¿€æ´»å€¼ä½**ï¼š`activation_value < 0.3`
2. **é•¿æœŸæœªè®¿é—®**ï¼š`last_access_time < 30 days ago`
3. **å­˜åœ¨å…³ç³»è¾¹**ï¼š`Statement-Entity` ä¹‹é—´æœ‰è¾¹
4. **éäººç‰©å®ä½“**ï¼š`entity_type <> 'Person'`ï¼ˆäººç‰©ä¸é—å¿˜ï¼‰

**æ’åºç­–ç•¥**ï¼š
- `ORDER BY avg_activation ASC`
- æ¿€æ´»å€¼æœ€ä½çš„èŠ‚ç‚¹å¯¹ä¼˜å…ˆå¤„ç†

---

### 3.3 èŠ‚ç‚¹èåˆ

**èåˆæµç¨‹**ï¼š
```
1. è¯»å– Statement å’Œ Entity èŠ‚ç‚¹
   â†“
2. ç”Ÿæˆæ‘˜è¦å†…å®¹ï¼ˆLLM æˆ–æ‹¼æ¥ï¼‰
   â†“
3. åˆ›å»º MemorySummary èŠ‚ç‚¹
   â†“
4. ç»§æ‰¿è¾ƒé«˜çš„æ¿€æ´»å€¼å’Œé‡è¦æ€§
   â†“
5. ä¿ç•™æº¯æºä¿¡æ¯ï¼ˆoriginal IDsï¼‰
   â†“
6. åˆ é™¤åŸå§‹ Statement å’Œ Entity èŠ‚ç‚¹
```

**ä»£ç å®ç°**ï¼š
```python
async def merge_nodes_to_summary(
    self,
    statement_node: Dict[str, Any],
    entity_node: Dict[str, Any],
    config_id: Optional[UUID] = None,
    db = None
) -> str:
    """å°† Statement å’Œ Entity èŠ‚ç‚¹èåˆä¸º MemorySummary èŠ‚ç‚¹"""
    
    # 1. ç”Ÿæˆæ‘˜è¦å†…å®¹
    if self.enable_llm_summary:
        # ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦
        summary_text = await self._generate_llm_summary(
            statement_text=statement_node['statement_text'],
            entity_name=entity_node['entity_name'],
            entity_type=entity_node['entity_type'],
            config_id=config_id,
            db=db
        )
    else:
        # é™çº§åˆ°ç®€å•æ‹¼æ¥
        summary_text = f"{entity_node['entity_name']} ({entity_node['entity_type']}): {statement_node['statement_text']}"
    
    # 2. ç»§æ‰¿è¾ƒé«˜çš„æ¿€æ´»å€¼å’Œé‡è¦æ€§
    inherited_activation = max(
        statement_node['statement_activation'],
        entity_node['entity_activation']
    )
    inherited_importance = max(
        statement_node['statement_importance'],
        entity_node['entity_importance']
    )
    
    # 3. åˆ›å»º MemorySummary èŠ‚ç‚¹
    create_query = """
    CREATE (ms:MemorySummary {
        id: randomUUID(),
        content: $summary_text,
        created_at: datetime(),
        expired_at: null,
        end_user_id: $end_user_id,
        importance_score: $inherited_importance,
        activation_value: $inherited_activation,
        access_history: [],
        last_access_time: null,
        access_count: 0,
        original_statement_id: $statement_id,
        original_entity_id: $entity_id
    })
    """
    
    params = {
        'summary_text': summary_text,
        'end_user_id': statement_node.get('end_user_id'),
        'inherited_importance': inherited_importance,
        'inherited_activation': inherited_activation,
        'statement_id': statement_node['statement_id'],
        'entity_id': entity_node['entity_id']
    }
    
    await self.connector.execute_query(create_query, **params)
    
    # 4. åˆ é™¤åŸå§‹èŠ‚ç‚¹
    delete_query = """
    MATCH (s:Statement {id: $statement_id})
    MATCH (e:ExtractedEntity {id: $entity_id})
    DETACH DELETE s, e
    """
    
    await self.connector.execute_query(delete_query, **params)
    
    return summary_text
```

---

### 3.4 LLM æ‘˜è¦ç”Ÿæˆ

**æç¤ºè¯æ¨¡æ¿**ï¼š
```python
async def _generate_llm_summary(
    self,
    statement_text: str,
    entity_name: str,
    entity_type: str,
    config_id: Optional[UUID] = None,
    db = None
) -> str:
    """ä½¿ç”¨ LLM ç”Ÿæˆé«˜è´¨é‡æ‘˜è¦"""
    
    # è·å– LLM å®¢æˆ·ç«¯
    llm_client = self._get_llm_client(config_id, db)
    
    # æç¤ºè¯
    prompt = f"""
You are a memory summarization expert.

Merge the following Statement and Entity into a concise MemorySummary:

Statement: {statement_text}
Entity: {entity_name} ({entity_type})

Requirements:
1. Preserve all key information
2. Make it self-contained (no pronouns without clear antecedents)
3. Keep it concise (1-3 sentences)
4. Maintain temporal and causal relationships
5. Use natural language

Output only the summary text, no explanations.
"""
    
    # è°ƒç”¨ LLM
    response = await llm_client.chat(
        system_prompt="You are a memory summarization expert.",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,  # ä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
        max_tokens=200
    )
    
    summary_text = response.content.strip()
    
    return summary_text
```

**æ‘˜è¦è¦æ±‚**ï¼š
1. ä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯
2. è‡ªåŒ…å«ï¼ˆæ— ä»£è¯æ­§ä¹‰ï¼‰
3. ç®€æ´ï¼ˆ1-3 å¥è¯ï¼‰
4. ä¿æŒæ—¶é—´å’Œå› æœå…³ç³»
5. ä½¿ç”¨è‡ªç„¶è¯­è¨€

**ç¤ºä¾‹**ï¼š
```
è¾“å…¥:
- Statement: "å¼ ä¸‰åœ¨ 2025 å¹´ 3 æœˆåŠ å…¥äº† ABC å…¬å¸ï¼Œæ‹…ä»»è½¯ä»¶å·¥ç¨‹å¸ˆ"
- Entity: "å¼ ä¸‰ (Person)"

è¾“å‡º:
"å¼ ä¸‰äº 2025 å¹´ 3 æœˆåŠ å…¥ ABC å…¬å¸ï¼Œæ‹…ä»»è½¯ä»¶å·¥ç¨‹å¸ˆèŒä½ã€‚"
```

---

## 4ï¸âƒ£ æ€§èƒ½ä¼˜åŒ–

### 4.1 æ‰¹é‡å¤„ç†

**æ‰¹é‡å¤§å°é™åˆ¶**ï¼š
```python
max_merge_batch_size = 100  # é»˜è®¤å€¼
```

**åŸå› **ï¼š
- é¿å…å•æ¬¡äº‹åŠ¡è¿‡å¤§
- å‡å°‘å†…å­˜å ç”¨
- ä¾¿äºè¿›åº¦è·Ÿè¸ª

**æ€§èƒ½å¯¹æ¯”**ï¼š
| æ‰¹é‡å¤§å° | è€—æ—¶ | å†…å­˜å ç”¨ | æ¨èåœºæ™¯ |
|---------|------|---------|---------|
| 10 | ~15 ç§’ | ä½ | æµ‹è¯• |
| 100 | ~125 ç§’ | ä¸­ | ç”Ÿäº§ï¼ˆæ¨èï¼‰ |
| 1000 | ~1200 ç§’ | é«˜ | å¤§è§„æ¨¡æ¸…ç† |

---

### 4.2 å»é‡ä¼˜åŒ–

**é—®é¢˜**ï¼šåŒä¸€èŠ‚ç‚¹å¯èƒ½å‡ºç°åœ¨å¤šä¸ªèŠ‚ç‚¹å¯¹ä¸­

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
processed_statement_ids = set()
processed_entity_ids = set()

unique_pairs = []
for pair in pairs_to_process:
    if (pair['statement_id'] not in processed_statement_ids and
        pair['entity_id'] not in processed_entity_ids):
        unique_pairs.append(pair)
        processed_statement_ids.add(pair['statement_id'])
        processed_entity_ids.add(pair['entity_id'])
```

**æ•ˆæœ**ï¼š
- é¿å…é‡å¤å¤„ç†åŒä¸€èŠ‚ç‚¹
- å‡å°‘ 10-20% çš„å¤„ç†é‡
- é˜²æ­¢å¹¶å‘å†²çª

---

### 4.3 è¿›åº¦è·Ÿè¸ª

**æ—¥å¿—è®°å½•**ï¼š
```python
progress_interval = max(1, actual_batch_size // 10)

for i, pair in enumerate(unique_pairs):
    # ... å¤„ç†èŠ‚ç‚¹å¯¹ ...
    
    # æ¯ 10% è®°å½•ä¸€æ¬¡è¿›åº¦
    if (i + 1) % progress_interval == 0:
        progress = (i + 1) / actual_batch_size * 100
        logger.info(f"é—å¿˜è¿›åº¦ï¼š{progress:.1f}% ({i+1}/{actual_batch_size})")
```

**æ—¥å¿—è¾“å‡ºç¤ºä¾‹**ï¼š
```
INFO - é—å¿˜è¿›åº¦ï¼š10.0% (10/100)
INFO - é—å¿˜è¿›åº¦ï¼š20.0% (20/100)
INFO - é—å¿˜è¿›åº¦ï¼š30.0% (30/100)
...
INFO - é—å¿˜è¿›åº¦ï¼š100.0% (100/100)
```

---

## 5ï¸âƒ£ è°ƒåº¦å™¨é›†æˆ

### 5.1 Celery Beat å®šæœŸä»»åŠ¡

**æ–‡ä»¶**ï¼š[`app/tasks.py`](https://github.com/qudi17/MemoryBear/blob/main/api/app/tasks.py)

**å®šæœŸä»»åŠ¡é…ç½®**ï¼š
```python
@app.task
def run_forgetting_cycle_task(
    end_user_id: Optional[str] = None,
    max_merge_batch_size: int = 100,
    min_days_since_access: int = 30
):
    """å®šæœŸè¿è¡Œé—å¿˜å‘¨æœŸï¼ˆCelery Beat è°ƒåº¦ï¼‰"""
    
    # åˆ›å»ºè¿æ¥å™¨å’Œç­–ç•¥
    connector = Neo4jConnector()
    actr_calculator = ACTRCalculator()
    forgetting_strategy = ForgettingStrategy(connector, actr_calculator)
    scheduler = ForgettingScheduler(forgetting_strategy, connector)
    
    try:
        # è¿è¡Œé—å¿˜å‘¨æœŸ
        report = asyncio.run(scheduler.run_forgetting_cycle(
            end_user_id=end_user_id,
            max_merge_batch_size=max_merge_batch_size,
            min_days_since_access=min_days_since_access
        ))
        
        logger.info(f"é—å¿˜å‘¨æœŸå®Œæˆï¼š{report}")
        
    except Exception as e:
        logger.error(f"é—å¿˜å‘¨æœŸå¤±è´¥ï¼š{e}")
        raise
```

**Celery Beat é…ç½®**ï¼š
```python
beat_schedule = {
    'run-forgetting-cycle-weekly': {
        'task': 'app.tasks.run_forgetting_cycle_task',
        'schedule': crontab(hour=3, minute=0, day_of_week='sunday'),  # æ¯å‘¨æ—¥å‡Œæ™¨ 3 ç‚¹
        'options': {
            'max_merge_batch_size': 100,
            'min_days_since_access': 30
        }
    },
    'run-reflection-monthly': {
        'task': 'app.tasks.run_reflection_task',
        'schedule': crontab(hour=2, minute=0, day_of_month=1),  # æ¯æœˆ 1 æ—¥å‡Œæ™¨ 2 ç‚¹
    }
}
```

---

## 6ï¸âƒ£ å…³é”®å‘ç°

### 6.1 é—å¿˜é˜ˆå€¼è°ƒä¼˜

**æ¨èé…ç½®**ï¼š
```python
ForgettingStrategy(
    forgetting_threshold=0.3,      # æ¿€æ´»å€¼ä½äº 0.3 å¯é—å¿˜
    enable_llm_summary=True,       # å¯ç”¨ LLM æ‘˜è¦
    min_days_since_access=30       # 30 å¤©æœªè®¿é—®
)
```

**è°ƒä¼˜å»ºè®®**ï¼š
- é™ä½ `forgetting_threshold`: æ›´æ¿€è¿›é—å¿˜ï¼ˆèŠ‚çœå­˜å‚¨ï¼‰
- å¢åŠ  `min_days_since_access`: æ›´ä¿å®ˆé—å¿˜ï¼ˆä¿ç•™æ›´å¤šï¼‰
- ç¦ç”¨ `enable_llm_summary`: æé«˜é€Ÿåº¦ï¼ˆä½†æ‘˜è¦è´¨é‡ä¸‹é™ï¼‰

---

### 6.2 åæ€å‘¨æœŸé…ç½®

**æ¨èé…ç½®**ï¼š
```python
ReflectionConfig(
    enabled=True,
    iteration_period="3",          # æ¯ 3 å¤©åæ€ä¸€æ¬¡
    reflexion_range="partial",     # ä»æ£€ç´¢ç»“æœåæ€
    baseline="TIME",               # åŸºäºæ—¶é—´åæ€
    memory_verify=True,            # å¯ç”¨è®°å¿†éªŒè¯
    quality_assessment=True        # å¯ç”¨è´¨é‡è¯„ä¼°
)
```

**å‘¨æœŸé€‰æ‹©**ï¼š
- `1-3 å¤©`: é«˜é¢‘åæ€ï¼ˆé€‚åˆæ´»è·ƒç”¨æˆ·ï¼‰
- `7-14 å¤©`: ä¸­é¢‘åæ€ï¼ˆé€‚åˆæ™®é€šç”¨æˆ·ï¼‰
- `30 å¤©`: ä½é¢‘åæ€ï¼ˆé€‚åˆå½’æ¡£ç”¨æˆ·ï¼‰

---

### 6.3 LLM æˆæœ¬ä¼˜åŒ–

**ä¼˜åŒ–ç­–ç•¥**ï¼š
1. **æ‰¹é‡å¤„ç†**ï¼šå¤šä¸ªå†²çªåˆå¹¶ä¸ºä¸€ä¸ªæç¤ºè¯
2. **é™ä½æ¸©åº¦**ï¼š`temperature=0.3`ï¼ˆå‡å°‘ token æ¶ˆè€—ï¼‰
3. **é™åˆ¶é•¿åº¦**ï¼š`max_tokens=200`ï¼ˆæ§åˆ¶è¾“å‡ºé•¿åº¦ï¼‰
4. **é™çº§ç­–ç•¥**ï¼šLLM å¤±è´¥æ—¶é™çº§åˆ°ç®€å•æ‹¼æ¥

**æˆæœ¬ä¼°ç®—**ï¼š
- æ¯æ¬¡é—å¿˜å‘¨æœŸï¼š~50 æ¬¡ LLM è°ƒç”¨
- æ¯æ¬¡è°ƒç”¨ï¼š~200 tokens
- æ€»æˆæœ¬ï¼š~10,000 tokens/å‘¨æœŸ
- æŒ‰ GPT-4 è®¡ä»·ï¼š~$0.30/å‘¨æœŸ

---

## ğŸ“‹ å¾…ç ”ç©¶åˆ†æ”¯

- [ ] **æƒ…ç»ªå½±å“** - æƒ…ç»ªå¯¹è®°å¿†å¼ºåº¦çš„å½±å“
- [ ] **éƒ¨åˆ†åŒ¹é…æƒ©ç½š** - ACT-R çš„ PC_k å‚æ•°å®ç°
- [ ] **å‘é‡ç´¢å¼•ä¼˜åŒ–** - Neo4j å‘é‡ç´¢å¼•æ€§èƒ½
- [ ] **å¤šç”¨æˆ·éš”ç¦»** - å¤§è§„æ¨¡å¤šç”¨æˆ·åœºæ™¯ä¼˜åŒ–

---

**ç ”ç©¶äºº**ï¼šJarvis  
**æ—¥æœŸ**ï¼š2026-02-28  
**æ–¹æ³•**ï¼šæ¯›çº¿å›¢ç ”ç©¶æ³•ï¼ˆåˆ†æ”¯æ·±å…¥ï¼‰  
**çŠ¶æ€**ï¼šâœ… åæ€å¼•æ“ + é—å¿˜è°ƒåº¦å™¨ + é—å¿˜ç­–ç•¥å®Œæˆ
