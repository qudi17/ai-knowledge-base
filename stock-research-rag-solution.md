# å¤§è§„æ¨¡è‚¡ç¥¨ç ”ç©¶æŠ¥å‘Š RAG è§£å†³æ–¹æ¡ˆ

**æ•°æ®è§„æ¨¡**ï¼š100ä¸‡ä»½æŠ¥å‘Š Ã— 4-5é¡µ = 400-500ä¸‡é¡µ
**æ€»æ–‡æœ¬é‡**ï¼šçº¦ 80-100äº¿ä¸­æ–‡å­—ç¬¦ï¼ˆå‡è®¾æ¯é¡µ 2000 å­—ï¼‰
**æ•°æ®æ ¼å¼**ï¼šPDF

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

```
æ•°æ®å±‚ (100ä¸‡ä»½ PDF)
    â†“
  è§£æå±‚
    â†“
  æ¸…æ´—å±‚
    â†“
  åˆ†å—å±‚ (chunking)
    â†“
  å‘é‡åŒ–å±‚
    â†“
  å­˜å‚¨/ç´¢å¼•å±‚
    â†“
  æ£€ç´¢å±‚
    â†“
  æŸ¥è¯¢å±‚
```

---

## ğŸ“Š æ–¹æ¡ˆé€‰æ‹©

| ç»„ä»¶ | æ¨èæ–¹æ¡ˆ | ç†ç”± |
|---|---|---|
| PDF è§£æ | **pdfplumber** / **PyMuPDF** | ä¸­æ–‡æ”¯æŒå¥½ï¼Œé€Ÿåº¦å¿« |
| æ–‡æœ¬æ¸…æ´— | **jieba** + æ­£åˆ™ | ä¸­æ–‡åˆ†è¯ï¼Œå»é™¤å™ªå£° |
| åˆ†å—ç­–ç•¥ | **è¯­ä¹‰åˆ†å—** + **é‡å ** | æ›´å¥½çš„æ£€ç´¢æ•ˆæœ |
| å‘é‡åµŒå…¥ | **æ™ºè°± GLM** / **OpenAI** | ä¸­æ–‡æ•ˆæœä¼˜ç§€ |
| ç´¢å¼•å­˜å‚¨ | **Weaviate** / **Milvus** / **Chroma** | å¯æ‰©å±•ï¼Œæ”¯æŒå¤§è§„æ¨¡ |
| æ£€ç´¢å¼•æ“ | **Hybrid** (BM25 + å‘é‡) | æå‡å¬å›ç‡ |
| ç¼“å­˜å±‚ | **Redis** / **å†…å­˜ç¼“å­˜** | å‡å°‘é‡å¤è®¡ç®— |

---

## ğŸ”„ å®Œæ•´æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å‡†å¤‡é˜¶æ®µ                            â”‚
â”‚  â€¢ 100ä¸‡ä»½ PDF ç ”æŠ¥                                       â”‚
â”‚  â€¢ æ‰«ææ–‡ä»¶ç³»ç»Ÿ                                           â”‚
â”‚  â€¢ éªŒè¯æ–‡ä»¶å®Œæ•´æ€§                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è§£æé˜¶æ®µ                               â”‚
â”‚  â€¢ pdfplumber é€é¡µè§£æ                                    â”‚
â”‚  â€¢ æå–æ–‡æœ¬ + å…ƒæ•°æ®ï¼ˆå…¬å¸ã€è¡Œä¸šã€æ—¥æœŸï¼‰                     â”‚
â”‚  â€¢ è¯†åˆ«è¡¨æ ¼å’Œå…¬å¼                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ¸…æ´—é˜¶æ®µ                               â”‚
â”‚  â€¢ å»é™¤å¤šä½™ç©ºç™½                                            â”‚
â”‚  â€¢ ä¸­æ–‡åˆ†è¯ (jieba)                                        â”‚
â”‚  â€¢ è¿‡æ»¤å™ªå£°ï¼ˆé¡µçœ‰ã€é¡µè„šã€å¹¿å‘Šï¼‰                             â”‚
â”‚  â€¢ æ ‡å‡†åŒ–æ ‡ç‚¹                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åˆ†å—é˜¶æ®µ                               â”‚
â”‚  â€¢ è¯­ä¹‰åˆ†å— (åŸºäºå¥å­è¾¹ç•Œ)                                 â”‚
â”‚  â€¢ æ¯å— 500-1000 å­—                                      â”‚
â”‚  â€¢ é‡å  50-100 å­—                                         â”‚
â”‚  â€¢ æ·»åŠ å…ƒæ•°æ®æ ‡ç­¾                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å‘é‡åŒ–é˜¶æ®µ                              â”‚
â”‚  â€¢ è°ƒç”¨åµŒå…¥ API (GLM-4 / OpenAI)                          â”‚
â”‚  â€¢ æ‰¹é‡å¤„ç† (50-100 æ¡/æ‰¹æ¬¡)                               â”‚
â”‚  â€¢ é”™è¯¯é‡è¯• + é™çº§å¤„ç†                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç´¢å¼•æ„å»ºé˜¶æ®µ                            â”‚
â”‚  â€¢ ä¿å­˜å‘é‡ + å…ƒæ•°æ®                                      â”‚
â”‚  â€¢ å»ºç«‹ BM25 ç´¢å¼•                                         â”‚
â”‚  â€¢ æ•°æ®é¢„è®¡ç®—                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ£€ç´¢é˜¶æ®µ                               â”‚
â”‚  â€¢ æ··åˆæ£€ç´¢ (å‘é‡ + BM25)                                  â”‚
â”‚  â€¢ é‡æ’åº (Cross-Encoder)                                  â”‚
â”‚  â€¢ ç»“æœåˆå¹¶                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æŸ¥è¯¢é˜¶æ®µ                               â”‚
â”‚  â€¢ ç”¨æˆ·æé—®                                               â”‚
â”‚  â€¢ æ£€ç´¢ç›¸å…³æ–‡æ¡£                                           â”‚
â”‚  â€¢ ç”Ÿæˆç­”æ¡ˆ                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» å…³é”®æ­¥éª¤ä»£ç å®ç°

### æ­¥éª¤ 1: PDF è§£æ

```python
# pdf_parser.py
import pdfplumber
from typing import List, Dict
import re
from pathlib import Path

class PDFParser:
    """PDF è§£æå™¨ - æå–æ–‡æœ¬å’Œå…ƒæ•°æ®"""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def parse_file(self, file_path: Path) -> List[Dict]:
        """è§£æå•ä¸ª PDF æ–‡ä»¶"""
        text_chunks = []
        metadata = {
            "file_name": file_path.name,
            "file_path": str(file_path),
            "page_count": 0
        }

        with pdfplumber.open(file_path) as pdf:
            for i, page in enumerate(pdf.pages):
                # æå–é¡µé¢æ–‡æœ¬
                page_text = self._extract_text(page)
                metadata["page_count"] += 1

                # æå–å…ƒæ•°æ®ï¼ˆé¡µçœ‰ã€é¡µè„šã€æ—¥æœŸç­‰ï¼‰
                page_metadata = self._extract_metadata(page, i + 1)

                # åˆ†å—
                chunks = self._chunk_text(page_text, metadata, page_metadata)
                text_chunks.extend(chunks)

        print(f"âœ“ è§£æå®Œæˆ: {file_path.name} -> {len(text_chunks)} ä¸ªå—")
        return text_chunks

    def _extract_text(self, page) -> str:
        """æå–é¡µé¢æ–‡æœ¬"""
        # ä¿ç•™æ®µè½ç»“æ„
        text = page.extract_text(layout=True)
        # å»é™¤å¤šä½™çš„ç©ºç™½
        text = re.sub(r'\n+', '\n', text)
        return text.strip()

    def _extract_metadata(self, page, page_num: int) -> Dict:
        """æå–é¡µé¢å…ƒæ•°æ®"""
        metadata = {"page_num": page_num}

        # å°è¯•æå–æ—¥æœŸï¼ˆå¸¸è§æ ¼å¼ï¼‰
        date_pattern = r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)'
        dates = re.findall(date_pattern, page.extract_text() or "")
        if dates:
            metadata["date"] = dates[-1]

        # å°è¯•æå–å…¬å¸åç§°
        company_pattern = r'([A-Za-z0-9\u4e00-\u9fa5]{2,20})ç ”ç©¶æŠ¥å‘Š'
        companies = re.findall(company_pattern, page.extract_text() or "")
        if companies:
            metadata["company"] = companies[-1]

        # å°è¯•æå–è¡Œä¸š
        industry_pattern = r'(è®¡ç®—æœº|äº’è”ç½‘|åŒ»è¯|é‡‘è|æ¶ˆè´¹|æ±½è½¦|ç”µå­|é€šä¿¡|ä¼ åª’|åŒ–å·¥|å»ºç­‘|åŸææ–™)'
        industries = re.findall(industry_pattern, page.extract_text() or "")
        if industries:
            metadata["industry"] = industries[-1]

        return metadata

    def _chunk_text(
        self,
        text: str,
        file_metadata: Dict,
        page_metadata: Dict
    ) -> List[Dict]:
        """æ–‡æœ¬åˆ†å—"""
        if not text:
            return []

        chunks = []
        words = text.split()

        # è¯­ä¹‰åˆ†å—ï¼ˆæŒ‰å¥å­è¾¹ç•Œï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!?]', text)
        current_chunk = ""
        overlap_chunk = ""

        for sentence in sentences:
            # æ·»åŠ é‡å 
            if overlap_chunk:
                sentence = overlap_chunk + sentence
                overlap_chunk = ""

            # å°è¯•åˆå¹¶å¥å­ç›´åˆ°è¾¾åˆ° chunk_size
            while len(current_chunk) + len(sentence) < self.chunk_size:
                current_chunk += sentence + "ã€‚"
                sentence = ""

                # å¦‚æœå¥å­ä¸ºç©ºï¼Œè¯´æ˜å·²ç»è¾¾åˆ°å¤§å°
                if not sentence:
                    break

            # å¦‚æœå½“å‰å¥å­å¤ªé•¿ï¼Œå¼ºåˆ¶åˆ†å—
            if len(current_chunk) + len(sentence) >= self.chunk_size:
                # ä¿å­˜å½“å‰å—
                if len(current_chunk) >= self.chunk_size // 2:
                    chunks.append({
                        "text": current_chunk.strip(),
                        "metadata": {
                            **file_metadata,
                            **page_metadata,
                            "chunk_id": len(chunks) + 1
                        }
                    })

                # ä¿å­˜é‡å éƒ¨åˆ†
                overlap_chunk = current_chunk[-self.chunk_overlap:]

                # å¼€å§‹æ–°å—
                current_chunk = sentence
            else:
                current_chunk += sentence + "ã€‚"

        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append({
                "text": current_chunk.strip(),
                "metadata": {
                    **file_metadata,
                    **page_metadata,
                    "chunk_id": len(chunks) + 1
                }
            })

        return chunks

    def parse_directory(self, directory: Path, pattern: str = "*.pdf") -> List[Dict]:
        """æ‰¹é‡è§£æç›®å½•"""
        import glob
        import time

        all_chunks = []
        pdf_files = sorted(Path(directory).glob(pattern))

        print(f"ğŸ“‚ æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")

        for pdf_file in pdf_files:
            try:
                chunks = self.parse_file(pdf_file)
                all_chunks.extend(chunks)

                # æ§åˆ¶é€Ÿç‡
                time.sleep(0.1)  # é¿å…è§¦å‘ API é™æµ

            except Exception as e:
                print(f"âœ— è§£æå¤±è´¥: {pdf_file.name} - {e}")

        print(f"âœ“ å…¨éƒ¨å®Œæˆ: å…± {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        return all_chunks


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    parser = PDFParser(chunk_size=1000, chunk_overlap=50)
    chunks = parser.parse_directory(Path("./research_papers"))
    print(f"æ€»å—æ•°: {len(chunks)}")
```

---

### æ­¥éª¤ 2: æ–‡æœ¬æ¸…æ´—

```python
# text_cleaner.py
import re
import jieba
from typing import List, Dict

class TextCleaner:
    """æ–‡æœ¬æ¸…æ´—å™¨"""

    def __init__(self, stop_words_path: str = None):
        # åŠ è½½åœç”¨è¯
        self.stop_words = self._load_stop_words(stop_words_path)

    def clean(self, text: str) -> str:
        """æ¸…æ´—å•ä¸ªæ–‡æœ¬å—"""
        # 1. å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = self._remove_special_chars(text)

        # 2. å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        # 3. ä¸­æ–‡åˆ†è¯
        words = self._segment(text)

        # 4. å»é™¤åœç”¨è¯
        words = [w for w in words if w not in self.stop_words]

        # 5. åˆå¹¶å›æ–‡æœ¬
        return ' '.join(words)

    def clean_batch(self, chunks: List[Dict]) -> List[Dict]:
        """æ‰¹é‡æ¸…æ´—"""
        return [
            {
                "text": self.clean(chunk["text"]),
                "metadata": chunk["metadata"]
            }
            for chunk in chunks
        ]

    def _remove_special_chars(self, text: str) -> str:
        """å»é™¤ç‰¹æ®Šå­—ç¬¦"""
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸è§æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹]', ' ', text)
        return text

    def _segment(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        return jieba.lcut(text)

    def _load_stop_words(self, path: str) -> set:
        """åŠ è½½åœç”¨è¯è¡¨"""
        if path and Path(path).exists():
            with open(path, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f)
        return set()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    cleaner = TextCleaner()

    test_text = """
    ç”²è‚¡å¸‚åœºæ–¹é¢ï¼ŒåŒ»è¯è¡Œä¸šæ•´ä½“è¡¨ç°ç¨³å¥ã€‚è¡Œä¸šé¾™å¤´å…¬å¸å‡­å€Ÿå¼ºå¤§çš„ç ”å‘å®åŠ›ï¼Œ
    æŒç»­æ¨å‡ºåˆ›æ–°è¯ç‰©ã€‚åŒæ—¶ï¼Œéšç€äººå£è€é¾„åŒ–åŠ å‰§ï¼ŒåŒ»è¯è¡Œä¸šé•¿æœŸå‰æ™¯å‘å¥½ã€‚
    æŠ•èµ„è€…åº”å…³æ³¨å…·æœ‰æ ¸å¿ƒç«äº‰åŠ›çš„ç»†åˆ†é¢†åŸŸé¾™å¤´ã€‚
    """

    cleaned = cleaner.clean(test_text)
    print(f"æ¸…æ´—å‰: {test_text[:100]}...")
    print(f"æ¸…æ´—å: {cleaned[:100]}...")
```

---

### æ­¥éª¤ 3: å‘é‡åŒ–

```python
# vectorizer.py
import os
from typing import List, Dict
import asyncio
from openai import AsyncOpenAI

class Vectorizer:
    """å‘é‡åŒ–å™¨ - ä½¿ç”¨ GLM åµŒå…¥"""

    def __init__(
        self,
        api_key: str,
        model: str = "glm-4-flashx",
        batch_size: int = 50,
        base_url: str = "https://api.z.ai/api/coding/paas/v4"
    ):
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = model
        self.batch_size = batch_size

    async def embed(self, text: str) -> List[float]:
        """å•ä¸ªæ–‡æœ¬åµŒå…¥"""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âœ— åµŒå…¥å¤±è´¥: {text[:50]}... - {e}")
            return [0.0] * 1536  # é»˜è®¤é›¶å‘é‡

    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥"""
        results = []
        total = len(texts)

        print(f"â³ æ‰¹é‡åµŒå…¥ä¸­: {total} æ¡æ–‡æœ¬")

        for i in range(0, total, self.batch_size):
            batch = texts[i:i + self.batch_size]

            try:
                response = await self.client.embeddings.create(
                    model=self.model,
                    input=batch
                )
                results.extend([d.embedding for d in response.data])
                print(f"âœ“ è¿›åº¦: {i + len(batch)}/{total}")

            except Exception as e:
                print(f"âœ— æ‰¹é‡åµŒå…¥å¤±è´¥: {e}")
                # é€æ¡é‡è¯•
                for text in batch:
                    embedding = await self.embed(text)
                    results.append(embedding)

        print(f"âœ“ å®Œæˆ: å…± {len(results)} ä¸ªå‘é‡")
        return results

    def to_jsonl(self, vectors: List[List[float]], metadata: List[Dict]) -> str:
        """è½¬æ¢ä¸º JSONL æ ¼å¼"""
        lines = []
        for vec, meta in zip(vectors, metadata):
            lines.append({
                "vector": vec,
                "metadata": meta
            })
        return '\n'.join(json.dumps(line, ensure_ascii=False) for line in lines)


# ä½¿ç”¨ç¤ºä¾‹
import json
import asyncio

async def main():
    vectorizer = Vectorizer(
        api_key=os.getenv("ZAI_API_KEY"),
        model="glm-4-flashx"
    )

    test_texts = [
        "Aè‚¡åŒ»è¯è¡Œä¸šé¾™å¤´å…¬å¸æŒç»­æ¨å‡ºåˆ›æ–°è¯ç‰©",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”",
        "æ–°èƒ½æºæ±½è½¦äº§ä¸šé“¾æ­£åœ¨å¿«é€Ÿæˆé•¿"
    ]

    vectors = await vectorizer.embed_batch(test_texts)

    # ä¿å­˜ä¸º JSONL
    metadata = [{"id": i, "text": t[:50] + "..."} for i, t in enumerate(test_texts)]
    jsonl_data = vectorizer.to_jsonl(vectors, metadata)

    with open("vectors.jsonl", "w", encoding="utf-8") as f:
        f.write(jsonl_data)

    print(f"âœ“ ä¿å­˜åˆ° vectors.jsonl ({len(vectors)} æ¡)")


if __name__ == "__main__":
    asyncio.run(main())
```

---

### æ­¥éª¤ 4: ç´¢å¼•æ„å»º

```python
# index_builder.py
import chromadb
from chromadb.config import Settings
from typing import List, Dict
import json
from pathlib import Path

class IndexBuilder:
    """ç´¢å¼•æ„å»ºå™¨ - ä½¿ç”¨ ChromaDB"""

    def __init__(self, persist_dir: str = "./chroma_index"):
        self.persist_dir = Path(persist_dir)
        self.persist_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ– ChromaDB
        self.client = chromadb.PersistentClient(
            path=str(self.persist_dir),
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )

        # åˆ›å»ºé›†åˆ
        self.collection = self.client.get_or_create_collection(
            name="stock_research",
            metadata={"hnsw:space": "cosine"}
        )

    def add_documents(self, texts: List[str], metadata: List[Dict]):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        if not texts:
            print("âœ“ æ²¡æœ‰æ–‡æ¡£éœ€è¦æ·»åŠ ")
            return

        print(f"â³ æ·»åŠ  {len(texts)} ä¸ªæ–‡æ¡£åˆ°ç´¢å¼•...")

        # å‡†å¤‡æ•°æ®
        ids = [f"doc_{i}" for i in range(len(texts))]
        embeddings = self._generate_embeddings(texts)  # éœ€è¦å¤–éƒ¨æä¾›åµŒå…¥

        # æ·»åŠ åˆ° ChromaDB
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=texts,
            metadatas=metadata
        )

        print(f"âœ“ æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªæ–‡æ¡£")

    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """æœç´¢"""
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self._generate_embeddings([query])[0]

        # æœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        # æ ¼å¼åŒ–ç»“æœ
        return self._format_results(results)

    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”ŸæˆåµŒå…¥ï¼ˆéœ€è¦å¤–éƒ¨æä¾›åµŒå…¥ APIï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å‘é‡åŒ– API
        # ç¤ºä¾‹ï¼šè¿”å›é›¶å‘é‡
        return [[0.0] * 1536 for _ in texts]

    def _format_results(self, results: Dict) -> List[Dict]:
        """æ ¼å¼åŒ–ç»“æœ"""
        formatted = []
        for i, doc in enumerate(results["documents"][0]):
            formatted.append({
                "text": doc,
                "score": 1 - results["distances"][0][i],  # è·ç¦»è½¬ç›¸ä¼¼åº¦
                "metadata": results["metadatas"][0][i]
            })
        return formatted

    def stats(self):
        """è·å–ç´¢å¼•ç»Ÿè®¡"""
        count = self.collection.count()
        print(f"ç´¢å¼•ç»Ÿè®¡: {count} ä¸ªæ–‡æ¡£")
        return count


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    builder = IndexBuilder(persist_dir="./stock_research_chroma")

    # æ·»åŠ æ–‡æ¡£
    texts = [
        "Aè‚¡åŒ»è¯è¡Œä¸šé¾™å¤´å…¬å¸æŒç»­æ¨å‡ºåˆ›æ–°è¯ç‰©",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”",
        "æ–°èƒ½æºæ±½è½¦äº§ä¸šé“¾æ­£åœ¨å¿«é€Ÿæˆé•¿"
    ]

    metadata = [
        {"company": "æ’ç‘åŒ»è¯", "industry": "åŒ»è¯"},
        {"company": "å­—èŠ‚è·³åŠ¨", "industry": "äº’è”ç½‘"},
        {"company": "æ¯”äºšè¿ª", "industry": "æ±½è½¦"}
    ]

    builder.add_documents(texts, metadata)

    # æœç´¢
    results = builder.search("åˆ›æ–°è¯ç‰©ç ”å‘", top_k=2)
    for r in results:
        print(f"[{r['score']:.2f}] {r['text']}")
        print(f"  {r['metadata']}")
```

---

### æ­¥éª¤ 5: æ··åˆæ£€ç´¢

```python
# hybrid_search.py
import chromadb
from rank_bm25 import BM25Okapi
from typing import List, Dict
import jieba

class HybridSearcher:
    """æ··åˆæ£€ç´¢å™¨ - å‘é‡ + BM25"""

    def __init__(self, vector_collection, bm25_index):
        self.vector_collection = vector_collection
        self.bm25_index = bm25_index

    def search(
        self,
        query: str,
        top_k: int = 10,
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3
    ) -> List[Dict]:
        """æ··åˆæ£€ç´¢"""
        # 1. å‘é‡æœç´¢
        vector_results = self._vector_search(query, top_k=int(top_k * 1.5))

        # 2. BM25 æœç´¢
        bm25_results = self._bm25_search(query, top_k=int(top_k * 1.5))

        # 3. ç»“æœåˆå¹¶å’Œé‡æ’åº
        combined = self._merge_results(
            vector_results,
            bm25_results,
            top_k=top_k,
            vector_weight=vector_weight,
            bm25_weight=bm25_weight
        )

        return combined

    def _vector_search(self, query: str, top_k: int) -> List[Dict]:
        """å‘é‡æœç´¢"""
        query_embedding = self._get_embedding([query])[0]

        results = self.vector_collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

        return self._format_results(results, method="vector")

    def _bm25_search(self, query: str, top_k: int) -> List[Dict]:
        """BM25 æœç´¢"""
        # ä¸­æ–‡åˆ†è¯
        tokens = jieba.lcut(query)

        # æœç´¢
        scores = self.bm25_index.get_scores(tokens)
        top_indices = scores.argsort()[::-1][:top_k]

        return self._format_bm25_results(top_indices, scores, top_k)

    def _merge_results(
        self,
        vector_results: List[Dict],
        bm25_results: List[Dict],
        top_k: int,
        vector_weight: float,
        bm25_weight: float
    ) -> List[Dict]:
        """åˆå¹¶ç»“æœ"""
        combined = {}

        # åˆå¹¶ç»“æœå¹¶è®¡ç®—åŠ æƒåˆ†æ•°
        for r in vector_results:
            combined[r["id"]] = {
                "text": r["text"],
                "score": r["score"] * vector_weight,
                "metadata": r["metadata"],
                "methods": {"vector": r["score"]}
            }

        for r in bm25_results:
            if r["id"] in combined:
                combined[r["id"]]["score"] += r["score"] * bm25_weight
                combined[r["id"]]["methods"]["bm25"] = r["score"]
            else:
                combined[r["id"]] = {
                    "text": r["text"],
                    "score": r["score"] * bm25_weight,
                    "metadata": r["metadata"],
                    "methods": {"bm25": r["score"]}
                }

        # æ’åºå¹¶è¿”å› top_k
        sorted_results = sorted(
            combined.values(),
            key=lambda x: x["score"],
            reverse=True
        )

        return sorted_results[:top_k]

    def _format_results(self, results: Dict, method: str) -> List[Dict]:
        """æ ¼å¼åŒ–å‘é‡ç»“æœ"""
        formatted = []
        for i, doc in enumerate(results["documents"][0]):
            formatted.append({
                "id": results["ids"][0][i],
                "text": doc,
                "score": 1 - results["distances"][0][i],  # ä½™å¼¦è·ç¦»è½¬ç›¸ä¼¼åº¦
                "metadata": results["metadatas"][0][i],
                "method": method
            })
        return formatted

    def _format_bm25_results(self, indices: List[int], scores: List[float], top_k: int) -> List[Dict]:
        """æ ¼å¼åŒ– BM25 ç»“æœ"""
        formatted = []
        for i in indices[:top_k]:
            formatted.append({
                "id": i,
                "text": self.bm25_index.get_doc(i),
                "score": float(scores[i]),
                "metadata": {},
                "method": "bm25"
            })
        return formatted

    def _get_embedding(self, texts: List[str]) -> List[List[float]]:
        """è·å–åµŒå…¥ï¼ˆéœ€è¦å¤–éƒ¨æä¾›åµŒå…¥ APIï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨åµŒå…¥ API
        return [[0.0] * 1536 for _ in texts]


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–
    vector_client = chromadb.PersistentClient(path="./chroma_index")
    vector_collection = vector_client.get_collection("stock_research")

    # åˆ›å»º BM25 ç´¢å¼•
    texts = [doc for doc in vector_collection.get()["documents"]]
    tokenized_corpus = [list(jieba.lcut(doc)) for doc in texts]
    bm25 = BM25Okapi(tokenized_corpus)

    searcher = HybridSearcher(vector_collection, bm25)

    # æœç´¢
    results = searcher.search("åŒ»è¯åˆ›æ–°è¯ç‰©ç ”å‘", top_k=5)
    for r in results:
        print(f"[{r['score']:.3f}] ({r['method']}) {r['text']}")
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

### 1. æ‰¹å¤„ç†ä¸å¹¶è¡Œ

```python
# parallel_processor.py
from concurrent.futures import ThreadPoolExecutor
import asyncio

def parallel_process(data: List, func, workers: int = 4):
    """å¹¶è¡Œå¤„ç†"""
    with ThreadPoolExecutor(max_workers=workers) as executor:
        results = list(executor.map(func, data))
    return results

def async_parallel_process(data: List, func, max_concurrent: int = 10):
    """å¼‚æ­¥å¹¶è¡Œå¤„ç†"""
    semaphore = asyncio.Semaphore(max_concurrent)

    async def limited_func(item):
        async with semaphore:
            return await func(item)

    return asyncio.run(asyncio.gather(*[limited_func(item) for item in data]))
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# cache_manager.py
from functools import lru_cache
import hashlib
import json

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_file: str = "query_cache.json"):
        self.cache_file = cache_file
        self.cache = self._load_cache()

    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜"""
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}

    def _get_key(self, query: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(query.encode()).hexdigest()

    def get(self, query: str) -> List[Dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        key = self._get_key(query)
        return self.cache.get(key, [])

    def set(self, query: str, results: List[Dict]):
        """è®¾ç½®ç¼“å­˜"""
        key = self._get_key(query)
        self.cache[key] = results
        self._save_cache()

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False)

    @lru_cache(maxsize=10000)
    def cached_search(self, query: str, *args, **kwargs) -> List[Dict]:
        """å¸¦ç¼“å­˜çš„æœç´¢"""
        results = self.search(query, *args, **kwargs)
        self.set(query, results)
        return results
```

### 3. æ•°æ®é¢„å¤„ç†æµæ°´çº¿

```python
# pipeline.py
from typing import Callable
import time

class ProcessingPipeline:
    """å¤„ç†æµæ°´çº¿"""

    def __init__(self):
        self.steps = []

    def add_step(self, name: str, func: Callable, desc: str = ""):
        """æ·»åŠ æ­¥éª¤"""
        self.steps.append({
            "name": name,
            "func": func,
            "desc": desc or name
        })

    def run(self, input_data):
        """æ‰§è¡Œæµæ°´çº¿"""
        current = input_data

        for step in self.steps:
            start = time.time()
            print(f"â³ æ‰§è¡Œæ­¥éª¤: {step['desc']}")

            try:
                current = step['func'](current)
                elapsed = time.time() - start
                print(f"âœ“ å®Œæˆ: {elapsed:.2f}s")

            except Exception as e:
                print(f"âœ— å¤±è´¥: {e}")
                raise

        return current

    def get_summary(self) -> Dict:
        """è·å–æ­¥éª¤æ‘˜è¦"""
        return {
            "steps": [
                {
                    "name": s["name"],
                    "desc": s["desc"]
                }
                for s in self.steps
            ]
        }
```

---

## ğŸš€ å®Œæ•´ç®¡é“å®ç°

```python
# complete_pipeline.py
from pathlib import Path
from pdf_parser import PDFParser
from text_cleaner import TextCleaner
from vectorizer import Vectorizer
from index_builder import IndexBuilder
from cache_manager import CacheManager

class StockResearchRAGPipeline:
    """è‚¡ç¥¨ç ”æŠ¥ RAG å®Œæ•´ç®¡é“"""

    def __init__(
        self,
        data_dir: str,
        output_dir: str = "./output",
        vectorizer: Vectorizer = None
    ):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–å„ç»„ä»¶
        self.parser = PDFParser(chunk_size=1000, chunk_overlap=50)
        self.cleaner = TextCleaner()
        self.index_builder = IndexBuilder(persist_dir=str(output_dir / "chroma_index"))
        self.cache = CacheManager(cache_file=str(output_dir / "query_cache.json"))
        self.vectorizer = vectorizer

    def build_index(self, rebuild: bool = False):
        """æ„å»ºç´¢å¼•"""
        if rebuild:
            print("ğŸ—‘ï¸  æ¸…ç©ºç°æœ‰ç´¢å¼•...")
            self.index_builder.collection.delete()

        print(f"ğŸ“‚ è¯»å–æ•°æ®ç›®å½•: {self.data_dir}")

        # 1. è§£æ PDF
        raw_chunks = self.parser.parse_directory(self.data_dir)
        print(f"âœ“ è§£æå®Œæˆ: {len(raw_chunks)} ä¸ªæ–‡æœ¬å—")

        # 2. æ¸…æ´—æ–‡æœ¬
        cleaned_chunks = self.cleaner.clean_batch(raw_chunks)
        print(f"âœ“ æ¸…æ´—å®Œæˆ")

        # 3. è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_chunks = [
            c for c in cleaned_chunks
            if len(c["text"]) > 50  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
        ]
        print(f"âœ“ è¿‡æ»¤å®Œæˆ: {len(valid_chunks)} ä¸ªæœ‰æ•ˆå—")

        # 4. å‘é‡åŒ–
        texts = [c["text"] for c in valid_chunks]
        metadatas = [c["metadata"] for c in valid_chunks]

        if self.vectorizer:
            embeddings = asyncio.run(self.vectorizer.embed_batch(texts))
        else:
            embeddings = [[0.0] * 1536 for _ in texts]

        # 5. æ„å»ºç´¢å¼•
        self.index_builder.add_documents(texts, metadatas)

        print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆ!")
        print(f"ğŸ“Š æ–‡æ¡£æ•°: {len(texts)}")

    def query(
        self,
        question: str,
        top_k: int = 10,
        use_cache: bool = True
    ) -> List[Dict]:
        """æŸ¥è¯¢"""
        if use_cache:
            cached_results = self.cache.get(question)
            if cached_results:
                print(f"ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ")
                return cached_results

        results = self.index_builder.search(question, top_k=top_k)

        if use_cache:
            self.cache.set(question, results)

        return results


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    pipeline = StockResearchRAGPipeline(
        data_dir="./research_papers"
    )

    # æ„å»ºç´¢å¼•
    pipeline.build_index(rebuild=True)

    # æŸ¥è¯¢
    results = pipeline.query(
        "Aè‚¡åŒ»è¯è¡Œä¸šé¾™å¤´å…¬å¸æœ‰å“ªäº›?",
        top_k=5
    )

    for r in results:
        print(f"\n[{r['score']:.3f}]")
        print(f"æ–‡æœ¬: {r['text'][:200]}...")
        print(f"å…ƒæ•°æ®: {r['metadata']}")


if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ’° æˆæœ¬ä¼˜åŒ–

### æŒ‰éœ€åŠ è½½ + åˆ†æ‰¹å¤„ç†

```python
# incremental_loading.py
class IncrementalIndexBuilder:
    """å¢é‡ç´¢å¼•æ„å»ºå™¨"""

    def __init__(self, total_files: int):
        self.total_files = total_files
        self.processed_files = 0

    def build_incremental(
        self,
        files: List[Path],
        batch_size: int = 1000,
        output_dir: str = "./incremental_index"
    ):
        """åˆ†æ‰¹æ„å»ºç´¢å¼•"""
        for i in range(0, len(files), batch_size):
            batch = files[i:i + batch_size]
            self._process_batch(batch)
            self.processed_files += len(batch)

            progress = (self.processed_files / self.total_files) * 100
            print(f"\nğŸ“Š è¿›åº¦: {progress:.1f}% ({self.processed_files}/{self.total_files})")

    def _process_batch(self, batch: List[Path]):
        """å¤„ç†ä¸€æ‰¹æ–‡ä»¶"""
        # è§£æ + æ¸…æ´— + å‘é‡åŒ–
        chunks = self.parser.parse_directory(Path(batch[0]).parent)
        cleaned = self.cleaner.clean_batch(chunks)
        texts = [c["text"] for c in cleaned]
        metadatas = [c["metadata"] for c in cleaned]
        embeddings = self.vectorizer.embed_batch(texts)

        # æ·»åŠ åˆ°ç´¢å¼•
        self.index_builder.add_documents(texts, metadatas)
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—

```python
# monitoring.py
import logging
from datetime import datetime

class RAGMonitor:
    """RAG ç›‘æ§"""

    def __init__(self, log_file: str = "rag_monitor.log"):
        self.logger = self._setup_logger(log_file)

    def _setup_logger(self, log_file: str):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger("RAG")
        logger.setLevel(logging.INFO)

        handler = logging.FileHandler(log_file, encoding="utf-8")
        formatter = logging.Formatter(
            '%(asctime)s | %(levelname)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        handler.setFormatter(formatter)

        logger.addHandler(handler)
        return logger

    def log_step(self, step_name: str, status: str, details: dict = None):
        """è®°å½•æ­¥éª¤"""
        message = f"[{step_name}] {status}"
        if details:
            message += f" - {details}"

        if status == "âœ“":
            self.logger.info(message)
        else:
            self.logger.error(message)

    def log_query(self, query: str, result_count: int, time_taken: float):
        """è®°å½•æŸ¥è¯¢"""
        self.logger.info(
            f"æŸ¥è¯¢: {query[:50]}... | ç»“æœ: {result_count} | è€—æ—¶: {time_taken:.2f}s"
        )

    def log_cost(self, model: str, input_tokens: int, output_tokens: int, cost: float):
        """è®°å½•æˆæœ¬"""
        self.logger.info(
            f"æ¨¡å‹: {model} | è¾“å…¥: {input_tokens} | è¾“å‡º: {output_tokens} | è´¹ç”¨: ${cost:.4f}"
        )

    def print_stats(self, stats: dict):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("\n" + "=" * 50)
        self.logger.info("ç»Ÿè®¡ä¿¡æ¯:")
        for k, v in stats.items():
            self.logger.info(f"  {k}: {v}")
        self.logger.info("=" * 50 + "\n")
```

---

## ğŸ¯ éƒ¨ç½²å»ºè®®

### 1. æ•°æ®é¢„å¤„ç† (ä¸€æ¬¡æ€§)

```bash
# åœ¨ä½å³°æœŸæ‰§è¡Œ
python pdf_parser.py --input ./research_papers --output ./processed
```

### 2. ç´¢å¼•æ„å»º (æ‰¹é‡)

```bash
# åˆ†æ‰¹æ„å»ºï¼Œé¿å…å†…å­˜æº¢å‡º
python index_builder.py --batch-size 10000
```

### 3. æŸ¥è¯¢æœåŠ¡ (é•¿æœŸè¿è¡Œ)

```bash
# å¯åŠ¨ FastAPI æœåŠ¡
uvicorn rag_server:app --host 0.0.0.0 --port 8000
```

---

## ğŸ“ æ€»ç»“

| ç»„ä»¶ | æ¨èæ–¹æ¡ˆ | ä¼°ç®—æˆæœ¬ |
|---|---|---|
| PDF è§£æ | pdfplumber | å…è´¹ |
| æ–‡æœ¬æ¸…æ´— | jieba + æ­£åˆ™ | å…è´¹ |
| å‘é‡åµŒå…¥ | GLM-4-flashx | ~Â¥0.001/1000å­— |
| ç´¢å¼•å­˜å‚¨ | ChromaDB | æœ¬åœ°å…è´¹ |
| æŸ¥è¯¢æœåŠ¡ | FastAPI + AsyncIO | ä½æˆæœ¬ |

**æ€»æˆæœ¬ä¼°ç®—**ï¼š
- 100ä¸‡ä»½æŠ¥å‘Š â†’ ~800-1000ä¸‡ä¸­æ–‡å­—
- åµŒå…¥æˆæœ¬ â†’ ~Â¥800-1000ï¼ˆä½¿ç”¨ flashx æ¨¡å‹ï¼‰
- ç¡¬ä»¶è¦æ±‚ â†’ 16GB RAM, SSD

éœ€è¦æˆ‘è¯¦ç»†å±•å¼€æŸä¸ªå…·ä½“æ¨¡å—çš„å®ç°å—ï¼Ÿ
