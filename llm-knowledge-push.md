# 🤖 LLM 实用知识推送（架构师版）

为您精选 10 条 LLM/RAG 落地实践知识：

---

## 【一、RAG 架构与实践】

### 1️⃣ RAG 检索策略：混合检索的优势

**核心概念**：结合关键词 (BM25)+ 向量检索，加权融合提升召回质量。关键词擅长精确匹配，向量擅长语义匹配。

**应用场景**：企业知识库、含专业术语/代码的检索场景。

**架构师建议**：
- 默认 0.7 向量权重 + 0.3 关键词权重
- 代码检索提高关键词至 0.5
- 使用 RRF (Reciprocal Rank Fusion) 算法融合更稳定

---

### 2️⃣ 向量化选型：Embedding 模型选择指南

**核心概念**：Embedding 将文本转为向量，直接影响检索效果。主流选择包括 bge-large-zh（中文优化）、m3e-base（通用）、text-embedding-3-large（OpenAI）。

**应用场景**：中文文档检索、多语言知识库、代码向量化。

**架构师建议**：
- 中文场景优先 bge-large-zh-v1.5（MTEB 中文榜单前列）
- 1024 维度性价比最佳
- 批量向量化控制并发，避免 API 限流
- 本地部署选 bge-m3 节省成本

---

### 3️⃣ 重排序 (Rerank)：提升检索精度的关键

**核心概念**：Cross-Encoder 对召回 Top-K 精细排序，计算 query-doc 相关性分数。

**应用场景**：高精度问答、法律/医疗垂直领域检索。

**架构师建议**：
- 先召回 Top-50，再 Rerank 取 Top-5 送入 LLM
- 中文推荐 bge-reranker-large
- 缓存热门 query 的 rerank 结果
- 延迟敏感场景可跳过此步骤

---

## 【二、性能优化】

### 4️⃣ 语义缓存：减少 LLM 重复调用

**核心概念**：Query 向量化匹配缓存池，相似度>0.95 直接返回缓存答案，避免重复调用 LLM。

**应用场景**：高并发问答、FAQ、重复问题多的客服系统。

**架构师建议**：
- 使用 Redis + RedisVL 实现
- TTL 设 7-30 天
- 低置信度答案不缓存
- 定期清理低命中率缓存项

---

### 5️⃣ 流式输出 (Streaming)：优化用户体验

**核心概念**：逐 token 返回响应，前端实时渲染，用户无需等待完整生成即可看到内容。

**应用场景**：长文本生成、对话机器人、快速响应交互场景。

**架构师建议**：
- 后端使用 SSE 或 WebSocket 传输
- 前端实现打字机效果
- chunk 大小 20-50 tokens
- 关注 TTFT 指标，目标<500ms

---

### 6️⃣ 批处理 (Batching)：提升吞吐量

**核心概念**：多请求合并为一次 API 调用，摊薄网络延迟和模型推理开销，显著提升吞吐量。

**应用场景**：离线向量化、批量生成、日志分析等非实时场景。

**架构师建议**：
- OpenAI Batch API 成本降低 50%
- 批处理窗口 100ms 或累积 10 条请求
- 不适合实时交互场景
- 监控队列长度，避免请求堆积

---

## 【三、成本优化】

### 7️⃣ 模型路由：按任务复杂度分配模型

**核心概念**：轻量模型处理简单任务（分类、提取），复杂任务（推理、创作）路由到大模型，实现成本与效果平衡。

**应用场景**：多任务 AI 平台、企业 AI 中台、成本敏感的生产环境。

**架构师建议**：
- 建立任务分类器（可用小模型实现）
- 简单任务用 Qwen-Turbo/DeepSeek-V2，复杂任务用 Qwen-Max/GPT-4
- 设置降级策略，大模型超时时自动切换
- 监控各模型调用占比，持续优化路由规则

---

## 【四、评估与监控】

### 8️⃣ RAG 评估指标体系

**核心概念**：评估检索质量 (Recall@K、MRR) 和生成质量 (忠实度、答案相关性)。使用 Ragas、TruLens 等框架自动化评估。

**应用场景**：RAG 系统迭代优化、A/B 测试、上线前验收。

**架构师建议**：
- 构建 100-500 条黄金测试集（query+ 标准答案 + 相关文档）
- 自动化评估集成到 CI/CD
- 关注 Context Recall（检索是否召回关键信息）和 Answer Faithfulness（答案是否忠实于上下文）
- 定期人工抽检，避免指标欺骗

---

## 【五、安全与隐私】

### 9️⃣ Prompt 注入防护

**核心概念**：攻击者通过精心构造的输入诱导 LLM 越狱、泄露系统指令或执行未授权操作。防护需多层防御。

**应用场景**：对外开放的 AI 应用、处理用户生成内容的系统。

**架构师建议**：
- 输入层：过滤特殊字符、检测注入模式
- 系统层：使用<system>隔离指令，禁止用户输入修改系统提示
- 输出层：检测敏感信息泄露
- 使用 LLM Guard 等工具做实时检测
- 定期红队测试

---

## 【六、最新技术动态】

### 🔟 Agent 框架选型：LangChain vs LlamaIndex

**核心概念**：LangChain 侧重通用 Agent 编排，组件丰富；LlamaIndex 专注 RAG 场景，检索优化更深入。2025 年趋势是轻量化和专业化。

**应用场景**：企业 AI 应用开发、RAG 系统搭建、Agent 工作流设计。

**架构师建议**：
- 纯 RAG 场景优先 LlamaIndex（检索优化更成熟）
- 复杂 Agent 工作流选 LangChain
- 生产环境建议抽象接口层，便于切换框架
- 关注新兴框架如 Haystack 2.0、Semantic Kernel
- 避免过度依赖框架，核心逻辑保持可移植性

---

> 💡 以上知识基于生产实践总结，适合 RAG 系统搭建参考。如需深入某个主题，欢迎随时交流！
