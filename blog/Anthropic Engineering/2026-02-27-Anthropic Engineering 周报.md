---
tags:
  - Anthropic
  - AI 工程
  - 周报
  - LLM
  - Agent
  - RAG
  - 评估体系
  - 上下文工程
created: 2026-02-27T22:30:00+08:00
source: https://www.anthropic.com/engineering
author: 贾维斯
---

# Anthropic Engineering 周报 | 2026-02-27

## 📋 本期概览

本期精选 3 篇 Anthropic Engineering 博客文章，聚焦：
- **并行 AI 代理架构**：16 个 Claude 协作构建 C 编译器
- **评估体系建设**：AI 代理评估的完整方法论
- **上下文工程**：从提示词工程到上下文工程的范式转变

---

## 🔧 文章一：用并行 Claude 团队构建 C 编译器

**链接**：https://www.anthropic.com/engineering/building-c-compiler  
**发布日期**：2026-02-05  
**作者**：Nicholas Carlini（Safeguards 团队研究员）

### 一句话总结

16 个 Claude 实例并行协作，耗时 2 周、花费$20K，从零编写了 10 万行 Rust 代码的 C 编译器，可编译 Linux 6.9 内核（x86/ARM/RISC-V）。

### 核心内容

#### 1. 多代理并行架构
- **独立容器**：每个代理在独立 Docker 容器中运行，挂载 git 仓库
- **任务锁机制**：通过 `current_tasks/` 目录下的锁文件协调任务分配
- **自主决策**：无编排代理，每个 Claude 自主决定"下一个最明显的任务"
- **冲突处理**：git 合并冲突由 Claude 自主解决

#### 2. 长周期运行 Harness
```bash
while true; do
  COMMIT=$(git rev-parse --short=6 HEAD)
  claude --dangerously-skip-permissions \
    -p "$(cat AGENT_PROMPT.md)" \
    --model claude-opus-X-Y &> "$LOGFILE"
done
```
- 无限循环 + 任务队列模式
- 代理提示词要求：拆解任务、追踪进度、持续迭代

#### 3. 自动化测试驱动
- **高质量测试套件**：GCC torture test suite、开源项目编译测试
- **CI 流水线**：防止新功能破坏现有功能
- **随机采样测试**：1%/10% 样本快速检测回归（确定性 per-agent，随机 cross-VM）
- **测试输出优化**：简洁、结构化、易 grep（ERROR 标记 + 原因同行）

#### 4. 专业化角色分工
- 代码去重代理：发现并合并重复实现
- 性能优化代理：优化编译器本身性能
- 代码质量代理：从 Rust 开发者视角审查设计
- 文档代理：维护 README 和进度文件

#### 5. 创新性并行策略
**问题**：编译 Linux 内核是单一任务，16 个代理会重复解决相同 bug  
**解决方案**：用 GCC 作为"在线已知正确答案"
- 随机用 GCC 编译大部分内核文件
- 仅剩余文件用 Claude 编译器
- 通过二分法定位问题文件
- 实现真正的并行调试

### 项目成果

| 指标 | 数值 |
|------|------|
| Claude Code 会话 | ~2,000 次 |
| 输入 token | 20 亿 |
| 输出 token | 1.4 亿 |
| API 成本 | ~$20,000 |
| 代码行数 | 100,000 行 Rust |
| 测试通过率 | 99%（GCC torture suite） |
| 可编译项目 | Linux 6.9, QEMU, FFmpeg, SQLite, Postgres, Redis, Doom |

### 局限性

- 缺少 16 位 x86 代码生成器（引导 Linux 实模式需要），此处调用 GCC
- 没有自己的汇编器和链接器（仍有点 bug）
- 生成代码效率不高（优化后仍不如 GCC 无优化）
- Rust 代码质量合理但不及专家水平

### 架构师视角洞察

#### 1. 测试即规范
> "Claude 会自主解决我给的任何问题。所以测试验证器必须几乎完美，否则 Claude 会解决错误的问题。"

- 测试质量决定自主代理的工作质量
- 测试输出需针对 LLM 优化（非人类）
- 持续观察失败模式，迭代设计新测试

#### 2. 并行度瓶颈与创新
- 当任务不可拆分时，需要创新性设计（GCC 比对策略）
- 并行不是万能药，需要任务可分解性

#### 3. 成本效益分析
- $20K 完成需要整个团队数月的工作
- 但接近 Opus 4.6 能力上限，说明当前模型仍有边界
- 新功能和 bug 修复频繁破坏现有功能，维护成本随复杂度增长

#### 4. 自主开发的风险
> "当人类在开发过程中与 Claude 坐在一起时，他们可以确保一致的质量并实时捕获错误。对于自主系统，很容易看到测试通过就认为工作完成了，但很少是这样。"

- 自主系统需要更严格的验证
- 测试通过≠真正完成

### 可落地实践建议

#### 公司场景

**RAG 系统优化**
- 引入多代理并行处理：
  - 检索代理：负责向量搜索和关键词匹配
  - 重排序代理：根据相关性重新排序
  - 答案生成代理：综合信息生成回答
  - 质量验证代理：检查事实准确性和引用完整性
- 建立自动化评估流水线：
  - 每次 RAG 优化后自动运行测试集
  - 用 LLM 判断回答质量变化
  - 设置回归测试防止倒退

**LLM 应用架构**
- 为复杂任务设计"任务锁"机制：
  - 避免多个工作流处理相同问题
  - 用共享存储（数据库/文件系统）实现协调
- 设计针对 LLM 优化的测试输出：
  - 简洁（避免上下文污染）
  - 结构化（易解析）
  - 错误标记清晰（易 grep）

#### 个人场景

**Claude Code 使用策略**
- 采用"测试先行"策略：
  - 先编写测试用例
  - 让 AI 自主迭代直到通过
- 对长期任务使用笔记持久化：
  - 维护 PROJECT.md 记录架构决策
  - 维护 TODO.md 追踪进度
  - 维护 NOTES.md 记录中间发现

**项目组织**
- 为 AI 生成代码设置专门目录
- 使用清晰的命名约定（AI 能理解）
- 定期审查 AI 生成代码的质量

---

## 📊 文章二：AI 代理评估体系解密

**链接**：https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents  
**发布日期**：2026-01-09  
**作者**：Mikaela Grace, Jeremy Hadfield, Rodrigo Olivares, Jiri De Jonghe

### 一句话总结

系统化讲解如何为 AI 代理设计评估体系，涵盖评估类型、评分器设计、非确定性处理及长期维护策略。

### 核心内容

#### 1. 评估组件定义

| 术语 | 定义 |
|------|------|
| 任务（Task） | 单个测试，有定义的输入和成功标准 |
| 试验（Trial） | 任务的一次尝试（因模型输出变化，需多次试验） |
| 评分器（Grader） | 评分逻辑，一个任务可有多个评分器 |
| 转录记录（Transcript） | 试验的完整记录（输出、工具调用、推理、中间结果） |
| 结果（Outcome） | 试验结束时环境中的最终状态 |
| 评估 Harness | 端到端运行评估的基础设施 |
| 代理 Harness | 使模型能够作为代理行动的系统 |
| 评估套件（Suite） | 设计用于衡量特定能力或行为的任务集合 |

#### 2. 三类评分器

**代码评分器（Code-based Graders）**
- 方法：字符串匹配、二进制测试、静态分析、结果验证、工具调用验证
- 优点：快速、便宜、客观、可重现、易调试
- 缺点：对有效变体死板、缺乏细微差别

**模型评分器（Model-based Graders）**
- 方法：基于 rubric 评分、自然语言断言、成对比较、参考评估、多法官共识
- 优点：灵活、可扩展、捕捉细微差别、处理开放式任务
- 缺点：非确定性、比代码贵、需要与人工校准

**人工评分器（Human Graders）**
- 方法：SME 审查、众包判断、抽样检查、A/B 测试、标注者间一致性
- 优点：黄金标准质量、匹配专家判断、用于校准模型评分器
- 缺点：昂贵、慢、需要大规模专家访问

#### 3. 两类评估集

**能力评估（Capability Evals）**
- 问题："这个代理能做好什么？"
- 目标：从低通过率开始，给团队爬升空间
- 用途：衡量改进空间

**回归评估（Regression Evals）**
- 问题："代理仍能处理所有以前的任务吗？"
- 目标：保持近 100% 通过率
- 用途：防止倒退

> 能力评估通过率高后可"毕业"成为回归套件

#### 4. 非确定性处理

**pass@k**：k 次尝试中至少一次成功的概率
- k 增加→分数上升
- 适用于：只要有一次成功就足够的场景（如代码生成）

**pass^k**：k 次尝试全部成功的概率
- k 增加→分数下降
- 适用于：需要一致可靠性的场景（如面向客户的代理）

示例：75% 单次成功率，3 次试验
- pass@3 ≈ 98%（至少一次成功）
- pass^3 ≈ 42%（全部成功）

#### 5. 评估驱动开发路线图

**Step 0：尽早开始**
- 20-50 个简单任务即可开始
- 早期开发中，每次变更影响明显，小样本足够

**Step 1：从手动测试开始**
- 转换发布前手动检查为测试用例
- 从 bug 追踪器和支持队列提取真实失败案例

**Step 2：编写无歧义任务**
- 两个领域专家应独立得出相同通过/失败判断
- 为每个任务创建参考解决方案（证明任务可解）

**Step 3：构建平衡问题集**
- 测试行为应发生和不应发生的情况
- 避免类别不平衡评估

**Step 4：构建稳健的评估 Harness**
- 每次试验从干净环境开始
- 避免试验间共享状态导致的关联失败

**Step 5：精心设计评分器**
- 优先使用确定性评分器
- 必要时使用 LLM 评分器
- 谨慎使用人工评分器进行验证
- 避免过于刚性的步骤检查（奖励创造力）
- 为多组件任务设计部分得分

**Step 6：检查转录记录**
- 阅读失败案例判断是代理问题还是评估设计问题
- 验证评估测量的是真正重要的东西

**Step 7：监控评估饱和**
- 100% 通过率的评估只追踪回归，不提供改进信号
- 饱和时进步会变慢（只有最难任务剩余）

**Step 8：长期维护**
- 建立专门评估团队拥有核心基础设施
- 领域专家和产品团队贡献评估任务
- 实践评估驱动开发：在功能实现前先定义评估

### 架构师视角洞察

#### 1. 评估是团队沟通工具
> "评估成为产品团队和研究团队之间最高带宽的沟通渠道，定义研究人员可以优化的指标。"

- 评估集消除需求歧义
- 两个工程师读同一规范可能有不同理解，评估解决歧义

#### 2. 避免"评估饱和"
- SWE-Bench Verified 从 30% 到>80%，前沿模型接近饱和
- 饱和时大能力改进表现为小分数增长（具有欺骗性）
- 需要持续引入更难任务

#### 3. 读取转录记录是核心技能
> "在 Anthropic，我们投资于查看评估转录记录的工具，并定期花时间阅读它们。"

- 通过阅读失败案例判断问题根源
- 验证评估公平性

#### 4. 瑞士奶酪模型
> "像安全工程中的瑞士奶酪模型，单一评估层无法捕获所有问题。多层组合时，穿过一层的失败会被另一层捕获。"

- 自动化评估 + 生产监控 + A/B 测试 + 人工审查 + 用户反馈
- 各方法对应不同开发阶段

#### 5. 评估的复合价值
- 成本 upfront 可见，收益后期累积
- 新模型发布时，有评估的团队可在几天内确定优势、调整 prompt、升级
- 无评估团队需要数周测试

### 可落地实践建议

#### 公司场景

**RAG 评估体系建设**

代码评分器：
- 答案包含关键事实（字符串/正则匹配）
- 引用来源正确（URL/文档 ID 验证）
- 响应时间<阈值
- Token 消耗<阈值

模型评分器：
- 用 LLM 判断回答相关性（rubric 评分）
- 完整性（是否覆盖问题所有方面）
- 可读性（结构清晰度）
-  grounding（声称是否有检索来源支持）

回归评估集：
- 将历史问题加入测试集
- 确保优化不倒退
- 每周运行一次

**LLM 应用评估指标**
- 任务完成率（pass@1, pass@3）
- 对话轮次（越少越好）
- Token 消耗（成本指标）
- 用户满意度（ thumbs up/down）
- 工具调用准确性（是否调用正确工具、参数正确）

#### 个人场景

**Prompt 评估**
- 为常用 Prompt 建立小型评估集（10-20 个典型问题）
- 每次修改前跑一遍
- 记录通过率变化

**重要任务测试**
- 用 pass@3 指标：连续跑 3 次，看成功率而非单次结果
- 对于关键任务，考虑 pass@5 或 pass@10

**评估记录**
- 维护评估结果日志
- 记录每次改进的通过率变化
- 识别饱和点并更新评估集

---

## 🧠 文章三：AI 代理上下文工程实践

**链接**：https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents  
**发布日期**：2025-09-29  
**作者**：Prithvi Rajasekaran, Ethan Dixon, Carly Ryan, Jeremy Hadfield

### 一句话总结

从"提示词工程"进化到"上下文工程"，核心是优化有限注意力预算内的高信号 token 配置。

### 核心内容

#### 1. 上下文工程定义

> "上下文工程是指策划和维护 LLM 推理期间最优 token 集（信息）的策略集合，包括所有可能进入那里的其他信息（系统指令、工具、MCP、外部数据、消息历史等）。"

**与提示词工程的区别**：
- 提示词工程：离散任务，写一次
- 上下文工程：迭代过程，每次推理前策划

#### 2. 上下文衰减（Context Rot）

> "随着上下文窗口中 token 数量增加，模型从该上下文准确回忆信息的能力下降。"

**原因**：
- Transformer 架构：n² 对关系，上下文增长导致注意力分散
- 训练数据分布：短序列更常见，模型缺乏长上下文依赖经验
- 位置编码插值：允许处理更长序列但位置理解降级

**启示**：上下文必须被视为有限资源，边际收益递减

#### 3. 有效上下文的解剖

**系统提示词设计**
- 避免两个极端：
  - 过度具体：硬编码复杂逻辑→脆弱、维护复杂
  - 过度模糊：假设共享上下文→缺乏具体信号
- 最佳实践：
  - 具体到有效指导行为
  - 灵活到提供强启发式
  - 用 XML 标签或 Markdown 头组织章节
  - 最小信息集完全概述预期行为

**工具设计**
- 常见失败模式：臃肿工具集、功能重叠、决策点模糊
- 原则：
  - 自包含、健壮、清晰
  - 输入参数描述性、无歧义
  - 如果人类工程师无法确定使用哪个工具，AI 也不能
  - 返回结果 token 高效

**示例（Few-shot Prompting）**
- 不推荐：堆砌所有边缘情况
- 推荐：策划多样化、规范示例集
- 对 LLM，示例是"值千言的图片"

#### 4. 上下文检索与代理搜索

**预检索 vs 即时检索**

| 策略 | 描述 | 优点 | 缺点 |
|------|------|------|------|
| 预检索 | 推理前基于嵌入检索相关数据 | 快速 | 可能检索无关数据、索引过时 |
| 即时检索 | 维护轻量引用，运行时动态加载 | token 高效、灵活 | 较慢、需要良好工具设计 |

**混合策略（Claude Code 采用）**
- CLAUDE.md 文件预先放入上下文
- glob/grep 等原语允许即时导航和检索
- 绕过陈旧索引和复杂语法树问题

**渐进式披露**
- 代理通过探索逐步发现相关上下文
- 文件大小暗示复杂度
- 命名约定提示目的
- 时间戳可作为相关性代理

#### 5. 长周期任务策略

**压缩（Compaction）**
- 实践：将接近上下文窗口限制的对话摘要，用摘要重新初始化新上下文
- Claude Code 实现：
  - 传递消息历史让模型摘要
  - 保留架构决策、未解决 bug、实现细节
  - 丢弃冗余工具输出
  - 继续时携带压缩上下文 + 最近 5 个访问文件
- 艺术：选择保留 vs 丢弃的内容
  - 过度激进压缩可能丢失微妙但关键的上下文
  - 建议：先最大化召回，再迭代提高精度

**结构化笔记（Structured Note-taking）**
- 代理定期将笔记持久化到上下文外存储
- 后续时间拉回上下文
- 示例：
  - Claude Code 创建待办列表
  - 自定义代理维护 NOTES.md
  - Claude 玩 Pokémon 时维护精确计数（"过去 1,234 步我在 Route 1 训练，Pikachu 获得 8 级，目标 10 级"）
- 优势：持久记忆、最小开销、跨复杂任务追踪进度

**子代理架构**
- 主代理协调高层计划
- 子代理处理专注任务，有干净上下文窗口
- 子代理深入探索（可能用数万 token）
- 返回浓缩摘要（通常 1-2K token）
- 优势：关注点分离、详细搜索上下文隔离

**策略选择**：
- 压缩：需要大量来回的任务
- 笔记：有清晰里程碑的迭代开发
- 多代理：复杂研究和分析，并行探索有回报

### 架构师视角洞察

#### 1. 注意力预算思维
> "LLM 有'注意力预算'，每个新 token 引入都会消耗这个预算。"

- 设计时需问："这个信息值得占用注意力预算吗？"
- 寻找最小高信号 token 集最大化期望结果概率

#### 2. 混合检索策略
- 静态数据预加载（如 CLAUDE.md）
- 动态数据即时检索（如 grep/glob）
- 决策边界取决于任务

#### 3. 压缩的艺术
- 保留：架构决策、未解决 bug、实现细节
- 丢弃：冗余工具输出、重复消息
- 最轻量压缩：工具结果清除（工具调用后无需再看原始结果）

#### 4. 多代理架构优势
- 子代理返回 1-2K token 摘要
- 主代理专注于综合分析和决策
- 复杂研究任务上显著优于单代理系统

#### 5. 智能模型需要更少工程
> "随着模型能力提升，智能模型可以更自主操作，需要更少预设工程。"

- 但即使能力提升，将上下文视为珍贵有限资源仍是核心

### 可落地实践建议

#### 公司场景

**RAG 系统优化**

分层检索：
- 第一层：检索文档元数据（标题、摘要、标签）
- 第二层：用户/代理选择后加载全文
- 第三层：按需提取特定段落

上下文压缩：
- 对多轮对话进行摘要
- 保留关键决策点和用户偏好
- 比较压缩前后回答质量

结构化笔记系统：
- 代理将中间结果写入外部存储（数据库/文件）
- 后续按需读取
- 维护项目状态跨会话

**LLM 应用架构**

工具返回结果设计：
- 结构化摘要而非原始数据
- 例如：数据库查询返回聚合统计而非所有行
- 错误信息简洁、结构化、易解析

长任务检查点：
- 定期将状态持久化
- 支持从中断点恢复
- 用 NOTES.md 或数据库表存储

#### 个人场景

**Claude Code 使用策略**
- 主动维护 PROJECT.md 记录架构决策
- 维护 TODO.md 追踪进度
- 维护 NOTES.md 记录中间发现

**复杂分析任务**
- 让 AI 先输出大纲再逐步展开
- 避免一次性加载过多上下文
- 对长对话定期请求摘要

**文件组织**
- 清晰目录层次（AI 能理解）
- 有意义的文件名
- 时间戳暗示相关性

---

## 🎯 本周行动建议

### 1. 【RAG 评估体系搭建】（优先级：高，预计耗时：2-3 天）

**目标**：建立 RAG 系统的基础评估框架

**步骤**：
1. 从生产环境抽取 20-30 个典型查询作为初始评估集
   - 包含成功和失败案例
   - 覆盖不同查询类型（事实性、分析性、创造性）

2. 设计三类评分器：
   - **事实准确性**（代码评分器）：
     - 答案包含关键实体/数字
     - 引用来源存在且相关
   - **回答完整性**（LLM 评分器）：
     - 用 rubric 评分（1-5 分）
     - 维度：相关性、完整性、清晰度
   - **响应时间**（指标）：
     - P95 延迟<阈值
     - Token 消耗监控

3. 建立回归测试基线：
   - 将当前版本性能记录为基线
   - 确保后续优化不降低现有质量
   - 每周自动运行一次

**预期产出**：
- 评估集（JSON/YAML 格式）
- 评分器脚本
- 基线报告

### 2. 【上下文工程实验】（优先级：中，预计耗时：1-2 天）

**目标**：验证上下文优化对 RAG 性能的影响

**实验 A：即时检索**
- 当前：检索后直接加载全文到上下文
- 实验：先返回文档列表，代理选择后再加载全文
- 指标：回答质量、Token 消耗、延迟

**实验 B：上下文压缩**
- 当前：保留完整对话历史
- 实验：对历史对话进行摘要（保留关键点）
- 指标：多轮对话连贯性、回答质量

**步骤**：
1. 选择 10 个典型多轮对话场景
2. 实现两种策略
3. A/B 测试比较效果
4. 记录结果并决策

### 3. 【多代理架构探索】（优先级：低，预计耗时：3-5 天）

**目标**：探索多代理架构在复杂业务场景的应用

**场景选择**：技术方案生成

**代理设计**：
- **需求分析代理**：理解用户需求，提取关键约束
- **技术调研代理**：搜索相关技术栈、最佳实践
- **方案设计代理**：综合信息生成技术方案
- **质量审查代理**：检查方案完整性、可行性

**步骤**：
1. 设计代理间通信机制（共享存储/消息队列）
2. 实现任务锁避免重复工作
3. 对比单代理 vs 多代理输出质量
4. 分析成本效益

**预期产出**：
- 多代理架构原型
- 质量对比报告
- 成本分析

---

## 📚 延伸阅读

- [Building effective agents](https://www.anthropic.com/engineering/building-effective-agents)
- [Effective harnesses for long-running agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)
- [How we built our multi-agent research system](https://www.anthropic.com/engineering/multi-agent-research-system)
- [Contextual Retrieval](https://www.anthropic.com/engineering/contextual-retrieval)
- [Writing effective tools for agents — with agents](https://www.anthropic.com/engineering/writing-tools-for-agents)

---

*周报由 贾维斯 生成 | 下次推送：2026-03-06*
